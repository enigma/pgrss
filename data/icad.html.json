{"href":"icad.html","title":"Revenge of the Nerds","content":"<font face=\"verdana\" size=\"2\"><table cellspacing=\"0\" width=\"100%\">\n<tr><td bgcolor=\"#ff9922\"><img height=\"15\" src=\"http://www.virtumundo.com/images/spacer.gif\" width=\"1\"/><font size=\"2\">\n<b>Want to start a startup?</b>  Get funded by\n<a href=\"http://ycombinator.com/apply.html\">Y Combinator</a>.\n</font>\n<br/><img height=\"5\" src=\"http://www.virtumundo.com/images/spacer.gif\" width=\"1\"/></td></tr>\n</table>\n<p>\nMay 2002<br><br/><table cellpadding=\"6\" cellspacing=\"0\" width=\"410\">\n<tr><td bgcolor=\"#ffffdd\">\n<table cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\n<tr><td bgcolor=\"#ffffdd\"><font size=\"2\">\n\"We were after the C++ programmers. We managed to drag a \nlot of them about halfway to Lisp.\"<br/><br/>- Guy Steele, co-author of the Java spec\n</font>\n</td></tr></table>\n</td></tr></table><br/><br/><!--<i>(This is an expanded version of the keynote lecture at the\nInternational ICAD User's Group conference in May 2002.\nIt explains how a language\ndeveloped in 1958 manages to be the most powerful  \navailable even today, what power is and when you need it, and\nwhy pointy-haired bosses (ideally, your\ncompetitors' pointy-haired bosses) deliberately ignore this issue.)<br /><br /><b>Note:</b> In this talk by \"Lisp\", I mean the Lisp family of<br /><br />languages, including Common Lisp, Scheme, Emacs Lisp, EuLisp,\nGoo, Arc, etc.</i><br /><br />\n-->\n<!-- Let me start by admitting that I don't know much about ICAD.\nI do know that it's written in Lisp, and in fact includes Lisp,\nin the sense that it lets users create and run Lisp programs.<br /><br />It's fairly common for programs written in Lisp to include\nLisp.  Emacs does, and so does Yahoo Store.\nBut if you think about it, that's kind of strange.\nHow many programs written in C include C, in the sense that   \nthe user actually runs the C compiler while he's using the\napplication?  I can't think of any, unless you count Unix\nas an application.  We're only a minute into this talk and\nalready Lisp is looking kind of unusual.<br /><br />Now, it is probably not news to any of you that Lisp is   \nlooking unusual.  In fact, that was probably the first thing\nyou noticed about it.<br /><br />Believe it or not, there is a reason Lisp code looks\nso strange.  Lisp doesn't look this way because it was designed\nby a bunch of pointy-headed academics.  It <i>was</i>\ndesigned by pointy-headed academics, but they had hard-headed  \nengineering reasons for making the syntax look so strange.<br /><br /><b>Are All Languages Equivalent?</b>\n-->\nIn the software business there is an ongoing\nstruggle between the pointy-headed academics, and another\nequally formidable force, the pointy-haired bosses.  Everyone\nknows who the pointy-haired boss is, right?  I think most\npeople in the technology world not only recognize this\ncartoon character, but know the actual person in their company\nthat he is modelled upon.<br/><br/>The pointy-haired boss miraculously combines two qualities\nthat are common by themselves, but rarely seen together:\n(a) he knows nothing whatsoever about technology, and\n(b) he has very strong opinions about it.<br/><br/>Suppose, for example, you need to write a piece of software.\nThe pointy-haired boss has no idea how this software\nhas to work, and can't tell one programming language from\nanother, and yet he knows what language you should write it in.\nExactly.  He thinks you should write it in Java.<br/><br/>Why does he think this?  Let's\ntake a look inside the brain of the pointy-haired boss.  What\nhe's thinking is something like this.  Java is a standard.\nI know it must be, because I read about it in the press all the time.\nSince it is a standard, I won't get in trouble for using it.\nAnd that also means there will always be lots of Java programmers,\nso if the programmers working for me now quit, as programmers\nworking for me mysteriously always do, I can easily replace\nthem.<br/><br/>Well, this doesn't sound that unreasonable.  But it's all\nbased on one unspoken assumption, and that assumption\nturns out to be false.  The pointy-haired boss believes that all\nprogramming languages are pretty much equivalent.\nIf that were true, he would be right on\ntarget.  If languages are all equivalent, sure, use whatever \nlanguage everyone else is using.<br/><br/>But all languages are not equivalent, and I think I can prove\nthis to you without even getting into the differences between them.\nIf you asked the pointy-haired boss in 1992 what language     \nsoftware should be written in, he would have answered with as\nlittle hesitation as he does today.  Software should be  \nwritten in C++.  But if languages are all equivalent, why should the\npointy-haired boss's opinion ever change?  In fact, why should\nthe developers of Java have even bothered to create a new\nlanguage?<br/><br/>Presumably, if you create a new language, it's because you think\nit's better in some way than what people already had.  And in fact, Gosling\nmakes it clear in the first Java white paper that Java\nwas designed to fix some problems with C++.\nSo there you have it: languages are not all equivalent.\nIf you follow the\ntrail through the pointy-haired boss's brain to Java and then\nback through Java's history to its origins, you end up holding\nan idea that contradicts the assumption you started with.<br/><br/>So, who's right?  James Gosling, or the pointy-haired boss?\nNot surprisingly, Gosling is right.  Some languages <i>are</i> better,\nfor certain problems, than others.  And you know, that raises some\ninteresting questions.  Java was designed to be better, for certain\nproblems, than C++.  What problems?  When is Java better and \nwhen is C++?  Are there situations where other languages are\nbetter than either of them?<br/><br/>Once you start considering this question, you have opened a\nreal can of worms.  If the pointy-haired boss had to think\nabout the problem in its full complexity, it would make his\nbrain explode.  As long as he considers all languages   \nequivalent, all he has to do is choose the one\nthat seems to have the most momentum, and since that is more\na question of fashion than technology, even he\ncan probably get the right answer.\nBut if languages vary, he suddenly\nhas to solve two simultaneous equations, trying to find\nan optimal balance between two things he knows nothing   \nabout: the relative suitability of the twenty or so leading\nlanguages for the problem he needs to solve, and the odds of\nfinding programmers, libraries, etc. for each.\nIf that's what's on the other side of the door, it\nis no surprise that the pointy-haired boss doesn't want to open it.<br/><br/>The disadvantage of believing that all programming languages\nare equivalent is that it's not true.  But the advantage is \nthat it makes your life a lot simpler.\nAnd I think that's the main reason the idea is so widespread.\nIt is a <i>comfortable</i> idea.<br/><br/>We know that Java must be pretty good, because it is the\ncool, new programming language.  Or is it?  If you look at the world of\nprogramming languages from a distance, it looks like Java is\nthe latest thing.  (From far enough away, all you can see is\nthe large, flashing billboard paid for by Sun.)\nBut if you look at this world\nup close, you find that there are degrees of coolness.  Within\nthe hacker subculture, there is another language called Perl\nthat is considered a lot cooler than Java.  Slashdot, for\nexample, is generated by Perl.  I don't think you would find\nthose guys using Java Server Pages.  But there is another,\nnewer language, called Python, whose users tend to look down on Perl,\nand <a href=\"accgen.html\">more</a> waiting in the wings.<br/><br/>If you look at these languages in order, Java, Perl, Python,\nyou notice an interesting pattern.  At least, you notice this\npattern if you are a Lisp hacker.  Each one is progressively \nmore like Lisp.  Python copies even features\nthat many Lisp hackers consider to be mistakes.\nYou could translate simple Lisp programs into Python line for line.\nIt's 2002, and programming languages have almost caught up \nwith 1958.<br/><br/><b>Catching Up with Math</b><br/><br/>What I mean is that\nLisp was first discovered by John McCarthy in 1958,\nand popular programming languages are only now\ncatching up with the ideas he developed then.<br/><br/>Now, how could that be true?  Isn't computer technology something\nthat changes very rapidly?  I mean, in 1958, computers were\nrefrigerator-sized behemoths with the processing power of    \na wristwatch.  How could any technology that old even be\nrelevant, let alone superior to the latest developments?<br/><br/>I'll tell you how.  It's because Lisp was not really\ndesigned to be a programming language, at least not in the sense\nwe mean today.  What we mean by a programming language is\nsomething we use to tell a computer what to do.   McCarthy\ndid eventually intend to develop a programming language in\nthis sense, but the Lisp that we actually ended up with was based\non something separate that he did as a \n<a href=\"rootsoflisp.html\">theoretical exercise</a>-- an effort\nto define a more convenient alternative to the Turing Machine.\nAs McCarthy said later,\n<blockquote>\nAnother way to show that Lisp was neater than Turing machines\nwas to write a universal Lisp function\nand show that it is briefer and more comprehensible than the\ndescription of a universal Turing machine.\nThis was the Lisp function <a href=\"https://sep.turbifycdn.com/ty/cdn/paulgraham/jmc.lisp?t=1688221954&amp;\"><i>eval</i></a>..., \nwhich computes the value of\na Lisp expression....\nWriting <i>eval</i> required inventing a notation representing Lisp\nfunctions as Lisp data, and such a notation\nwas devised for the purposes of the paper with no thought that\nit would be used to express Lisp programs in practice.\n</blockquote>\nWhat happened next was that, some time in late 1958, Steve Russell,\none of McCarthy's\ngrad students, looked at this definition of <i>eval</i> and realized  \nthat if he translated it into machine language, the result\nwould be a Lisp interpreter.<br/><br/>This was a big surprise at the time.\nHere is what McCarthy said about it later in an interview:\n<blockquote>\nSteve Russell said, look, why don't I program this <i>eval</i>..., and\nI said to him, ho, ho, you're confusing theory with practice,\nthis <i>eval</i> is intended for reading, not for\ncomputing. But he went ahead and did it. That is, he compiled the <i>eval</i>\nin my paper into [IBM] 704 machine\ncode, fixing bugs, and then advertised this as a Lisp interpreter,\nwhich it certainly was. So at that point Lisp\nhad essentially the form that it has today....\n</blockquote>\nSuddenly, in a matter of weeks I think, McCarthy found his theoretical\nexercise transformed into an actual programming language-- and a\nmore powerful one than he had intended.<br/><br/>So the short explanation of why this 1950s language is not\nobsolete is that it was not technology but math, and\nmath doesn't get stale.   The right thing to compare Lisp\nto is not 1950s hardware, but, say, the Quicksort\nalgorithm, which was discovered in 1960 and is still\nthe fastest general-purpose sort.<br/><br/>There is one other language still\nsurviving from the 1950s, Fortran, and it represents the\nopposite approach to language design.  Lisp was a\npiece of theory that unexpectedly got turned into a\nprogramming language.  Fortran was developed intentionally as\na programming language, but what we would now consider a\nvery low-level one.<br/><br/><a href=\"history.html\">Fortran I</a>, the language that was\ndeveloped in 1956, was a very different animal from present-day\nFortran.   Fortran I was pretty much assembly\nlanguage with math.  In some ways it was less\npowerful than more recent assembly languages; there were no   \nsubroutines, for example, only branches.\nPresent-day Fortran is now arguably closer to Lisp than to\nFortran I.<br/><br/>Lisp and Fortran were the trunks of two separate evolutionary trees, \none rooted in math and one rooted in machine architecture.\nThese two trees have been converging ever since.\nLisp started out powerful, and over the next twenty years\ngot fast.  So-called mainstream languages started out\nfast, and over the next forty years gradually got more powerful,\nuntil now the most advanced\nof them are fairly close to Lisp.\nClose, but they are still missing a few things....<br/><br/><b>What Made Lisp Different</b><br/><br/>When it was first developed, Lisp embodied nine new\nideas.  Some of these we now take for granted, others are\nonly seen in more advanced languages, and two are still\nunique to Lisp.  The nine ideas are, in order of their\nadoption by the mainstream,\n<ol>\n<li> Conditionals.  A conditional is an if-then-else\nconstruct.  We take these for granted now, but Fortran I\ndidn't have them. It had only a conditional goto\nclosely based on the underlying machine instruction.<br/><br/><li> A function type. In Lisp, functions are\na data type just like integers or strings.\nThey have a literal representation, can be stored in variables,\ncan be passed as arguments, and so on.<br/><br/><li> Recursion.  Lisp was the first programming language to\nsupport it.<br/><br/><li> Dynamic typing.  In Lisp, all variables\nare effectively pointers. Values are what\nhave types, not variables, and assigning or binding\nvariables means copying pointers, not what they point to.<br/><br/><li> Garbage-collection.<br/><br/><li> Programs composed of expressions.  Lisp programs are\ntrees of expressions, each of which returns a value.\nThis is in contrast to Fortran\nand most succeeding languages, which distinguish between\nexpressions and statements.<br/><br/>It was natural to have this\ndistinction in Fortran I because\nyou could not nest statements.  And\nso while you needed expressions for math to work, there was\nno point in making anything else return a value, because\nthere could not be anything waiting for it.<br/><br/>This limitation\nwent away with the arrival of block-structured languages,\nbut by then it was too late. The distinction between\nexpressions and statements was entrenched.  It spread from\nFortran into Algol and then to both their descendants.<br/><br/><li> A symbol type.  Symbols are effectively pointers to strings\nstored in a hash table.  So\nyou can test equality by comparing a pointer,\ninstead of comparing each character.<br/><br/><li> A notation for code using trees of symbols and constants.<br/><br/><li> The whole language there all the time.  There is\nno real distinction between read-time, compile-time, and runtime.\nYou can compile or run code while reading, read or run code\nwhile compiling, and read or compile code at runtime.<br/><br/>Running code at read-time lets users reprogram Lisp's syntax;\nrunning code at compile-time is the basis of macros; compiling\nat runtime is the basis of Lisp's use as an extension\nlanguage in programs like Emacs; and reading at runtime\nenables programs to communicate using s-expressions, an\nidea recently reinvented as XML.\n</li></li></li></li></li></li></li></li></li></ol>\nWhen Lisp first appeared, these ideas were far\nremoved from ordinary programming practice, which was\ndictated largely by the hardware available in the late 1950s.\nOver time, the default language, embodied\nin a succession of popular languages, has\ngradually evolved toward Lisp.  Ideas 1-5 are now widespread.\nNumber 6 is starting to appear in the mainstream.  \nPython has a form of 7, though there doesn't seem to be    \nany syntax for it.<br/><br/>As for number 8, this may be the most interesting of the\nlot.  Ideas 8 and 9 only became part of Lisp\nby accident, because Steve Russell implemented\nsomething McCarthy had never intended to be implemented.\nAnd yet these ideas turn out to be responsible for\nboth Lisp's strange appearance and its most distinctive\nfeatures.  Lisp looks strange not so much because\nit has a strange syntax as because it has no syntax;\nyou express programs directly in the parse trees that\nget built behind the scenes when other languages are\nparsed, and these trees are made\nof lists, which are Lisp data structures.<br/><br/>Expressing the language in its own data structures turns\nout to be a very powerful feature. Ideas 8 and 9\ntogether mean that you\ncan write programs that write programs.  That may sound\nlike a bizarre idea, but it's an everyday thing in Lisp. \nThe most common way to do it is with something called a        \n<i>macro.</i><br/><br/>The term \"macro\" does not mean in Lisp what it means in other\nlanguages.\nA Lisp macro can be anything from an abbreviation\nto a compiler for a new language.\nIf you want to really understand Lisp,\nor just expand your programming horizons, I would \n<a href=\"onlisp.html\">learn more</a> about macros.<br/><br/>Macros (in the Lisp sense) are still, as far as\nI know, unique to Lisp.\nThis is partly because in order to have macros you\nprobably have to make your language look as strange as\nLisp.  It may also be because if you do add that final\nincrement of power, you can no\nlonger claim to have invented a new language, but only\na new dialect of Lisp.<br/><br/>I mention this mostly\nas a joke, but it is quite true. If you define\na language that has car, cdr, cons, quote, cond, atom,\neq, and\na notation for functions expressed as lists, then you\ncan build all the rest of Lisp out of it.  That is in\nfact the defining quality of Lisp: it was in order to\nmake this so that McCarthy gave Lisp the shape it has.<br/><br/><b>Where Languages Matter</b><br/><br/>So suppose Lisp does represent a kind of limit     \nthat mainstream languages are approaching asymptotically-- does\nthat mean you should actually use it to write software?\nHow much do you lose by using a less powerful language?\nIsn't it wiser, sometimes, not to be\nat the very edge of innovation?\nAnd isn't popularity to some extent\nits own justification?  Isn't the pointy-haired boss right,\nfor example, to want to use a language for which he can easily\nhire programmers?<br/><br/>There are, of course, projects where the choice of programming\nlanguage doesn't matter much.  As a\nrule, the more demanding the application, the more\nleverage you get from using a powerful language.  But\nplenty of projects are not demanding at all.\nMost programming probably consists of writing \nlittle glue programs, and for \nlittle glue programs you\ncan use any language that you're already\nfamiliar with and that has good libraries for whatever you\nneed to do.  If you just need to feed data from one   \nWindows app to another, sure, use Visual Basic.<br/><br/>You can write little glue programs in Lisp too\n(I use it as a desktop calculator), but the biggest win\nfor languages like Lisp is at the other end of\nthe spectrum, where you need to write sophisticated\nprograms to solve hard problems in the face of fierce competition.\nA good example is the\n<a href=\"carl.html\">airline fare search program</a> that ITA Software licenses to\nOrbitz.  These\nguys entered a market already dominated by two big,\nentrenched competitors, Travelocity and Expedia, and  \nseem to have just humiliated them technologically.<br/><br/>The core of ITA's application is a 200,000 line Common Lisp program\nthat searches many orders of magnitude more possibilities\nthan their competitors, who apparently\nare still using mainframe-era programming techniques.\n(Though ITA is also in a sense\nusing a mainframe-era programming language.)\nI have never seen any of ITA's code, but according to\none of their top hackers they use a lot of macros,\nand I am not surprised to hear it.<br/><br/><b>Centripetal Forces</b><br/><br/>I'm not saying there is no cost to using uncommon  \ntechnologies.  The pointy-haired boss is not completely\nmistaken to worry about this.  But because he doesn't understand\nthe risks, he tends to magnify them.<br/><br/>I can think of three problems that could arise from using\nless common languages.  Your programs might not work well with\nprograms written in other languages.  You might have fewer\nlibraries at your disposal.  And you might have trouble\nhiring programmers.<br/><br/>How much of a problem is each of these?  The importance of\nthe first varies depending on whether you have control\nover the whole system.  If you're writing software that has\nto run on a remote user's machine on top of a buggy,\nclosed operating system (I mention no names), there may be\nadvantages to writing your application in the\nsame language as the OS.\nBut if you control the whole system and\nhave the source code of all the parts, as ITA presumably does, you\ncan use whatever languages you want.  If\nany incompatibility arises, you can fix it yourself.<br/><br/>In server-based applications you can\nget away with using the most advanced technologies,\nand I think this is the main\ncause of what Jonathan Erickson calls the \"<a href=\"http://www.byte.com/documents/s=1821/byt20011214s0003/\">programming language\nrenaissance</a>.\"  This is why we even hear about new\nlanguages like Perl and Python.  We're not hearing about these\nlanguages because people are using them to write Windows\napps, but because people are using them on servers.  And as\nsoftware shifts \n<a href=\"road.html\">off the desktop</a> and onto servers (a future even\nMicrosoft seems resigned to), there will be less\nand less pressure to use middle-of-the-road technologies.<br/><br/>As for libraries, their importance also\ndepends on the application.  For less demanding problems,\nthe availability of libraries can outweigh the intrinsic power\nof the language.  Where is the breakeven point?  Hard to say\nexactly, but wherever it is, it is short of anything you'd\nbe likely to call an application.  If a company considers\nitself to be in the software business, and they're writing\nan application that will be one of their products,\nthen it will probably involve several hackers and take at\nleast six months to write.  In a project of that\nsize, powerful languages probably start to outweigh\nthe convenience of pre-existing libraries.<br/><br/>The third worry of the pointy-haired boss, the difficulty\nof hiring programmers, I think is a red herring.   How many\nhackers do you need to hire, after all?  Surely by now we\nall know that software is best developed by teams of less\nthan ten people.   And you shouldn't have trouble hiring\nhackers on that scale for any language anyone has ever heard\nof.  If you can't find ten Lisp hackers, then your company is\nprobably based in the wrong city for developing software.<br/><br/>In fact, choosing a more powerful language probably decreases the\nsize of the team you need, because (a) if you use a more powerful\nlanguage you probably won't need as many hackers,\nand (b) hackers who work in more advanced languages are likely\nto be smarter.<br/><br/>I'm not saying that you won't get a lot of pressure to use\nwhat are perceived as \"standard\" technologies.  At Viaweb\n(now Yahoo Store),\nwe raised some eyebrows among VCs and potential acquirers by\nusing Lisp.  But we also raised eyebrows by using\ngeneric Intel boxes as servers instead of\n\"industrial strength\" servers like Suns, for using a\nthen-obscure open-source Unix variant called FreeBSD instead\nof a real commercial OS like Windows NT, for ignoring\na supposed e-commerce standard called \n<a href=\"http://news.com.com/2100-1017-225723.html\">SET</a> that no one now\neven remembers, and so on.<br/><br/>You can't let the suits make technical decisions for you.\nDid it\nalarm some potential acquirers that we used Lisp?  Some, slightly,\nbut if we hadn't used Lisp, we wouldn't have been\nable to write the software that made them want to buy us.\nWhat seemed like an anomaly to them was in fact\ncause and effect.<br/><br/>If you start a startup, don't design your product to please\nVCs or potential acquirers.  <i>Design your product to please\nthe users.</i>  If you win the users, everything else will\nfollow.  And if you don't, no one will care\nhow comfortingly orthodox your technology choices were.<br/><br/><b>The Cost of Being Average</b><br/><br/>How much do you lose by using a less powerful language?  \nThere is actually some data out there about that.<br/><br/>The most convenient measure of power is probably \n<a href=\"power.html\">code size</a>.\nThe point of high-level\nlanguages is to give you bigger abstractions-- bigger bricks,\nas it were, so you don't need as many to build\na wall of a given size.\nSo the more powerful\nthe language, the shorter the program (not simply in\ncharacters, of course, but in distinct elements).<br/><br/>How does a more powerful language enable you to write\nshorter programs?  One technique you can use, if the language will\nlet you, is something called \n<a href=\"progbot.html\">bottom-up programming</a>.  Instead of\nsimply writing your application in the base language, you\nbuild on top of the base language a language for writing\nprograms like yours, then write your program\nin it. The combined code can be much shorter than if you\nhad written your whole program in the base language-- indeed,\nthis is how most compression algorithms work.\nA bottom-up program should be easier to modify as well,  \nbecause in many cases the language layer won't have to change\nat all.<br/><br/>Code size is important, because the time it takes\nto write a program depends mostly on its length.\nIf your program would be three times as long in another\nlanguage, it will take three times as long to write-- and\nyou can't get around this by hiring more people, because\nbeyond a certain size new hires are actually a net lose.\nFred Brooks described this phenomenon in his famous\nbook <i>The Mythical Man-Month,</i> and everything I've seen\nhas tended to confirm what he said.<br/><br/>So how much shorter are your programs if you write them in\nLisp?  Most of the numbers I've heard for Lisp\nversus C, for example, have been around 7-10x.\nBut a recent article about ITA in \n<a href=\"http://www.newarchitectmag.com/documents/s=2286/new1015626014044/\"><i>New\nArchitect</i></a> magazine said that\n\"one line of Lisp can replace 20 lines of C,\" and since\nthis article was full of quotes from ITA's president, I\nassume they got this number from ITA.  If so then\nwe can put some faith in it; ITA's software includes a lot\nof C and  C++ as well as Lisp, so they are speaking from\nexperience.<br/><br/>My guess is that these multiples aren't even constant.\nI think they increase when\nyou face harder problems and also when you have smarter\nprogrammers.  A really good hacker can squeeze more\nout of better tools.<br/><br/>As one data point on the curve, at any rate,\nif you were to compete with ITA and\nchose to write your software in C, they would be able to develop\nsoftware twenty times faster than you.\nIf you spent a year on a new feature, they'd be able to\nduplicate it in less than three weeks.  Whereas if they spent\njust three months developing something new, it would be\n<i>five years</i> before you had it too.<br/><br/>And you know what?  That's the best-case scenario.\nWhen you talk about code-size ratios, you're implicitly assuming\nthat you can actually write the program in the weaker language.\nBut in fact there are limits on what programmers can do.\nIf you're trying to solve a hard problem with a language that's\ntoo low-level, you reach a point where there is just too \nmuch to keep in your head at once.<br/><br/>So when I say it would take ITA's imaginary\ncompetitor five years to duplicate something ITA could\nwrite in Lisp in three months, I mean five years\nif nothing goes wrong.  In fact, the way things work in \nmost companies, any\ndevelopment project that would take five years is\nlikely never to get finished at all.<br/><br/>I admit this is an extreme case.  ITA's hackers seem to\nbe unusually smart, and C is a pretty low-level language.\nBut in a competitive market, even a differential of two or\nthree to one would\nbe enough to guarantee that you'd always be behind.<br/><br/><b>A Recipe</b><br/><br/>This is the kind of possibility that the pointy-haired boss\ndoesn't even want to think about.  And so most of them don't.\nBecause, you know, when it comes down to it, the pointy-haired\nboss doesn't mind if his company gets their ass kicked, so\nlong as no one can prove it's his fault.\nThe safest plan for him personally\nis to stick close to the center of the herd.<br/><br/>Within large organizations, the phrase used to\ndescribe this approach is \"industry best practice.\"\nIts purpose is to shield the pointy-haired\nboss from responsibility: if he chooses\nsomething that is \"industry best practice,\" and the company\nloses, he can't be blamed.  He didn't choose, the industry did.<br/><br/>I believe this term was originally used to describe\naccounting methods and so on.  What it means, roughly,\nis <i>don't do anything weird.</i>  And in accounting that's\nprobably a good idea.  The terms \"cutting-edge\" and  \n\"accounting\" do not sound good together.  But when you import\nthis criterion into decisions about technology, you start\nto get the wrong answers.<br/><br/>Technology often <i>should</i> be\ncutting-edge.  In programming languages, as Erann Gat\nhas pointed out, what \"industry best practice\"  actually\ngets you is not the best, but merely the\naverage.  When a decision causes you to develop software at\na fraction of the rate of more aggressive competitors,  \n\"best practice\" is a misnomer.<br/><br/>\nSo here we have two pieces of information that I think are\nvery valuable.  In fact, I know it from my own experience.\nNumber 1, languages vary in power.  Number 2, most managers\ndeliberately ignore this.  Between them, these two facts\nare literally a recipe for making money.  ITA is an example\nof this recipe in action.\nIf you want to win in a software\nbusiness, just take on the hardest problem you can find,\nuse the most powerful language you can get, and wait for\nyour competitors' pointy-haired bosses to revert to the mean.<br/><br/>\n<hr/><br/><br/><br/><br/>\n<b>Appendix: Power</b><br/><br/>As an illustration of what I mean about the relative power\nof programming languages, consider the following problem.\nWe want to write a function that generates accumulators-- a\nfunction that takes a number n, and\nreturns a function that takes another number i and\nreturns n incremented by i.<br/><br/>(That's <i>incremented by</i>, not plus.  An accumulator\nhas to accumulate.)<br/><br/>In Common Lisp this would be\n<font face=\"courier\"><xmp>\n(defun foo (n)\n  (lambda (i) (incf n i)))\n</xmp></font>\nand in Perl 5,\n<font face=\"courier\"><xmp>\nsub foo {  \n  my ($n) = @_;\n  sub {$n += shift}\n}\n</xmp></font>\nwhich has more elements than the Lisp version because\nyou have to extract parameters manually in Perl.<br/><br/>In Smalltalk the code is slightly longer than in Lisp\n<font face=\"courier\"><xmp>\nfoo: n                              \n  |s|                      \n  s := n.                          \n  ^[:i| s := s+i. ] \n</xmp></font>\nbecause although in general lexical variables work, you can't\ndo an assignment to a parameter, so you have to create a\nnew variable s.<br/><br/>In Javascript the example is, again, slightly longer, because \nJavascript retains\nthe distinction between statements and\nexpressions, so you need explicit <tt>return</tt> statements\nto return values:\n<font face=\"courier\"><xmp>\nfunction foo(n) { \n  return function (i) { \n           return n += i } }\n</xmp></font>\n(To be fair, Perl also retains\nthis distinction, but deals with it in typical Perl fashion\nby letting you omit <tt>return</tt>s.)<br/><br/>If you try to translate the Lisp/Perl/Smalltalk/Javascript code into \nPython you run into some limitations.  Because Python\ndoesn't fully support lexical variables,\nyou have to create a data structure to hold the value of n.\nAnd although\nPython does have a function data type, there is no\nliteral representation for one (unless the body is\nonly a single expression) so you need to create a named\nfunction to return.  This is what you end up with:\n<font face=\"courier\"><xmp>\ndef foo(n):\n  s = [n]\n  def bar(i):\n    s[0] += i\n    return s[0] \n  return bar\n</xmp></font>\nPython users might legitimately ask why they can't\njust write\n<font face=\"courier\"><xmp>\ndef foo(n):\n  return lambda i: return n += i\n</xmp></font>\nor even\n<font face=\"courier\"><xmp>\ndef foo(n):\n  lambda i: n += i\n</xmp></font>\nand my guess is that they probably will, one day.\n(But if they don't want to wait for Python to evolve the rest\nof the way into Lisp, they could always just...)\n<!-- (Oscar Wilde: \"I wish I had said that.\" Whistler: \"You will,\nOscar, you will.\")  --><br/><br/>In OO languages, you can, to a limited extent, simulate\na closure (a function that refers to variables defined in\nenclosing scopes) by defining a class with one method\nand a field to replace each variable from an enclosing\nscope.  This makes the programmer do the kind of code\nanalysis that would be done by the compiler in a language\nwith full support for lexical scope, and it won't work\nif more than one function refers to the same variable,\nbut it is enough in simple cases like this.<br/><br/>Python experts seem to agree that this is the\npreferred way to solve the problem in Python, writing\neither\n<font face=\"courier\"><xmp>\ndef foo(n):\n  class acc:\n    def __init__(self, s):\n        self.s = s\n    def inc(self, i):\n        self.s += i\n        return self.s\n  return acc(n).inc\n</xmp></font>\nor\n<font face=\"courier\"><xmp>\nclass foo:\n  def __init__(self, n):\n      self.n = n\n  def __call__(self, i):\n      self.n += i\n      return self.n\n</xmp></font>\nI include these because I wouldn't want Python\nadvocates to say I was misrepresenting the language,   \nbut both seem to me more complex than the first   \nversion.  You're doing the same thing, setting up\na separate place to hold the accumulator; it's just\na field in an object instead of the head of a list.\nAnd the use of these special,\nreserved field names, especially <tt>__call__</tt>, seems\na bit of a hack.<br/><br/>In the rivalry between Perl and Python, the claim of the\nPython hackers seems to be that\nthat Python is a more elegant alternative to Perl, but what\nthis case shows is that power is the ultimate elegance:\nthe Perl program is simpler (has fewer elements), even if the\nsyntax is a bit uglier.<br/><br/>How about other languages? In the other languages\nmentioned in this talk-- Fortran, C, C++, Java, and\nVisual Basic-- it is not clear whether you can actually\nsolve this problem.\nKen Anderson says that the following code is about as close\nas you can get in Java:\n<font face=\"courier\"><xmp>\npublic interface Inttoint {\n  public int call(int i);\n}\n</xmp></font>\n<font face=\"courier\"><xmp>\npublic static Inttoint foo(final int n) {\n  return new Inttoint() {\n    int s = n;\n    public int call(int i) {\n    s = s + i;\n    return s;\n    }};\n}\n</xmp></font>\nThis falls short of the spec because it only works for\nintegers.  After many email exchanges with Java hackers,\nI would say that writing a properly polymorphic version\nthat behaves like the preceding examples is somewhere\nbetween damned awkward and impossible.  If anyone wants to\nwrite one I'd be very curious to see it, but I personally\nhave timed out.<br/><br/>It's not literally true that you can't solve this\nproblem in other languages, of course.  The fact\nthat all these languages are Turing-equivalent means\nthat, strictly speaking, you can write any program in\nany of them.  So how would you do it?  In the limit case,\nby writing a Lisp\ninterpreter in the less powerful language.<br/><br/>That sounds like a joke, but it happens so often to\nvarying degrees in large programming projects that\nthere is a name for the phenomenon, Greenspun's Tenth\nRule:\n<blockquote>\n Any sufficiently\n     complicated C or Fortran program contains an ad hoc\n     informally-specified bug-ridden slow implementation of half of\n     Common Lisp.\n</blockquote>\nIf you try to solve a\nhard problem, the question is not whether you will use\na powerful enough language, but whether you will (a)\nuse a powerful language, (b) write a de facto interpreter\nfor one, or (c) yourself become a human compiler for one.\nWe see this already\nbegining to happen in the Python example, where we are\nin effect simulating the code that a compiler\nwould generate to implement a lexical variable.<br/><br/>This practice is not only common, but institutionalized.  For example,\nin the OO world you hear a good deal about \n\"patterns\".\nI wonder if these patterns are not sometimes evidence of case (c),\nthe human compiler, at work.  When I see patterns in my programs,\nI consider it a sign of trouble.  The shape of a program\nshould reflect only the problem it needs to solve.\nAny other regularity in the code is a sign, to me at\nleast, that I'm using abstractions that aren't powerful\nenough-- often that I'm generating by hand the\nexpansions of some macro that I need to write.<br/><br/><br/><br/><b>Notes</b><br/><br/><ul>\n<li> The IBM 704 CPU was about the size of a refrigerator,\nbut a lot heavier.  The CPU weighed 3150 pounds,\nand the 4K of RAM was in a separate\nbox weighing another 4000 pounds.  The\nSub-Zero 690, one of the largest household refrigerators,\nweighs 656 pounds.<br/><br/><li> Steve Russell also wrote the first (digital) computer\ngame, Spacewar, in 1962.<br/><br/><li> If you want to trick a pointy-haired boss into letting you\nwrite software in Lisp, you could try telling him it's XML.<br/><br/><li> Here is the accumulator generator in other Lisp dialects:\n<font face=\"courier\"><xmp>\nScheme: (define (foo n) \n          (lambda (i) (set! n (+ n i)) n))\nGoo:    (df foo (n) (op incf n _)))\nArc:    (def foo (n) [++ n _])\n</xmp></font>\n<li> Erann Gat's sad tale about\n\"industry best practice\" at JPL inspired me to address\nthis generally misapplied phrase.<br/><br/><li> Peter Norvig found that\n16 of the 23 patterns in <i>Design Patterns</i> were \n\"<a href=\"http://www.norvig.com/design-patterns/\">invisible\nor simpler</a>\" in Lisp.<br/><br/><li> Thanks to the many people who answered my questions about\nvarious languages and/or read drafts of this, including\nKen Anderson, Trevor Blackwell, Erann Gat, Dan Giffin, Sarah Harlin,\nJeremy Hylton, Robert Morris, Peter Norvig, Guy Steele, and Anton\nvan Straaten.\nThey bear no blame for any opinions expressed.<br/><br/></li></li></li></li></li></li></li></ul><br/><br/>\n<b>Related:</b><br/><br/>Many people have responded to this talk,\nso I have set up an additional page to deal with the issues they have\nraised: <a href=\"icadmore.html\">Re: Revenge of the Nerds</a>.<br/><br/>It also set off an extensive and often useful discussion on the \n<a href=\"http://www.ai.mit.edu/~gregs/ll1-discuss-archive-html/threads.html\">LL1</a>\nmailing list.  See particularly the mail by Anton van Straaten on semantic\ncompression.<br/><br/>Some of the mail on LL1 led me to try to go deeper into the subject\nof language power in <a href=\"power.html\">Succinctness is Power</a>.<br/><br/>A larger set of canonical implementations of the <a href=\"accgen.html\">accumulator\ngenerator benchmark</a> are collected together on their own page.<br/><br/><a href=\"http://www.shiro.dreamhost.com/scheme/trans/icad-j.html\">Japanese Translation</a>, <a href=\"http://kapcoweb.com/p/docs/translations/revenge_of_the_nerds/revenge_of_the_nerds-es.html\">Spanish\nTranslation</a>, \n<a href=\"http://flyingapplet.spaces.live.com/blog/cns!F682AFBD82F7E261!375.entry \">Chinese Translation</a><br/><br/></br></p></font>","date":"2002-05-01T00:00:00Z"}