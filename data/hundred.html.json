{"href":"hundred.html","title":"The Hundred-Year Language","content":"<font face=\"verdana\" size=\"2\">April 2003<br/><br/><i>(This essay is derived from a keynote talk at PyCon 2003.)</i><br/><br/>It's hard to predict what\nlife will be like in a hundred years.  There are only a few\nthings we can say with certainty.  We know that everyone will\ndrive flying cars,\nthat zoning laws will be relaxed to allow buildings\nhundreds of stories tall, that it will be dark most of the\ntime, and that women will all be trained in the martial arts.  \nHere I want to zoom in on one detail of this\npicture.  What kind of programming language will they use to\nwrite the software controlling those flying cars?<br/><br/>This is worth thinking about not so\nmuch because we'll actually get to use these languages as because,\nif we're lucky, we'll use languages on the path from this\npoint to that.<br/><br/><br/><br/>I think that, like species, languages will form evolutionary trees,\nwith dead-ends branching off all over.  We can see this\nhappening already.\nCobol, for all its sometime popularity, does not seem to have any\nintellectual descendants.  It is an evolutionary dead-end-- a\nNeanderthal language.<br/><br/>I predict a similar fate for Java.  People\nsometimes send me mail saying, \"How can you say that Java\nwon't turn out to be a successful language?  It's already\na successful language.\"  And I admit that it is, if you\nmeasure success by shelf space taken up by books on it\n(particularly individual books on it), or by\nthe number of undergrads who believe they have to\nlearn it to get a job.  When I say Java won't\nturn out to be a successful language, I mean something more\nspecific:  that Java\nwill turn out to be an evolutionary dead-end, like Cobol.<br/><br/>This is just a guess.  I may be wrong.  My point here is not to dis Java,\nbut to raise the issue of evolutionary\ntrees and get people asking, where on the tree is language X?\nThe reason to ask this question isn't just so that\nour ghosts can say, in a\nhundred years, I told you so.  It's because staying close to  \nthe main branches is a useful heuristic for finding languages that will\nbe good to program in now.<br/><br/>At any given time, you're probably happiest on\nthe main branches of an evolutionary tree.\nEven when there were still plenty of Neanderthals, \nit must have sucked to be one.  The\nCro-Magnons would have been constantly coming over and\nbeating you up and stealing your food.<br/><br/>The reason I want to\nknow what languages will be like in a hundred years is so that\nI know what branch of the tree to bet on now.<br/><br/><br/><br/>The evolution of languages differs from the evolution of species\nbecause branches can converge.  The Fortran branch, for example,\nseems to be merging with the descendants\nof Algol.  In theory this is possible for species too, but it's\nnot likely to have happened to any bigger than a cell.<br/><br/>Convergence\nis more likely for languages partly because the space of\npossibilities is smaller, and partly because mutations\nare not random.  Language designers deliberately incorporate\nideas from other languages.<br/><br/>It's especially useful for language designers to think\nabout where the evolution of programming languages is likely\nto lead, because they can steer accordingly. \nIn that case, \"stay on a main branch\" becomes more than a\nway to choose a good language.\nIt becomes a heuristic for making the right decisions about\nlanguage design.<br/><br/><br/><br/>Any programming language can be divided into\ntwo parts:  some set of fundamental operators that play the role\nof axioms, and the rest of the language, which could in principle\nbe written in terms of these fundamental operators.<br/><br/>I think the fundamental operators are the most important factor in a\nlanguage's long term survival.  The rest you can change.  It's\nlike the rule that in buying a house you should consider\nlocation first of all.  Everything else you can fix later, but you\ncan't fix the location.<br/><br/>I think it's important not just that the axioms be well chosen, \nbut that there be few of them.  Mathematicians have always felt \nthis way about axioms-- the fewer, the better-- and I think they're\nonto something.<br/><br/>At the very least, it has to be a useful exercise to look closely\nat the core of a language to see if there are any axioms that\ncould be weeded out.  I've found in my long career as a slob that\ncruft breeds cruft, and I've seen this happen in software as\nwell as under beds and in the corners of rooms.<br/><br/>I have a hunch that\nthe main branches of the evolutionary tree pass through the languages\nthat have the smallest, cleanest cores.\nThe more of a language you can write in itself,\nthe better.<br/><br/><br/><br/>Of course, I'm making a big assumption in even asking what\nprogramming languages will be like in a hundred years.\nWill we even be writing programs in a hundred years?  Won't\nwe just tell computers what we want them to do?<br/><br/>There hasn't been a lot of progress in that department\nso far.\nMy guess is that a hundred years from now people will\nstill tell computers what to do using programs we would recognize\nas such.  There may be tasks that we\nsolve now by writing programs and which in a hundred years\nyou won't have to write programs to solve, but I think\nthere will still be a good deal of\nprogramming of the type that we do today.<br/><br/>It may seem presumptuous to think anyone can predict what\nany technology will look like in a hundred years.  But\nremember that we already have almost fifty years of history behind us.\nLooking forward a hundred years is a graspable idea\nwhen we consider how slowly languages have evolved in the\npast fifty.<br/><br/>Languages evolve slowly because they're not really technologies.\nLanguages are notation.  A program is a formal description of \nthe problem you want a computer to solve for you.  So the rate\nof evolution in programming languages is more like the\nrate of evolution in mathematical notation than, say,\ntransportation or communications.\nMathematical notation does evolve, but not with the giant\nleaps you see in technology.<br/><br/><br/><br/>Whatever computers are made of in a hundred years, it seems  \nsafe to predict they will be much faster than\nthey are now.  If Moore's Law continues to put out, they will be 74\nquintillion (73,786,976,294,838,206,464) times faster.  That's kind of\nhard to imagine.  And indeed, the most likely prediction in the\nspeed department may be that Moore's Law will stop working.\nAnything that is supposed to double every eighteen months seems\nlikely to run up against some kind of fundamental limit eventually.\nBut I have no trouble believing that computers will be very much\nfaster. Even if they only end up being a paltry million\ntimes faster, that should change the ground rules for programming\nlanguages substantially.  Among other things, there\nwill be more room for what\nwould now be considered slow languages, meaning languages\nthat don't yield very efficient code.<br/><br/>And yet some applications will still demand speed.\nSome of the problems we want to solve with\ncomputers are created by computers; for example, the\nrate at which you have to process video images depends\non the rate at which another computer can\ngenerate them.  And there is another class of problems\nwhich inherently have an unlimited capacity to soak up cycles:\nimage rendering, cryptography, simulations.<br/><br/>If some applications can be increasingly inefficient while\nothers continue to demand all the speed the hardware can\ndeliver, faster computers will mean that languages have\nto cover an ever wider range of efficiencies.  We've seen\nthis happening already.  Current implementations of some\npopular new languages are shockingly wasteful by the\nstandards of previous decades.<br/><br/>This isn't just something that happens with programming\nlanguages.  It's a general historical trend.  As technologies improve,\neach generation can do things that the previous generation\nwould have considered wasteful.  People thirty years ago would\nbe astonished at how casually we make long distance phone calls.\nPeople a hundred years ago would be even more astonished that \na package would one day travel from Boston to New York via Memphis.<br/><br/><br/><br/>I can already tell you what's going to happen to all those extra\ncycles that faster hardware is going to give us in the   \nnext hundred years.  They're nearly all going to be wasted.<br/><br/>I learned to program when computer power was scarce.\nI can remember taking all the spaces out of my Basic programs\nso they would fit into the memory of a 4K TRS-80.  The\nthought of all this stupendously inefficient software\nburning up cycles doing the same thing over and over seems\nkind of gross to me.  But I think my intuitions here are wrong.  I'm\nlike someone who grew up poor, and can't bear to spend money\neven for something important, like going to the doctor.<br/><br/>Some kinds of waste really are disgusting.  SUVs, for example, would\narguably be gross even if they ran on a fuel which would never\nrun out and generated no pollution.  SUVs are gross because they're\nthe solution to a gross problem. (How to make minivans look more\nmasculine.)\nBut not all waste is bad.  Now that we have the infrastructure\nto support it, counting the minutes of your long-distance\ncalls starts to seem niggling.   If you have the\nresources, it's more elegant to think of all phone calls as\none kind of thing, no matter where the other person is.<br/><br/>There's good waste, and bad waste.  I'm interested\nin good waste-- the kind where, by spending more, we can get  \nsimpler designs.  How will we take advantage of the opportunities\nto waste cycles that we'll get from new, faster hardware?<br/><br/>The desire for speed is so deeply engrained in us, with \nour puny computers, that it will take a conscious effort\nto overcome it.  In language design, we should be consciously seeking out\nsituations where we can trade efficiency for even the\nsmallest increase in convenience.<br/><br/><br/><br/>Most data structures exist because of speed.  For example,\nmany languages today have both strings and lists.  Semantically, strings\nare more or less a subset of lists in which the elements are\ncharacters.  So why do you need a separate data type?\nYou don't, really.  Strings only\nexist for efficiency.  But it's lame to clutter up the semantics\nof the language with hacks to make programs run faster.\nHaving strings in a language seems to be a case of\npremature optimization.<br/><br/>If we think of the core of a language as a set of axioms,  \nsurely it's gross to have additional axioms that add no expressive\npower, simply for the sake of efficiency.  Efficiency is\nimportant, but I don't think that's the right way to get it.<br/><br/>The right way to solve that problem, I think, is to separate\nthe meaning of a program from the implementation details. \nInstead of having both lists and strings, have just lists,\nwith some way to give the compiler optimization advice that \nwill allow it to lay out strings as contiguous bytes if\nnecessary.<br/><br/>Since speed doesn't matter in most of a program, you won't\nordinarily need to bother with\nthis sort of micromanagement.\nThis will be more and more true as computers get faster.<br/><br/><br/><br/>Saying less about implementation should also make programs\nmore flexible.\nSpecifications change while a program is being written, and this is not\nonly inevitable, but desirable.<br/><br/>The word \"essay\" comes\nfrom the French verb \"essayer\", which means \"to try\".\nAn essay, in the original sense, is something you\nwrite to try to figure something out.  This happens in\nsoftware too.  I think some of the best programs were essays,\nin the sense that the authors didn't know when they started\nexactly what they were trying to write.<br/><br/>Lisp hackers already know about the value of being flexible\nwith data structures.  We tend to write the first version of\na program so that it does everything with lists.  These\ninitial versions can be so shockingly inefficient that it\ntakes a conscious effort not to think about what they're\ndoing, just as, for me at least, eating a steak requires a\nconscious effort not to think where it came from.<br/><br/>What programmers in a hundred years will be looking for, most of\nall, is a language where you can throw together an unbelievably\ninefficient version 1 of a program with the least possible\neffort.  At least, that's how we'd describe it in present-day\nterms.  What they'll say is that they want a language that's\neasy to program in.<br/><br/>Inefficient software isn't gross.  What's gross is a language\nthat makes programmers do needless work.  Wasting programmer time\nis the true inefficiency, not wasting machine time.  This will\nbecome ever more clear as computers get faster.<br/><br/><br/><br/>I think getting rid of strings is already something we\ncould bear to think about.  We did it in <a href=\"arc.html\">Arc</a>, and it seems\nto be a win;  some operations that would be awkward to\ndescribe as regular expressions can be described\neasily as recursive functions.<br/><br/>How far will this flattening of data structures go?  I can think\nof possibilities that shock even me, with my conscientiously broadened\nmind.  Will we get rid of arrays, for example?  After all, they're\njust a subset of hash tables where the keys are vectors of\nintegers.   Will we replace hash tables themselves with lists?<br/><br/>There are more shocking prospects even than that.  The Lisp\nthat McCarthy described in 1960, for example, didn't\nhave numbers.  Logically, you don't need to have a separate notion\nof numbers, because you can represent them as lists:  the integer\nn could be represented as a list of n elements.  You can do math this\nway.  It's just unbearably inefficient.<br/><br/>No one actually proposed implementing numbers as lists in\npractice.  In fact, McCarthy's 1960 paper was not, at the time,\nintended to be implemented at all.  It was a <a href=\"rootsoflisp.html\">theoretical exercise</a>,\nan attempt to create a more elegant alternative to the Turing\nMachine.  When someone did, unexpectedly, take this paper and\ntranslate it into a working Lisp interpreter, numbers certainly\nweren't represented as lists; they were represented in binary,\nas in every other language.<br/><br/>Could a programming language go so far as to get rid of numbers\nas a fundamental data type?  I ask this not so much as a serious\nquestion as as a way to play chicken with the future.  It's like\nthe hypothetical case of an irresistible force meeting an \nimmovable object-- here, an unimaginably inefficient\nimplementation meeting unimaginably great resources.\nI don't see why not.  The future is pretty long.  If there's\nsomething we can do to decrease the number of axioms in the core\nlanguage, that would seem to be the side to bet on as t approaches\ninfinity.  If the idea still seems unbearable in a hundred years,\nmaybe it won't in a thousand.<br/><br/>Just to be clear about this, I'm not proposing that all numerical\ncalculations would actually be carried out using lists.  I'm proposing\nthat the core language, prior to any additional notations about\nimplementation, be defined this way.  In practice any program\nthat wanted to do any amount of math would probably represent\nnumbers in binary, but this would be an optimization, not part of\nthe core language semantics.<br/><br/><br/><br/>Another way to burn up cycles is to have many layers of\nsoftware between the application and the hardware.  This too is\na trend we see happening already: many recent languages are\ncompiled into byte code.  Bill Woods once told me that,\nas a rule of thumb, each layer of interpretation costs a\nfactor of 10 in speed.  This extra cost buys you flexibility.<br/><br/>The very first version of Arc was an extreme case of this sort\nof multi-level slowness, with corresponding benefits.  It\nwas a classic \"metacircular\" interpreter written\non top of Common Lisp, with a definite family resemblance\nto the eval function defined in McCarthy's original Lisp paper.\nThe whole thing was only a couple hundred lines of\ncode, so it was very easy to understand and change.  The \nCommon Lisp we used, CLisp, itself runs on top\nof a byte code interpreter.  So here we had two levels of\ninterpretation, one of them (the top one) shockingly inefficient,\nand the language was usable.  Barely usable, I admit, but\nusable.<br/><br/>Writing software as multiple layers is a powerful technique\neven within applications.  Bottom-up programming means writing\na program as a series of layers, each of which serves as a\nlanguage for the one above.  This approach tends to yield\nsmaller, more flexible programs.  It's also the best route to   \nthat holy grail, reusability.  A language is by definition\nreusable.  The more\nof your application you can push down into a language for writing\nthat type of application, the more of your software will be \nreusable.<br/><br/>Somehow the idea of reusability got attached\nto object-oriented programming in the 1980s, and no amount of\nevidence to the contrary seems to be able to shake it free.  But\nalthough some object-oriented software is reusable, what makes\nit reusable is its bottom-upness, not its object-orientedness.\nConsider libraries: they're reusable because they're language,\nwhether they're written in an object-oriented style or not.<br/><br/>I don't predict the demise of object-oriented programming, by the\nway.  Though I don't think it has much to offer good programmers,\nexcept in certain specialized domains, it is irresistible to   \nlarge organizations.  Object-oriented programming\noffers a sustainable way to write spaghetti code.  It lets you accrete\nprograms as a series of patches.\n<!--, without the effort of understanding, \nor the risk of breaking, existing code. -->\nLarge organizations\nalways tend to develop software this way, and I expect this\nto be as true in a hundred years as it is today.<br/><br/><br/><br/>\nAs long as we're talking about the future, we had better\ntalk about parallel computation, because that's where this \nidea seems to live.  That is, no matter when you're talking, parallel\ncomputation seems to be something that is going to happen\nin the future.<br/><br/>Will the future ever catch up with it?  People have been\ntalking about parallel computation as something imminent \nfor at least 20\nyears, and it hasn't affected programming practice much so far.\nOr hasn't it?  Already\nchip designers have to think about it, and so must\npeople trying to write systems software on multi-cpu computers.<br/><br/>The real question is, how far up the ladder of abstraction will\nparallelism go?\nIn a hundred years will it affect even application programmers?  Or\nwill it be something that compiler writers think about, but\nwhich is usually invisible in the source code of applications?<br/><br/>One thing that does seem likely is that most opportunities for\nparallelism will be wasted.  This is a special case of my more   \ngeneral prediction that most of the extra computer power we're\ngiven will go to waste.  I expect that, as with the stupendous\nspeed of the underlying hardware, parallelism will be something\nthat is available if you ask for it explicitly, but ordinarily\nnot used.  This implies that the kind of parallelism we have in\na hundred years will not, except in special applications, be\nmassive parallelism.  I expect for\nordinary programmers it will be more like being able to fork off\nprocesses that all end up running in parallel.<br/><br/>And this will, like asking for specific implementations of data\nstructures, be something that you do fairly late in the life of a\nprogram, when you try to optimize it.  Version 1s will ordinarily\nignore any advantages to be got from parallel computation, just\nas they will ignore advantages to be got from specific representations\nof data.<br/><br/>Except in special kinds of applications, parallelism won't\npervade the programs that are written in a hundred years.  It would be\npremature optimization if it did.<br/><br/><br/><br/>How many programming languages will there\nbe in a hundred years?  There seem to be a huge number of new\nprogramming languages lately.  Part of the reason is that\nfaster hardware has allowed programmers to make different\ntradeoffs between speed and convenience, depending on the\napplication.  If this is a real trend, the hardware we'll  \nhave in a hundred years should only increase it.<br/><br/>And yet there may be only a few widely-used languages in a\nhundred years.  Part of the reason I say this\nis optimism: it seems that, if you did a really good job,\nyou could make a language that was ideal for writing a   \nslow version 1, and yet with the right optimization advice\nto the compiler, would also yield very fast code when necessary.\nSo, since I'm optimistic, I'm going to predict that despite\nthe huge gap they'll have between acceptable and maximal\nefficiency, programmers in a hundred years will have languages \nthat can span most of it.<br/><br/>As this gap widens, profilers will become increasingly important.\nLittle attention is paid to profiling now.  Many people still\nseem to believe that the way to get fast applications is to\nwrite compilers that generate fast code.  As the gap between    \nacceptable and maximal performance widens, it will become\nincreasingly clear that the way to get fast applications is   \nto have a good guide from one to the other.<br/><br/>When I say there may only be a few languages, I'm not including\ndomain-specific \"little languages\".  I think such embedded languages\nare a great idea, and I expect them to proliferate.  But I expect\nthem to be written as thin enough skins that users can see\nthe general-purpose language underneath.<br/><br/><br/><br/>Who will design the languages of the future?  One of the most exciting\ntrends in the last ten years has been the rise of open-source  \nlanguages like Perl, Python, and Ruby.\nLanguage design is being taken over by hackers.  The results\nso far are messy, but encouraging.  There are some stunningly  \nnovel ideas in Perl, for example. Many are stunningly bad, but\nthat's always true of ambitious efforts.  At its current rate\nof mutation, God knows what Perl might evolve into in a hundred\nyears.<br/><br/>It's not true that those who can't do, teach (some of the best\nhackers I know are professors), but it is true that there are a\nlot of things that those who teach can't do.  <a href=\"desres.html\">Research</a> imposes\nconstraining caste restrictions.  In any academic\nfield there are topics that are ok to work on and others that\naren't.  Unfortunately the distinction between acceptable and\nforbidden topics is usually based on how intellectual\nthe work sounds when described in research papers, rather than\nhow important it is for getting good results.  The extreme case\nis probably literature; people studying literature rarely  \nsay anything that would be of the slightest use to those\nproducing it.<br/><br/>Though the situation is better in the sciences,\nthe overlap between the kind of work you're allowed to do and the\nkind of work that yields good languages is distressingly small.\n(Olin Shivers has grumbled eloquently\nabout this.)  For example, types seem to be an inexhaustible source\nof research papers, despite the fact that static typing\nseems to preclude true macros-- without which, in my opinion, no\nlanguage is worth using.<br/><br/>The trend is not merely toward languages being developed\nas open-source projects rather than \"research\", but toward\nlanguages being designed by the application programmers who need\nto use them, rather than by compiler writers.  This seems a good\ntrend and I expect it to continue.<br/><br/><br/><br/>\nUnlike physics in a hundred years, which is almost necessarily\nimpossible to predict, I think it may be possible in principle\nto design a language now that would appeal to users in a hundred\nyears.<br/><br/>One way to design a language is to just write down the program\nyou'd like to be able to write, regardless of whether there \nis a compiler that can translate it or hardware that can run it.\nWhen you do this you can assume unlimited resources.  It seems\nlike we ought to be able to imagine unlimited resources as well\ntoday as in a hundred years.<br/><br/>What program would one like to write?  Whatever is least work.\nExcept not quite: whatever <i>would be</i> least work if your ideas about\nprogramming weren't already influenced by the languages you're \ncurrently used to.  Such influence can be so pervasive that   \nit takes a great effort to overcome it.  You'd think it would\nbe obvious to creatures as lazy as us how to express a program\nwith the least effort.  In fact, our ideas about what's possible\ntend to be so <a href=\"avg.html\">limited</a> by whatever language we think in  that\neasier formulations of programs seem very surprising.  They're\nsomething you have to discover, not something you naturally\nsink into.<br/><br/>One helpful trick here\nis to use the <a href=\"power.html\">length</a> of the program as an approximation for\nhow much work it is to write.  Not the length in characters,\nof course, but the length in distinct syntactic elements-- basically,\nthe size of the parse tree.  It may not be quite true that\nthe shortest program is the least work to write, but it's\nclose enough that you're better off aiming for the solid\ntarget of brevity than the fuzzy, nearby one of least work.\nThen the algorithm for language design becomes: look at a program\nand ask, is there any way to write this that's shorter?<br/><br/>In practice, writing programs in an imaginary hundred-year\nlanguage will work to varying degrees depending\non how close you are to the core.  Sort routines you can\nwrite now.  But it would be\nhard to predict now what kinds of libraries might be needed in\na hundred years.  Presumably many libraries will be for domains that\ndon't even exist yet.  If SETI@home works, for example, we'll  \nneed libraries for communicating with aliens.  Unless of course\nthey are sufficiently advanced that they already communicate\nin XML.<br/><br/>At the other extreme, I think you might be able to design the\ncore language today.  In fact, some might argue that it was already\nmostly designed in 1958.<br/><br/><br/><br/>If the hundred year language were available today, would we\nwant to program in it?  One way to answer this question is to\nlook back.  If present-day programming languages had been available\nin 1960, would anyone have wanted to use them?<br/><br/>In some ways, the answer is no.  Languages today assume\ninfrastructure that didn't exist in 1960.  For example, a language\nin which indentation is significant, like Python, would not\nwork very well on printer terminals.  But putting such problems\naside-- assuming, for example, that programs were all just\nwritten on paper-- would programmers of the 1960s have liked\nwriting programs in the languages we use now?<br/><br/>I think so.\nSome of the less imaginative ones,\nwho had artifacts of early languages built into their ideas of  \nwhat a program was, might have had trouble.  (How can you manipulate\ndata without doing pointer arithmetic?  How can you implement \nflow charts without gotos?)  But I think the smartest programmers\nwould have had no trouble making the most of present-day\nlanguages, if they'd had them.<br/><br/>If we had the hundred-year language now, it would at least make a\ngreat pseudocode.  What about using it to write software?   \nSince the hundred-year language\nwill need to generate fast code for some applications, presumably\nit could generate code efficient enough to run acceptably well\non our hardware.  We might have to give more optimization advice\nthan users in a hundred years, but it still might be a net win.<br/><br/><br/><br/>Now we have two ideas that, if you combine them, suggest interesting\npossibilities: (1) the hundred-year language could, in principle, be\ndesigned today, and (2) such a language, if it existed, might be good to\nprogram in today.  When you see these ideas laid out like that,\nit's hard not to think, why not try writing the hundred-year language\nnow?<br/><br/>When you're working on language design, I think it is good to\nhave such a target and to keep it consciously in mind.  When you\nlearn to drive, one of the principles they teach you is to\nalign the car not by lining up the hood with the stripes painted\non the road, but by aiming at some point in the distance.  Even\nif all you care about is what happens in the next ten feet, this\nis the right answer.  I\nthink we can and should do the same thing with programming languages.<br/><br/><br/><br/>\n<b>Notes</b><br/><br/>I believe Lisp Machine Lisp was the first language to embody\nthe principle that declarations (except those of dynamic variables)\nwere merely optimization advice,\nand would not change the meaning of a correct program.  Common Lisp\nseems to have been the first to state this explicitly.<br/><br/><b>Thanks</b> to Trevor Blackwell, Robert Morris, and Dan Giffin for\nreading drafts of this, and to Guido van Rossum, Jeremy Hylton, and the\nrest of the Python crew for inviting me to speak at PyCon.<br/><br/><br clear=\"all\"/></font>","date":"2003-04-01T00:00:00Z"}