{"href":"better.html","title":"Better Bayesian Filtering","content":"<font face=\"verdana\" size=\"2\">January 2003<br/><br/><i>(This article was given as a talk at the 2003 Spam Conference.\nIt describes the work I've done to improve the performance of\nthe algorithm described in <a href=\"https://paulgraham.com/spam.html\">A Plan for Spam</a>,\nand what I plan to do in the future.)</i><br/><br/>The first discovery I'd like to present here is an algorithm for\nlazy evaluation of research papers.  Just\nwrite whatever you want and don't cite any previous work, and\nindignant readers will send you references to all the papers you\nshould have cited.   I discovered this algorithm\nafter ``A Plan for Spam'' [1] was on Slashdot.<br/><br/>Spam filtering is a subset of text classification,\nwhich is a well established field, but the first papers about\nBayesian\nspam filtering per se seem to have been two\ngiven at the same conference in 1998,\none by Pantel and Lin [2],\nand another by a group from\nMicrosoft Research [3].<br/><br/>When I heard about this work I was a bit surprised.  If\npeople had been onto Bayesian filtering four years ago,\nwhy wasn't everyone using it?\nWhen I read the papers I found out why.  Pantel and Lin's filter was the\nmore effective of the two, but it\nonly caught 92% of spam, with 1.16% false positives.<br/><br/>When I tried writing a Bayesian spam filter,\nit caught 99.5% of spam with less than .03% false\npositives [4].\nIt's always alarming when two people\ntrying the same experiment get widely divergent results.\nIt's especially alarming here because those two sets of numbers\nmight yield opposite conclusions.\nDifferent users have different requirements, but I think for\nmany people a filtering rate of 92% with 1.16% false positives means\nthat filtering is not an acceptable solution, whereas\n99.5% with less than .03% false positives means that it is.<br/><br/>So why did we get such different numbers?\nI haven't tried to reproduce Pantel and Lin's results, but\nfrom reading the paper I see five things that probably account\nfor the difference.<br/><br/>One is simply that they trained their filter on very little\ndata: 160 spam and 466 nonspam mails.\nFilter performance should still be climbing with data\nsets that small.  So their numbers may not even be an accurate\nmeasure of the performance of their algorithm, let alone of\nBayesian spam filtering in general.<br/><br/>But I think the most important difference is probably\nthat they ignored message headers.  To anyone who has worked\non spam filters, this will seem a perverse decision.\nAnd yet in the very first filters I tried writing, I ignored the\nheaders too.  Why?  Because I wanted to keep the problem neat.\nI didn't know much about mail headers then, and they seemed to me\nfull of random stuff.  There is a lesson here for filter\nwriters: don't ignore data.  You'd think this lesson would\nbe too obvious to mention, but I've had to learn it several times.<br/><br/>Third, Pantel and Lin stemmed the tokens, meaning they reduced e.g. both\n``mailing'' and ``mailed'' to the root ``mail''.   They may\nhave felt they were forced to do this by the small size\nof their corpus, but if so this is a kind of premature \noptimization.<br/><br/>Fourth, they calculated probabilities differently.\nThey used all the tokens, whereas I only\nuse the 15 most significant.  If you use all the tokens\nyou'll tend to miss longer spams, the type where someone tells you their life\nstory up to the point where they got rich from some multilevel\nmarketing scheme.  And such an algorithm\nwould be easy for spammers to spoof: just add a big\nchunk of random text to counterbalance the spam terms.<br/><br/>Finally, they didn't bias against false positives.\nI think\nany spam filtering algorithm ought to have a convenient\nknob you can twist to decrease the\nfalse positive rate at the expense of the filtering rate.\nI do this by counting the occurrences\nof tokens in the nonspam corpus double.  \n<!--ref to Woodhead's graphs--><br/><br/>I don't think it's a good idea to treat spam filtering as\na straight text classification problem.  You can use\ntext classification techniques, but solutions can and should\nreflect the fact that the text is email, and spam\nin particular.  Email is not just text; it has structure.\nSpam filtering is not just classification, because\nfalse positives are so much worse than false negatives\nthat you should treat them as a different kind of error.\nAnd the source of error is not just random variation, but\na live human spammer working actively to defeat your filter.<br/><br/><b>Tokens</b><br/><br/>Another project I heard about\nafter the Slashdot article was Bill Yerazunis' \n<a href=\"http://crm114.sourceforge.net\">CRM114</a> [5].\nThis is the counterexample to the design principle I\njust mentioned.  It's a straight text classifier,\nbut such a stunningly effective one that it manages to filter\nspam almost perfectly without even knowing that's\nwhat it's doing.<br/><br/>Once I understood how CRM114 worked, it seemed\ninevitable that I would eventually have to move from filtering based\non single words to an approach like this.  But first, I thought,\nI'll see how far I can get with single words.  And the answer is,\nsurprisingly far.<br/><br/>Mostly I've been working on smarter tokenization.  On\ncurrent spam, I've been able to achieve filtering rates that\napproach CRM114's.  These techniques are mostly orthogonal to Bill's;\nan optimal solution might incorporate both.<br/><br/>``A Plan for Spam'' uses a very simple\ndefinition of a token.  Letters, digits, dashes, apostrophes,\nand dollar signs are constituent characters, and everything\nelse is a token separator.  I also ignored case.<br/><br/>Now I have a more complicated definition of a token:\n<ol>\n<li> Case is preserved.<br/><br/><li> Exclamation points are constituent characters.<br/><br/><li> Periods and commas are constituents if they occur\n between two digits.  This lets me get ip addresses\n and prices intact.<br/><br/><li> A price range like $20-25 yields two tokens,\n $20 and $25.<br/><br/><li> Tokens that occur within the\n To, From, Subject, and Return-Path lines, or within urls,\n get marked accordingly.  E.g. ``foo'' in the Subject line\n becomes ``Subject*foo''.  (The asterisk could\n be any character you don't allow as a constituent.)\n</li></li></li></li></li></ol>\nSuch measures increase the filter's vocabulary, which\nmakes it more discriminating.  For example, in the current\nfilter, ``free'' in the Subject line\nhas a spam probability of 98%, whereas the same token\nin the body has a spam probability of only 65%.<br/><br/>Here are some of the current probabilities [6]:<br/><br/><font face=\"courier\"><pre>\nSubject*FREE      0.9999\nfree!!            0.9999\nTo*free           0.9998\nSubject*free      0.9782\nfree!             0.9199\nFree              0.9198\nUrl*free          0.9091\nFREE              0.8747\nFrom*free         0.7636\nfree              0.6546\n</pre></font>\nIn the Plan for Spam filter, all these tokens would have had the\nsame probability, .7602.  That filter recognized about 23,000\ntokens.  The current one recognizes about 187,000.<br/><br/>The disadvantage of having a larger universe of tokens\nis that there is more\nchance of misses.\nSpreading your corpus out over more tokens\nhas the same effect as making it smaller.\nIf you consider exclamation points as\nconstituents, for example, then you could end up\nnot having a spam probability for free with seven exclamation\npoints, even though you know that free with just two   \nexclamation points has a probability of 99.99%.<br/><br/>One solution to this is what I call degeneration.  If you\ncan't find an exact match for a token,\ntreat it as if it were a less specific\nversion.  I consider terminal exclamation\npoints, uppercase letters, and occurring in one of the\nfive marked contexts as making a token more specific.\nFor example, if I don't find a probability for\n``Subject*free!'', I look for probabilities for\n``Subject*free'', ``free!'', and ``free'', and take whichever one\nis farthest from .5.<br/><br/>Here are the alternatives [7]\nconsidered if the filter sees ``FREE!!!'' in the\nSubject line and doesn't have a probability for it.<br/><br/><font face=\"courier\"><pre>\nSubject*Free!!!\nSubject*free!!!\nSubject*FREE!\nSubject*Free!\nSubject*free!\nSubject*FREE\nSubject*Free\nSubject*free\nFREE!!!\nFree!!!\nfree!!!\nFREE!\nFree!\nfree!\nFREE\nFree\nfree              \n</pre></font>\nIf you do this, be sure to consider versions with initial\ncaps as well as all uppercase and all lowercase.  Spams\ntend to have more sentences in imperative mood, and in\nthose the first word is a verb.  So verbs with initial caps\nhave higher spam probabilities than they would in all \nlowercase.  In my filter, the spam probability of ``Act''\nis 98% and for ``act'' only 62%.<br/><br/>If you increase your filter's vocabulary, you can end up\ncounting the same word multiple times, according to your old\ndefinition of ``same''.\nLogically, they're not the\nsame token anymore.  But if this still bothers you, let\nme add from experience that the words you seem to be\ncounting multiple times tend to be exactly the ones you'd\nwant to.<br/><br/>Another effect of a larger vocabulary is that when you\nlook at an incoming mail you find more interesting tokens,\nmeaning those with probabilities far from .5.  I use the\n15 most interesting to decide if mail is spam.\nBut you can run into a problem when you use a fixed number\nlike this.  If you find a lot of maximally interesting tokens,\nthe result can end up being decided by whatever random factor\ndetermines the ordering of equally interesting tokens.\nOne way to deal with this is to treat some\nas more interesting than others.<br/><br/>For example, the\ntoken ``dalco'' occurs 3 times in my spam corpus and never\nin my legitimate corpus.  The token ``Url*optmails''\n(meaning ``optmails'' within a url) occurs 1223 times.\nAnd yet, as I used to calculate probabilities for tokens,\nboth would have the same spam probability, the threshold of .99.<br/><br/>That doesn't feel right.  There are theoretical\narguments for giving these two tokens substantially different\nprobabilities (Pantel and Lin do), but I haven't tried that yet.\nIt does seem at least that if we find more than 15 tokens\nthat only occur in one corpus or the other, we ought to\ngive priority to the ones that occur a lot.  So now\nthere are two threshold values.  For tokens that occur only\nin the spam corpus, the probability is .9999 if they\noccur more than 10 times and .9998 otherwise.  Ditto\nat the other end of the scale for tokens found\nonly in the legitimate corpus.<br/><br/>I may later scale token probabilities substantially,\nbut this tiny amount of scaling at least ensures that \ntokens get sorted the right way.<br/><br/>Another possibility would be to consider not\njust 15 tokens, but all the tokens over a certain\nthreshold of interestingness.  Steven Hauser does this\nin his statistical spam filter [8].\nIf you use a threshold, make it very high, or\nspammers could spoof you by packing messages with\nmore innocent words.<br/><br/>Finally, what should one do\nabout html?  I've tried the whole spectrum of options, from\nignoring it to parsing it all.  Ignoring html is a bad idea,\nbecause it's full of useful spam signs.  But if you parse \nit all, your filter might degenerate into a mere html   \nrecognizer.  The most effective approach\nseems to be the middle course, to notice some tokens but not\nothers.  I look at a, img, and font tags, and ignore the\nrest.  Links and images you should certainly look at, because\nthey contain urls.<br/><br/>I could probably be smarter about dealing with html, but I\ndon't think it's worth putting a lot of time into this.\nSpams full of html are easy to filter.  The smarter\nspammers already avoid it.  So\nperformance in the future should not depend much on how\nyou deal with html.<br/><br/><b>Performance</b><br/><br/>Between December 10 2002 and January 10 2003 I got about\n1750 spams.  \nOf these, 4 got through.  That's a filtering\nrate of about 99.75%.<br/><br/>Two of the four spams I missed got through because they\nhappened to use words that occur often in my legitimate\nemail.<br/><br/>The third was one of those that exploit\nan insecure cgi script to send mail to third parties.\nThey're hard to filter based just\non the content because the headers are innocent and   \nthey're careful about the words they use.  Even so I can\nusually catch them.  This one squeaked by with a\nprobability of .88, just under the threshold of .9.<br/><br/>Of course, looking at multiple token sequences\nwould catch it easily.  ``Below is the result of\nyour feedback form'' is an instant giveaway.<br/><br/>The fourth spam was what I call\na spam-of-the-future, because this is what I expect spam to\nevolve into: some completely neutral\ntext followed by a url.  In this case it was was from\nsomeone saying they had finally finished their homepage\nand would I go look at it.  (The page was of course an    \nad for a porn site.)<br/><br/>If the spammers are careful about the headers and use a\nfresh url, there is nothing in spam-of-the-future for filters\nto notice.  We can of course counter by sending a\ncrawler to look at the page.  But that might not be necessary.\nThe response rate for spam-of-the-future must\nbe low, or everyone would be doing it.\nIf it's low enough,\nit <a href=\"https://paulgraham.com/wfks.html\">won't pay</a> for spammers to send it, and we won't \nhave to work too hard on filtering it.<br/><br/>Now for the really shocking news: during that same one-month\nperiod I got <i>three</i> false positives.<br/><br/>In a way it's\na relief to get some false positives.  When I wrote ``A Plan\nfor Spam'' I hadn't had any, and I didn't know what they'd\nbe like.  Now that I've had a few, I'm relieved to find\nthey're not as bad as I feared.\nFalse positives yielded by statistical\nfilters turn out to be mails that sound a lot like spam, and\nthese tend to be the ones you would least mind missing [9].<br/><br/>Two of the false positives were newsletters\nfrom companies I've bought things from.  I never\nasked to receive them, so arguably they\nwere spams, but I count them as false positives because\nI hadn't been deleting them as spams before.  The reason\nthe filters caught them was that both companies in   \nJanuary switched to commercial email senders\ninstead of sending the mails from their own servers,  \nand both the headers and the bodies became much spammier.<br/><br/>The third false positive was a bad one, though.  It was \nfrom someone in Egypt and written in all uppercase.  This was\na direct result of making tokens case sensitive; the Plan\nfor Spam filter wouldn't have caught it.<br/><br/>It's hard to say what the overall false positive rate is,\nbecause we're up in the noise, statistically.\nAnyone who has worked on filters (at least, effective filters) will\nbe aware of this problem.\nWith some emails it's\nhard to say whether they're spam or not, and these are\nthe ones you end up looking at when you get filters       \nreally tight.  For example, so far the filter has\ncaught two emails that were sent to my address because\nof a typo, and one sent to me in the belief that I was \nsomeone else.  Arguably, these are neither my spam\nnor my nonspam mail.<br/><br/>Another false positive was from a vice president at Virtumundo.\nI wrote to them pretending to be a customer,\nand since the reply came back through Virtumundo's \nmail servers it had the most incriminating\nheaders imaginable.  Arguably this isn't a real false\npositive either, but a sort of Heisenberg uncertainty\neffect: I only got it because I was writing about spam  \nfiltering.<br/><br/>Not counting these, I've had a total of five false positives\nso far, out of about 7740 legitimate emails, a rate of .06%.\nThe other two were a notice that something I bought\nwas back-ordered, and a party reminder from Evite.<br/><br/>I don't think this number can be trusted, partly\nbecause the sample is so small, and partly because\nI think I can fix the filter not to catch\nsome of these.<br/><br/>False positives seem to me a different kind of error from\nfalse negatives.\nFiltering rate is a measure of performance.  False\npositives I consider more like bugs.  I approach improving the\nfiltering rate as optimization, and decreasing false\npositives as debugging.<br/><br/>So these five false positives are my bug list.  For example, \nthe mail from Egypt got nailed because the uppercase text\nmade it look to the filter like a Nigerian spam.\nThis really is kind of a bug.  As with\nhtml, the email being all uppercase is really conceptually <i>one</i>\nfeature, not one for each word.  I need to handle case in a\nmore sophisticated way.<br/><br/>So what to make of this .06%?  Not much, I think.  You could\ntreat it as an upper bound, bearing in mind the small sample size.\nBut at this stage it is more a measure of the bugs\nin my implementation than some intrinsic false positive rate\nof Bayesian filtering.<br/><br/><b>Future</b><br/><br/>What next?  Filtering is an optimization problem,\nand the key to optimization is profiling.  Don't\ntry to guess where your code is slow, because you'll\nguess wrong.  <i>Look</i> at where your code is slow,\nand fix that.  In filtering, this translates to:   \nlook at the spams you miss, and figure out what you\ncould have done to catch them.<br/><br/>For example, spammers are now working aggressively to   \nevade filters, and one of the things they're doing is\nbreaking up and misspelling words to prevent filters from\nrecognizing them.  But working on this is not my first\npriority, because I still have no trouble catching these\nspams [10].<br/><br/>There are two kinds of spams I currently do\nhave trouble with.\nOne is the type that pretends to be an email from \na woman inviting you to go chat with her or see her profile on a dating\nsite.  These get through because they're the one type of\nsales pitch you can make without using sales talk.  They use\nthe same vocabulary as ordinary email.<br/><br/>The other kind of spams I have trouble filtering are those\nfrom companies in e.g. Bulgaria offering contract programming \nservices.   These get through because I'm a programmer too, and\nthe spams are full of the same words as my real mail.<br/><br/>I'll probably focus on the personal ad type first.  I think if\nI look closer I'll be able to find statistical differences\nbetween these and my real mail.  The style of writing is\ncertainly different, though it may take multiword filtering\nto catch that.\nAlso, I notice they tend to repeat the url,\nand someone including a url in a legitimate mail wouldn't do that [11].<br/><br/>The outsourcing type are going to be hard to catch.  Even if \nyou sent a crawler to the site, you wouldn't find a smoking\nstatistical gun.\nMaybe the only answer is a central list of\ndomains advertised in spams [12].  But there can't be that\nmany of this type of mail.  If the only\nspams left were unsolicited offers of contract programming\nservices from Bulgaria, we could all probably move on to\nworking on something else.<br/><br/>Will statistical filtering actually get us to that point?\nI don't know.  Right now, for me personally, spam is\nnot a problem.  But spammers haven't yet made a serious\neffort to spoof statistical filters.  What will happen when they do?<br/><br/>I'm not optimistic about filters that work at the\nnetwork level [13].\nWhen there is a static obstacle worth getting past, spammers\nare pretty efficient at getting past it.  There\nis already a company called Assurance Systems that will\nrun your mail through Spamassassin and tell you whether \nit will get filtered out.<br/><br/>Network-level filters won't be completely useless.\nThey may be enough to kill all the \"opt-in\"\nspam, meaning spam from companies like Virtumundo and\nEqualamail who claim that they're really running opt-in lists.\nYou can filter those based just on the headers, no\nmatter what they say in the body.  But anyone willing to\nfalsify headers or use open relays, presumably including\nmost porn spammers, should be able to get some message past\nnetwork-level filters if they want to.  (By no means the\nmessage they'd like to send though, which is something.)<br/><br/>The kind of filters I'm optimistic about are ones that\ncalculate probabilities based on each individual user's mail.\nThese can be much more effective, not only in\navoiding false positives, but in filtering too: for example,\nfinding the recipient's email address base-64 encoded anywhere in\na message is a very good spam indicator.<br/><br/>But the real advantage of individual filters is that they'll all be\ndifferent.  If everyone's filters have different probabilities,\nit will make the spammers' optimization loop, what programmers\nwould call their edit-compile-test cycle, appallingly slow.  \nInstead of just tweaking a spam till it gets through a copy of\nsome filter they have on their desktop, they'll have to do a\ntest mailing for each tweak.  It would be like programming in\na language without an interactive toplevel, \nand I wouldn't wish that\non anyone.<br/><br/><br/><br/><b>Notes</b><br/><br/>[1]\nPaul Graham.  ``A Plan for Spam.'' August 2002.\nhttp://paulgraham.com/spam.html.<br/><br/>Probabilities in this algorithm are\ncalculated using a degenerate case of Bayes' Rule.  There are\ntwo simplifying assumptions: that the probabilities\nof features (i.e. words) are independent, and that we know\nnothing about the prior probability of an email being\nspam.<br/><br/>The first assumption is widespread in text classification.\nAlgorithms that use it are called ``naive Bayesian.''<br/><br/>The second assumption I made because the proportion of spam in\nmy incoming mail fluctuated so much from day to day (indeed,\nfrom hour to hour) that the overall prior ratio seemed\nworthless as a predictor.  If you assume that P(spam) and\nP(nonspam) are both .5, they cancel out and you can\nremove them from the formula.<br/><br/>If you were doing Bayesian filtering in a situation where  \nthe ratio of spam to nonspam was consistently very high or\n(especially) very low, you could probably improve filter\nperformance by incorporating prior probabilities.  To do\nthis right you'd have to track ratios by time of day, because\nspam and legitimate mail volume both have distinct daily\npatterns.<br/><br/>[2]\nPatrick Pantel and Dekang Lin. ``SpamCop-- A Spam\nClassification &amp; Organization Program.''  Proceedings of AAAI-98\nWorkshop on Learning for Text Categorization.<br/><br/>[3]\nMehran Sahami, Susan Dumais, David Heckerman and Eric Horvitz.\n``A Bayesian Approach to Filtering Junk E-Mail.'' Proceedings of AAAI-98\nWorkshop on Learning for Text Categorization.<br/><br/>[4] At the time I had zero false positives out of about 4,000 \nlegitimate emails.  If the next legitimate email was\na false positive, this would give us .03%.  These false positive\nrates are untrustworthy, as I explain later. I quote\na number here only to emphasize that whatever the false positive rate\nis, it is less than 1.16%.\n<!--As an indication of how widely divergent these results\nare, a filter that simply looked for the word ``click'' would in\nAugust 2002 catch 79.7% of my spam with 1.2% false positives.--><br/><br/>[5] Bill Yerazunis. ``Sparse Binary Polynomial Hash Message\nFiltering and The CRM114 Discriminator.''  Proceedings of 2003\nSpam Conference.<br/><br/>[6] In ``A Plan for Spam'' I used thresholds of .99 and .01.\nIt seems justifiable to use thresholds proportionate to the\nsize of the corpora.  Since I now have on the order of 10,000 of each\ntype of mail, I use .9999 and .0001.<br/><br/>[7] There is a flaw here I should probably fix.  Currently,\nwhen ``Subject*foo'' degenerates to just ``foo'', what that means is\nyou're getting the stats for occurrences of ``foo'' in\nthe body or header lines other than those I mark.\nWhat I should do is keep track of statistics for ``foo''\noverall as well as specific versions, and degenerate from\n``Subject*foo'' not to ``foo'' but to ``Anywhere*foo''.  Ditto for\ncase: I should degenerate from uppercase to any-case, not\nlowercase.<br/><br/>It would probably be a win to do this with prices\ntoo, e.g. to degenerate from ``$129.99'' to ``$--9.99'', ``$--.99'',\nand ``$--''.<br/><br/>You could also degenerate from words to their stems,\nbut this would probably only improve filtering rates early on \nwhen you had small corpora.<br/><br/>[8] Steven Hauser.  ``Statistical Spam Filter Works for Me.''\nhttp://www.sofbot.com.<br/><br/>[9] False positives are not all equal, and we should remember\nthis when comparing techniques for stopping spam.\nWhereas many of the false positives caused by filters\nwill be near-spams that you wouldn't mind missing,\nfalse positives caused by blacklists, for example, will be just\nmail from people who chose the wrong ISP.  In both\ncases you catch mail that's near spam, but for blacklists nearness\nis physical, and for filters it's textual.\n<!--\nIn fairness, it should be added that the new generation of\nresponsible blacklists, like the SBL, cause far fewer false positives than\nearlier blacklists like the MAPS RBL, for whom causing\nlarge numbers of false positives was a deliberate technique\nto get the attention of ISPs. --><br/><br/>[10] If spammers get good enough at obscuring tokens   \nfor this to be a problem, we can respond by simply removing\nwhitespace, periods, commas, etc.  and using a dictionary to\npick the words out of the resulting sequence.\nAnd of course finding words this way that weren't visible in\nthe original text would in itself be evidence of spam.<br/><br/>Picking out the words won't be trivial.  It will require \nmore than just reconstructing word boundaries; spammers\nboth add (``xHot nPorn cSite'') and omit (``P#rn'') letters.\nVision research may be useful here, since human vision is\nthe limit that such tricks will approach.<br/><br/>[11] \nIn general, spams are more repetitive than regular email.   \nThey want to pound that message home.  I currently don't\nallow duplicates in the top 15 tokens, because\nyou could get a false positive if the sender happens to use\nsome bad word multiple times. (In my current filter, ``dick'' has\na spam probabilty of .9999, but it's also a name.)\nIt seems we should at least notice duplication though,\nso I may try allowing up to two of each token, as Brian Burton does in\nSpamProbe.<br/><br/>[12]  This is what approaches like Brightmail's will\ndegenerate into once spammers are pushed into using mad-lib\ntechniques to generate everything else in the message.<br/><br/>[13]\nIt's sometimes argued that we should be working on filtering\nat the network level, because it is more efficient.  What people\nusually mean when they say this is: we currently filter at the\nnetwork level, and we don't want to start over from scratch.\nBut you can't dictate the problem to fit your solution.<br/><br/>Historically, scarce-resource arguments have been the losing\nside in debates about software design.\nPeople only tend to use them to justify choices\n(inaction in particular) made for other reasons.<br/><br/><b>Thanks</b> to Sarah Harlin, Trevor Blackwell, and\nDan Giffin for reading drafts of this paper, and to Dan again\nfor most of the infrastructure that this filter runs on.<br/><br/><br/><br/><b>Related:</b><br/><br/></font>","date":"2003-01-01T00:00:00Z"}