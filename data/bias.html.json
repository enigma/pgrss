{"href":"bias.html","title":"A Way to Detect Bias","content":"<font face=\"verdana\" size=\"2\">October 2015<br/><br/>This will come as a surprise to a lot of people, but in some cases\nit's possible to detect bias in a selection process without knowing\nanything about the applicant pool.  Which is exciting because among\nother things it means third parties can use this technique to detect\nbias whether those doing the selecting want them to or not.<br/><br/>You can use this technique whenever (a) you have at least\na random sample of the applicants that were selected, (b) their\nsubsequent performance is measured, and (c) the groups of\napplicants you're comparing have roughly equal distribution of ability.<br/><br/>How does it work?  Think about what it means to be biased.  What\nit means for a selection process to be biased against applicants\nof type x is that it's harder for them to make it through.  Which\nmeans applicants of type x have to be better to get selected than\napplicants not of type x.\n<font color=\"#dddddd\">[<a href=\"#f1n\"><font color=\"#dddddd\">1</font></a>]</font>\nWhich means applicants of type x\nwho do make it through the selection process will outperform other\nsuccessful applicants.  And if the performance of all the successful\napplicants is measured, you'll know if they do.<br/><br/>Of course, the test you use to measure performance must be a valid\none.  And in particular it must not be invalidated by the bias you're\ntrying to measure.\nBut there are some domains where performance can be measured, and\nin those detecting bias is straightforward. Want to know if the\nselection process was biased against some type of applicant?  Check\nwhether they outperform the others.  This is not just a heuristic\nfor detecting bias.  It's what bias means.<br/><br/>For example, many suspect that venture capital firms are biased\nagainst female founders. This would be easy to detect: among their\nportfolio companies, do startups with female founders outperform\nthose without?  A couple months ago, one VC firm (almost certainly\nunintentionally) published a study showing bias of this type. First\nRound Capital found that among its portfolio companies, startups\nwith female founders <a href=\"http://10years.firstround.com/#one\"><u>outperformed</u></a>\nthose without by 63%. \n<font color=\"#dddddd\">[<a href=\"#f2n\"><font color=\"#dddddd\">2</font></a>]</font><br/><br/>The reason I began by saying that this technique would come as a\nsurprise to many people is that we so rarely see analyses of this\ntype.  I'm sure it will come as a surprise to First Round that they\nperformed one. I doubt anyone there realized that by limiting their\nsample to their own portfolio, they were producing a study not of\nstartup trends but of their own biases when selecting companies.<br/><br/>I predict we'll see this technique used more in the future.  The\ninformation needed to conduct such studies is increasingly available.\nData about who applies for things is usually closely guarded by the\norganizations selecting them, but nowadays data about who gets\nselected is often publicly available to anyone who takes the trouble\nto aggregate it.<br/><br/><br/><br/><br/><br/><br/><br/>\n<b>Notes</b><br/><br/>[<a name=\"f1n\"><font color=\"#000000\">1</font></a>]\nThis technique wouldn't work if the selection process looked\nfor different things from different types of applicantsâ€”for\nexample, if an employer hired men based on their ability but women\nbased on their appearance.<br/><br/>[<a name=\"f2n\"><font color=\"#000000\">2</font></a>]\nAs Paul Buchheit points out, First Round excluded their most \nsuccessful investment, Uber, from the study.  And while it \nmakes sense to exclude outliers from some types of studies, \nstudies of returns from startup investing, which is all about \nhitting outliers, are not one of them.<br/><br/>\n<b>Thanks</b> to Sam Altman, Jessica Livingston, and Geoff Ralston for reading\ndrafts of this.<br/><br/></font>","date":"2015-10-01T00:00:00Z"}