{"href":"ffb.html","title":"Filters that Fight Back","content":"<font face=\"verdana\" size=\"2\">August 2003<br/><br/><!-- <i>(Originally this essay began with a discussion of filtering.\nAn expanded version of that discussion now exists on its own as\n<a href=\"sofar.html\">So Far, So Good</a>.)</i>\n-->\nWe may be able to improve the accuracy of Bayesian spam filters\nby having them follow links to see what's\nwaiting at the other end.  Richard Jowsey of\n<a href=\"http://death2spam.com\">death2spam</a> now does\nthis in borderline cases, and reports that it works well.<br/><br/>Why only do it in borderline cases?  And why only do it once?<br/><br/>As I mentioned in <a href=\"https://paulgraham.com/wfks.html\">Will Filters Kill Spam?</a>,\nfollowing all the urls in\na spam would have an amusing side-effect.  If popular email clients\ndid this in order to filter spam, the spammer's servers\nwould take a serious pounding.  The more I think about this,\nthe better an idea it seems.  This isn't just amusing; it\nwould be hard to imagine a more perfectly targeted counterattack\non spammers.<br/><br/>So I'd like to suggest an additional feature to those\nworking on spam filters: a \"punish\" mode which,\nif turned on, would spider every url\nin a suspected spam n times, where n could be set by the user. [1]<br/><br/>As many people have noted, one of the problems with the\ncurrent email system is that it's too passive.  It does\nwhatever you tell it.  So far all the suggestions for fixing\nthe problem seem to involve new protocols.  This one  \nwouldn't.<br/><br/>If widely used, auto-retrieving spam filters would make\nthe email system <i>rebound.</i>  The huge volume of the\nspam, which has so far worked in the spammer's favor,\nwould now work against him, like a branch snapping back in   \nhis face.   Auto-retrieving spam filters would drive the\nspammer's \n<a href=\"http://www.bork.ca/pics/?path=incoming&amp;img=bill.jpg\">costs</a> up, \nand his sales down:  his bandwidth usage\nwould go through the roof, and his servers would grind to a\nhalt under the load, which would make them unavailable\nto the people who would have responded to the spam.<br/><br/>Pump out a million emails an hour, get a\nmillion hits an hour on your servers.<br/><br/><!--Of course, if any of the urls\nare \"web bugs\" they'll suggest to the spammer that\nthe mail got opened, and may result in more spam.  (However,\nthey'll also tend to make \"open rates\" meaningless, thus\ndepriving the spammer of valuable information.)\nAnd of course, some links will be unsubscribe links.  The\nnet effect might be less spam.\n-->\nWe would want to ensure that this is only done to\nsuspected spams.  As a rule, any url sent to millions of\npeople is likely to be a spam url, so submitting every http\nrequest in every email would work fine nearly all the time.\nBut there are a few cases where this isn't true: the urls\nat the bottom of mails sent from free email services like\nYahoo Mail and Hotmail, for example.<br/><br/>To protect such sites, and to prevent abuse, auto-retrieval\nshould be combined with blacklists of spamvertised sites.\nOnly sites on a blacklist would get crawled, and\nsites would be blacklisted\nonly after being inspected by humans. The lifetime of a spam\nmust be several hours at least, so\nit should be easy to update such a list in time to\ninterfere with a spam promoting a new site. [2]<br/><br/>High-volume auto-retrieval would only be practical for users\non high-bandwidth\nconnections, but there are enough of those to cause spammers\nserious trouble.   Indeed, this solution neatly\nmirrors the problem.  The problem with spam is that in\norder to reach a few gullible people the spammer sends \nmail to everyone.  The non-gullible recipients\nare merely collateral damage.  But the non-gullible majority\nwon't stop getting spam until they can stop (or threaten to\nstop) the gullible\nfrom responding to it.  Auto-retrieving spam filters offer\nthem a way to do this.<br/><br/>Would that kill spam?  Not quite.  The biggest spammers\ncould probably protect their servers against auto-retrieving \nfilters.  However, the easiest and cheapest way for them\nto do it would be to include working unsubscribe links in   \ntheir mails.  And this would be a necessity for smaller fry,\nand for \"legitimate\" sites that hired spammers to promote\nthem.  So if auto-retrieving filters became widespread,\nthey'd become auto-unsubscribing filters.<br/><br/>In this scenario, spam would, like OS crashes, viruses, and\npopups, become one of those plagues that only afflict people\nwho don't bother to use the right software.<br/><br/><br/><br/>\n<b>Notes</b><br/><br/>[1] Auto-retrieving filters will have to follow redirects,\nand should in some cases (e.g. a page that just says\n\"click here\") follow more than one level of links.\nMake sure too that\nthe http requests are indistinguishable from those of\npopular Web browsers, including the order and referrer.<br/><br/>If the response\ndoesn't come back within x amount of time, default to\nsome fairly high spam probability.<br/><br/>Instead of making n constant, it might be a good idea to\nmake it a function of the number of spams that have been\nseen mentioning the site.  This would add a further level of\nprotection against abuse and accidents.<br/><br/>[2] The original version of this article used the term\n\"whitelist\" instead of \"blacklist\".  Though they were\nto work like blacklists, I preferred to call them whitelists\nbecause it might make them less vulnerable to legal attack.\nThis just seems to have confused readers, though.<br/><br/>There should probably be multiple blacklists.  A single point\nof failure would be vulnerable both to attack and abuse.<br/><br/><!--[6] I don't pretend to have worked out all the\n<a href=\"ffbfaq.html\">details</a> of this\nscheme.  I can't claim to be certain it will work.  (Who\ncould till it has been tried in practice?)  Fortunately,\nunlike solutions that require new protocols, this one can\nbe tested on a subset of the problem.  Why not try using   \nFFBs on, say, domains that begin with A?\nIf spammers start to avoid such domains, we'll know we're winning,\nand we can roll\ndown the rest of the alphabet one letter at a time.\n--><br/><br/>\n<b>Thanks</b> to Brian Burton, Bill Yerazunis, Dan Giffin,\nEric Raymond, and Richard Jowsey for reading drafts of this.<br/><br/></font>","date":"2003-08-01T00:00:00Z"}