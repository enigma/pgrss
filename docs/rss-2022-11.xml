<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Paul Graham's Essays</title>
    <link>https://enigma.github.io/pgrss/rss.xml</link>
    <description>Paul Graham's Essays</description>
    <atom:link href="https://enigma.github.io/pgrss/rss-2022-11.xml" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Tue, 01 Nov 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>The Need to Read</title>
      <link>https://paulgraham.com//read.html</link>
      <description>&lt;font face="verdana" size="2"&gt;November 2022&lt;br/&gt;&lt;br/&gt;In the science fiction books I read as a kid, reading had often
been replaced by some more efficient way of acquiring knowledge.
Mysterious "tapes" would load it into one's brain like a program
being loaded into a computer.&lt;br/&gt;&lt;br/&gt;That sort of thing is unlikely to happen anytime soon. Not just
because it would be hard to build a replacement for reading, but
because even if one existed, it would be insufficient. Reading about
x doesn't just t&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;November 2022&lt;br/&gt;&lt;br/&gt;In the science fiction books I read as a kid, reading had often
been replaced by some more efficient way of acquiring knowledge.
Mysterious "tapes" would load it into one's brain like a program
being loaded into a computer.&lt;br/&gt;&lt;br/&gt;That sort of thing is unlikely to happen anytime soon. Not just
because it would be hard to build a replacement for reading, but
because even if one existed, it would be insufficient. Reading about
x doesn't just teach you about x; it also teaches you how to write.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/read.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Would that matter? If we replaced reading, would anyone need to be
good at writing?&lt;br/&gt;&lt;br/&gt;The reason it would matter is that writing is not just a way to
convey ideas, but also a way to have them.&lt;br/&gt;&lt;br/&gt;A good writer doesn't just think, and then write down what he
thought, as a sort of transcript. A good writer will almost always
discover new things in the process of writing. And there is, as far
as I know, no substitute for this kind of discovery. Talking about
your ideas with other people is a good way to develop them. But
even after doing this, you'll find you still discover new things
when you sit down to write. There is a kind of thinking that can
only be done by &lt;a href="https://paulgraham.com/words.html"&gt;&lt;u&gt;writing&lt;/u&gt;&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;There are of course kinds of thinking that can be done without
writing. If you don't need to go too deeply into a problem, you can
solve it without writing. If you're thinking about how two pieces
of machinery should fit together, writing about it probably won't
help much. And when a problem can be described formally, you can
sometimes solve it in your head. But if you need to solve a
complicated, ill-defined problem, it will almost always help to
write about it. Which in turn means that someone who's not good at
writing will almost always be at a disadvantage in solving such
problems.&lt;br/&gt;&lt;br/&gt;You can't think well without writing well, and you can't write well
without reading well. And I mean that last "well" in both senses.
You have to be good at reading, and read good things.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/read.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;People who just want information may find other ways to get it.
But people who want to have ideas can't afford to.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
Audiobooks can give you examples of good writing, but having
them read to you doesn't teach you as much about writing as reading
them yourself.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
By "good at reading" I don't mean good at the mechanics of
reading. You don't have to be good at extracting words from the
page so much as extracting meaning from the words.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//read.html</guid>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What You (Want to)* Want</title>
      <link>https://paulgraham.com//want.html</link>
      <description>&lt;font face="verdana" size="2"&gt;November 2022&lt;br/&gt;&lt;br/&gt;Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;November 2022&lt;br/&gt;&lt;br/&gt;Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I wasn't responsible
for my actions. I gradually lost hope of that, but the puzzle
remained: How do you reconcile being a machine made of matter with
the feeling that you're free to choose what you do?
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/want.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The best way to explain the answer may be to start with a slightly
wrong version, and then fix it. The wrong version is: You can do
what you want, but you can't want what you want. Yes, you can control
what you do, but you'll do what you want, and you can't control
that.&lt;br/&gt;&lt;br/&gt;The reason this is mistaken is that people do sometimes change what
they want. People who don't want to want something — drug addicts,
for example — can sometimes make themselves stop wanting it. And
people who want to want something — who want to like classical
music, or broccoli — sometimes succeed.&lt;br/&gt;&lt;br/&gt;So we modify our initial statement: You can do what you want, but
you can't want to want what you want.&lt;br/&gt;&lt;br/&gt;That's still not quite true. It's possible to change what you want
to want. I can imagine someone saying "I decided to stop wanting
to like classical music." But we're getting closer to the truth.
It's rare for people to change what they want to want, and the more
"want to"s we add, the rarer it gets.&lt;br/&gt;&lt;br/&gt;We can get arbitrarily close to a true statement by adding more "want
to"s in much the same way we can get arbitrarily close to 1 by adding
more 9s to a string of 9s following a decimal point. In practice
three or four "want to"s must surely be enough. It's hard even to
envision what it would mean to change what you want to want to want
to want, let alone actually do it.&lt;br/&gt;&lt;br/&gt;So one way to express the correct answer is to use a regular
expression. You can do what you want, but there's some statement
of the form "you can't (want to)* want what you want" that's true.
Ultimately you get back to a want that you don't control.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/want.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
I didn't know when I was 9 that matter might behave randomly,
but I don't think it affects the problem much. Randomness destroys
the ghost in the machine as effectively as determinism.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
If you don't like using an expression, you can make the same
point using higher-order desires: There is some n such that you
don't control your nth-order desires.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;font color="888888"&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell,
Jessica Livingston, Robert Morris, and
Michael Nielsen for reading drafts of this.&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//want.html</guid>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Alien Truth</title>
      <link>https://paulgraham.com//alien.html</link>
      <description>&lt;font face="verdana" size="2"&gt;October 2022&lt;br/&gt;&lt;br/&gt;If there were intelligent beings elsewhere in the universe, they'd
share certain truths in common with us. The truths of mathematics
would be the same, because they're true by definition. Ditto for
the truths of physics; the mass of a carbon atom would be the same
on their planet. But I think we'd share other truths with aliens
besides the truths of math and physics, and that it would be
worthwhile to think about what these might be.&lt;br/&gt;&lt;br/&gt;F&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;October 2022&lt;br/&gt;&lt;br/&gt;If there were intelligent beings elsewhere in the universe, they'd
share certain truths in common with us. The truths of mathematics
would be the same, because they're true by definition. Ditto for
the truths of physics; the mass of a carbon atom would be the same
on their planet. But I think we'd share other truths with aliens
besides the truths of math and physics, and that it would be
worthwhile to think about what these might be.&lt;br/&gt;&lt;br/&gt;For example, I think we'd share the principle that a controlled
experiment testing some hypothesis entitles us to have proportionally
increased belief in it. It seems fairly likely, too, that it would
be true for aliens that one can get better at something by practicing.
We'd probably share Occam's razor. There doesn't seem anything
specifically human about any of these ideas.&lt;br/&gt;&lt;br/&gt;We can only guess, of course. We can't say for sure what forms
intelligent life might take. Nor is it my goal here to explore that
question, interesting though it is. The point of the idea of alien
truth is not that it gives us a way to speculate about what forms
intelligent life might take, but that it gives us a threshold, or
more precisely a target, for truth. If you're trying to find the
most general truths short of those of math or physics, then presumably
they'll be those we'd share in common with other forms of intelligent
life.&lt;br/&gt;&lt;br/&gt;Alien truth will work best as a heuristic if we err on the side of
generosity. If an idea might plausibly be relevant to aliens, that's
enough. Justice, for example. I wouldn't want to bet that all
intelligent beings would understand the concept of justice, but I
wouldn't want to bet against it either.&lt;br/&gt;&lt;br/&gt;The idea of alien truth is related to Erdos's idea of God's book.
He used to describe a particularly good proof as being in God's
book, the implication being (a) that a sufficiently good proof was
more discovered than invented, and (b) that its goodness would be
universally recognized. If there's such a thing as alien truth,
then there's more in God's book than math.&lt;br/&gt;&lt;br/&gt;What should we call the search for alien truth? The obvious choice
is "philosophy." Whatever else philosophy includes, it should
probably include this. I'm fairly sure Aristotle would have thought
so. One could even make the case that the search for alien truth
is, if not an accurate description &lt;i&gt;of&lt;/i&gt; philosophy, a good
definition &lt;i&gt;for&lt;/i&gt; it. I.e. that it's what people who call
themselves philosophers should be doing, whether or not they currently
are. But I'm not wedded to that; doing it is what matters, not what
we call it.&lt;br/&gt;&lt;br/&gt;We may one day have something like alien life among us in the form
of AIs. And that may in turn allow us to be precise about what
truths an intelligent being would have to share with us. We might
find, for example, that it's impossible to create something we'd
consider intelligent that doesn't use Occam's razor. We might one
day even be able to prove that. But though this sort of research
would be very interesting, it's not necessary for our purposes, or
even the same field; the goal of philosophy, if we're going to call it that, would be
to see what ideas we come up with using alien truth as a target,
not to say precisely where the threshold of it is. Those two questions might one
day converge, but they'll converge from quite different directions,
and till they do, it would be too constraining to restrict ourselves
to thinking only about things we're certain would be alien truths.
Especially since this will probably be one of those areas where the
best guesses turn out to be surprisingly close to optimal. (Let's
see if that one does.)&lt;br/&gt;&lt;br/&gt;Whatever we call it, the attempt to discover alien truths would be
a worthwhile undertaking. And curiously enough, that is itself
probably an alien truth.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Greg Brockman, 
Patrick Collison, Robert Morris, and Michael Nielsen for reading drafts of this.&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//alien.html</guid>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What I've Learned from Users</title>
      <link>https://paulgraham.com//users.html</link>
      <description>&lt;font face="verdana" size="2"&gt;September 2022&lt;br/&gt;&lt;br/&gt;I recently told applicants to Y Combinator that the best advice I
could give for getting in, per word, was 
&lt;blockquote&gt;
  Explain what you've learned from users.
&lt;/blockquote&gt;
That tests a lot of things: whether you're paying attention to
users, how well you understand them, and even how much they need
what you're making.&lt;br/&gt;&lt;br/&gt;Afterward I asked myself the same question. What have I learned
from YC's users, the startups we've funded?&lt;br/&gt;&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;September 2022&lt;br/&gt;&lt;br/&gt;I recently told applicants to Y Combinator that the best advice I
could give for getting in, per word, was 
&lt;blockquote&gt;
  Explain what you've learned from users.
&lt;/blockquote&gt;
That tests a lot of things: whether you're paying attention to
users, how well you understand them, and even how much they need
what you're making.&lt;br/&gt;&lt;br/&gt;Afterward I asked myself the same question. What have I learned
from YC's users, the startups we've funded?&lt;br/&gt;&lt;br/&gt;The first thing that came to mind was that most startups have the
same problems. No two have exactly the same problems, but it's
surprising how much the problems remain the same, regardless of
what they're making. Once you've advised 100 startups all doing
different things, you rarely encounter problems you haven't seen
before.&lt;br/&gt;&lt;br/&gt;This fact is one of the things that makes YC work. But I didn't
know it when we started YC. I only had a few data points: our own
startup, and those started by friends. It was a surprise to me how
often the same problems recur in different forms. Many later stage
investors might never realize this, because later stage investors
might not advise 100 startups in their whole career, but a YC partner
will get this much experience in the first year or two.&lt;br/&gt;&lt;br/&gt;That's one advantage of funding large numbers of early stage companies
rather than smaller numbers of later-stage ones. You get a lot of
data. Not just because you're looking at more companies, but also
because more goes wrong.&lt;br/&gt;&lt;br/&gt;But knowing (nearly) all the problems startups can encounter doesn't
mean that advising them can be automated, or reduced to a formula.
There's no substitute for individual office hours with a YC partner.
Each startup is unique, which means they have to be advised
by specific partners who know them well.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/users.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;We learned that the hard way, in the notorious "batch that broke
YC" in the summer of 2012. Up till that point we treated the partners
as a pool. When a startup requested office hours, they got the next
available slot posted by any partner. That meant every partner had
to know every startup. This worked fine up to 60 startups, but when
the batch grew to 80, everything broke. The founders probably didn't
realize anything was wrong, but the partners were confused and
unhappy because halfway through the batch they still didn't know
all the companies yet.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/users.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;At first I was puzzled. How could things be fine at 60 startups and
broken at 80? It was only a third more. Then I realized what had
happened. We were using an &lt;i&gt;O(n&lt;sup&gt;&lt;small&gt;2&lt;/small&gt;&lt;/sup&gt;)&lt;/i&gt; algorithm. So of course it blew
up.&lt;br/&gt;&lt;br/&gt;The solution we adopted was the classic one in these situations.
We sharded the batch into smaller groups of startups, each overseen
by a dedicated group of partners. That fixed the problem, and has
worked fine ever since. But the batch that broke YC was a powerful
demonstration of how individualized the process of advising startups
has to be.&lt;br/&gt;&lt;br/&gt;Another related surprise is how bad founders can be at realizing
what their problems are. Founders will sometimes come in to talk
about some problem, and we'll discover another much bigger one in
the course of the conversation. For example (and this case is all
too common), founders will come in to talk about the difficulties
they're having raising money, and after digging into their situation,
it turns out the reason is that the company is doing badly, and
investors can tell. Or founders will come in worried that they still
haven't cracked the problem of user acquisition, and the reason turns out
to be that their product isn't good enough. There have been times
when I've asked "Would you use this yourself, if you hadn't built
it?" and the founders, on thinking about it, said "No." Well, there's
the reason you're having trouble getting users.&lt;br/&gt;&lt;br/&gt;Often founders know what their problems are, but not their relative
importance.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/users.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
They'll come in to talk about three problems
they're worrying about. One is of moderate importance, one doesn't
matter at all, and one will kill the company if it isn't addressed
immediately. It's like watching one of those horror movies where
the heroine is deeply upset that her boyfriend cheated on her, and
only mildly curious about the door that's mysteriously ajar. You
want to say: never mind about your boyfriend, think about that door!
Fortunately in office hours you can. So while startups still die
with some regularity, it's rarely because they wandered into a room
containing a murderer. The YC partners can warn them where the
murderers are.&lt;br/&gt;&lt;br/&gt;Not that founders listen. That was another big surprise: how often
founders don't listen to us. A couple weeks ago I talked to a partner
who had been working for YC for a couple batches and was starting
to see the pattern. "They come back a year later," she said, "and
say 'We wish we'd listened to you.'"&lt;br/&gt;&lt;br/&gt;It took me a long time to figure out why founders don't listen. At
first I thought it was mere stubbornness. That's part of the reason,
but another and probably more important reason is that so much about
startups is &lt;a href="https://paulgraham.com/before.html"&gt;counterintuitive&lt;/a&gt;. 
And when you tell someone something
counterintuitive, what it sounds to them is wrong. So the reason
founders don't listen to us is that they don't &lt;i&gt;believe&lt;/i&gt; us. At
least not till experience teaches them otherwise.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/users.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The reason startups are so counterintuitive is that they're so
different from most people's other experiences. No one knows what
it's like except those who've done it. Which is why YC partners
should usually have been founders themselves. But strangely enough,
the counterintuitiveness of startups turns out to be another of the
things that make YC work. If it weren't counterintuitive, founders
wouldn't need our advice about how to do it.&lt;br/&gt;&lt;br/&gt;Focus is doubly important for early stage startups, because not
only do they have a hundred different problems, they don't have
anyone to work on them except the founders. If the founders focus
on things that don't matter, there's no one focusing on the things
that do. So the essence of what happens at YC is to figure out which
problems matter most, then cook up ideas for solving them — ideally
at a resolution of a week or less — and then try those ideas and
measure how well they worked. The focus is on action, with measurable,
near-term results.&lt;br/&gt;&lt;br/&gt;This doesn't imply that founders should rush forward regardless of
the consequences. If you correct course at a high enough frequency,
you can be simultaneously decisive at a micro scale and tentative
at a macro scale. The result is a somewhat winding path, but executed
very rapidly, like the path a running back takes downfield. And in
practice there's less backtracking than you might expect. Founders
usually guess right about which direction to run in, especially if
they have someone experienced like a YC partner to bounce their
hypotheses off. And when they guess wrong, they notice fast, because
they'll talk about the results at office hours the next week.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/users.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;A small improvement in navigational ability can make you a lot
faster, because it has a double effect: the path is shorter, and
you can travel faster along it when you're more certain it's the
right one. That's where a lot of YC's value lies, in helping founders
get an extra increment of focus that lets them move faster. And
since moving fast is the essence of a startup, YC in effect makes
startups more startup-like.&lt;br/&gt;&lt;br/&gt;Speed defines startups. Focus enables speed. YC improves focus.&lt;br/&gt;&lt;br/&gt;Why are founders uncertain about what to do? Partly because startups
almost by definition are doing something new, which means no one
knows how to do it yet, or in most cases even what "it" is. Partly
because startups are so counterintuitive generally. And partly
because many founders, especially young and ambitious ones, have
been trained to win the wrong way. That took me years to figure
out. The educational system in most countries trains you to win by
&lt;a href="https://paulgraham.com/lesson.html"&gt;hacking the test&lt;/a&gt; 
instead of actually doing whatever it's supposed
to measure. But that stops working when you start a startup. So
part of what YC does is to retrain founders to stop trying to hack
the test. (It takes a surprisingly long time. A year in, you still
see them reverting to their old habits.)&lt;br/&gt;&lt;br/&gt;YC is not simply more experienced founders passing on their knowledge.
It's more like specialization than apprenticeship. The knowledge
of the YC partners and the founders have different shapes: It
wouldn't be worthwhile for a founder to acquire the encyclopedic
knowledge of startup problems that a YC partner has, just as it
wouldn't be worthwhile for a YC partner to acquire the depth of
domain knowledge that a founder has. That's why it can still be
valuable for an experienced founder to do YC, just as it can still
be valuable for an experienced athlete to have a coach.&lt;br/&gt;&lt;br/&gt;The other big thing YC gives founders is colleagues, and this may
be even more important than the advice of partners. If you look at
history, great work clusters around certain places and institutions:
Florence in the late 15th century, the University of Göttingen in
the late 19th, &lt;i&gt;The New Yorker&lt;/i&gt; under Ross, Bell Labs, Xerox PARC.
However good you are, good colleagues make you better. Indeed, very
ambitious people probably need colleagues more than anyone else,
because they're so starved for them in everyday life.&lt;br/&gt;&lt;br/&gt;Whether or not YC manages one day to be listed alongside those
famous clusters, it won't be for lack of trying. We were very aware
of this historical phenomenon and deliberately designed YC to be
one. By this point it's not bragging to say that it's the biggest
cluster of great startup founders. Even people trying to attack YC
concede that.&lt;br/&gt;&lt;br/&gt;Colleagues and startup founders are two of the most powerful forces
in the world, so you'd expect it to have a big effect to combine
them. Before YC, to the extent people thought about the question
at all, most assumed they couldn't be combined — that loneliness
was the price of independence. That was how it felt to us when we
started our own startup in Boston in the 1990s. We had a handful
of older people we could go to for advice (of varying quality), but
no peers. There was no one we could commiserate with about the
misbehavior of investors, or speculate with about the future of
technology. I often tell founders to make something they themselves
want, and YC is certainly that: it was designed to be exactly what
we wanted when we were starting a startup.&lt;br/&gt;&lt;br/&gt;One thing we wanted was to be able to get seed funding without
having to make the rounds of random rich people. That has become a
commodity now, at least in the US. But great colleagues can never
become a commodity, because the fact that they cluster in some
places means they're proportionally absent from the rest.&lt;br/&gt;&lt;br/&gt;Something magical happens where they do cluster though. The energy
in the room at a YC dinner is like nothing else I've experienced.
We would have been happy just to have one or two other startups to
talk to. When you have a whole roomful it's another thing entirely.&lt;br/&gt;&lt;br/&gt;YC founders aren't just inspired by one another. They also help one
another. That's the happiest thing I've learned about startup
founders: how generous they can be in helping one another. We noticed
this in the first batch and consciously designed YC to magnify it.
The result is something far more intense than, say, a university.
Between the partners, the alumni, and their batchmates, founders
are surrounded by people who want to help them, and can.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;] 
This is why I've never liked it when people refer to YC as a
"bootcamp." It's intense like a bootcamp, but the opposite in
structure. Instead of everyone doing the same thing, they're each
talking to YC partners to figure out what their specific startup
needs.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;] 
When I say the summer 2012 batch was broken, I mean it felt
to the partners that something was wrong. Things weren't yet so
broken that the startups had a worse experience. In fact that batch
did unusually well.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;] 
This situation reminds me of the research showing that people
are much better at answering questions than they are at judging how
accurate their answers are. The two phenomena feel very similar.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;] 
The &lt;a href="https://paulgraham.com/airbnbs.html"&gt;Airbnbs&lt;/a&gt; were 
particularly good at listening — partly
because they were flexible and disciplined, but also because they'd
had such a rough time during the preceding year. They were ready
to listen.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;] 
The optimal unit of decisiveness depends on how long it takes
to get results, and that depends on the type of problem you're
solving. When you're negotiating with investors, it could be a
couple days, whereas if you're building hardware it could be months.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;font color="888888"&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Jessica Livingston, 
Harj Taggar, and Garry Tan for reading drafts of this.&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//users.html</guid>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Heresy</title>
      <link>https://paulgraham.com//heresy.html</link>
      <description>&lt;font face="verdana" size="2"&gt;April 2022&lt;br/&gt;&lt;br/&gt;One of the most surprising things I've witnessed in my lifetime is
the rebirth of the concept of heresy.&lt;br/&gt;&lt;br/&gt;In his excellent biography of Newton, Richard Westfall writes about the
moment when he was elected a fellow of Trinity College:
&lt;blockquote&gt;
  Supported comfortably, Newton was free to devote himself wholly
  to whatever he chose. To remain on, he had only to avoid the three
  unforgivable sins: crime, heresy, and marriage.
  &lt;/blockquote&gt;&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;April 2022&lt;br/&gt;&lt;br/&gt;One of the most surprising things I've witnessed in my lifetime is
the rebirth of the concept of heresy.&lt;br/&gt;&lt;br/&gt;In his excellent biography of Newton, Richard Westfall writes about the
moment when he was elected a fellow of Trinity College:
&lt;blockquote&gt;
  Supported comfortably, Newton was free to devote himself wholly
  to whatever he chose. To remain on, he had only to avoid the three
  unforgivable sins: crime, heresy, and marriage.
  &lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
&lt;/blockquote&gt;
The first time I read that, in the 1990s, it sounded amusingly
medieval. How strange, to have to avoid committing heresy. But when
I reread it 20 years later it sounded like a description of
contemporary employment.&lt;br/&gt;&lt;br/&gt;There are an ever-increasing number of opinions you can be fired
for. Those doing the firing don't use the word "heresy" to describe
them, but structurally they're equivalent. Structurally there are
two distinctive things about heresy: (1) that it takes priority
over the question of truth or falsity, and (2) that it outweighs
everything else the speaker has done.&lt;br/&gt;&lt;br/&gt;For example, when someone calls a statement "x-ist," they're also
implicitly saying that this is the end of the discussion. They do
not, having said this, go on to consider whether the statement is
true or not. Using such labels is the conversational equivalent of
signalling an exception. That's one of the reasons they're used:
to end a discussion.&lt;br/&gt;&lt;br/&gt;If you find yourself talking to someone who uses these labels a
lot, it might be worthwhile to ask them explicitly if they believe
any babies are being thrown out with the bathwater. Can a statement
be x-ist, for whatever value of x, and also true? If the answer is
yes, then they're admitting to banning the truth. That's obvious
enough that I'd guess most would answer no. But if they answer no,
it's easy to show that they're mistaken, and that in practice such
labels are applied to statements regardless of their truth or
falsity.&lt;br/&gt;&lt;br/&gt;The clearest evidence of this is that whether a statement is
considered x-ist often depends on who said it. Truth doesn't work
that way. The same statement can't be true when one person says it,
but x-ist, and therefore false, when another person does.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The other distinctive thing about heresies, compared to ordinary
opinions, is that the public expression of them outweighs everything
else the speaker has done. In ordinary matters, like knowledge of
history, or taste in music, you're judged by the average of your
opinions. A heresy is qualitatively different. It's like dropping
a chunk of uranium onto the scale.&lt;br/&gt;&lt;br/&gt;Back in the day (and still, in some places) the punishment for
heresy was death. You could have led a life of exemplary goodness,
but if you publicly doubted, say, the divinity of Christ, you were
going to burn. Nowadays, in civilized countries, heretics only get
fired in the metaphorical sense, by losing their jobs. But the
structure of the situation is the same: the heresy
outweighs everything else. You could have spent the last ten years
saving children's lives, but if you express certain opinions, you're
automatically fired.&lt;br/&gt;&lt;br/&gt;It's much the same as if you committed a crime. No matter how
virtuously you've lived, if you commit a crime, you must still
suffer the penalty of the law. Having lived a previously blameless
life might mitigate the punishment, but it doesn't affect whether
you're guilty or not.&lt;br/&gt;&lt;br/&gt;A heresy is an opinion whose expression is treated like a crime —
one that makes some people feel not merely that you're mistaken,
but that you should be punished. Indeed, their desire to see you
punished is often stronger than it would be if you'd committed an
actual crime. There are many on the far left who believe
strongly in the reintegration of felons (as I do myself), and yet
seem to feel that anyone guilty of certain heresies should never
work again.&lt;br/&gt;&lt;br/&gt;There are always some heresies — some opinions you'd be punished
for expressing. But there are a lot more now than there were a few
decades ago, and even those who are happy about this would have to
agree that it's so.&lt;br/&gt;&lt;br/&gt;Why? Why has this antiquated-sounding religious concept come back
in a secular form? And why now?&lt;br/&gt;&lt;br/&gt;You need two ingredients for a wave of intolerance: intolerant
people, and an ideology to guide them. The intolerant people are
always there. They exist in every sufficiently large society. That's
why waves of intolerance can arise so suddenly; all they need is
something to set them off.&lt;br/&gt;&lt;br/&gt;I've already written an &lt;a href="https://paulgraham.com/conformism.html"&gt;&lt;u&gt;essay&lt;/u&gt;&lt;/a&gt; 
describing the aggressively
conventional-minded. The short version is that people can be
classified in two dimensions according to (1) how independent- or
conventional-minded they are, and (2) how aggressive they are about
it. The aggressively conventional-minded are the enforcers of
orthodoxy.&lt;br/&gt;&lt;br/&gt;Normally they're only locally visible. They're the grumpy, censorious
people in a group — the ones who are always first to complain when
something violates the current rules of propriety. But occasionally,
like a vector field whose elements become aligned, a large number
of aggressively conventional-minded people unite behind some ideology
all at once. Then they become much more of a problem, because a mob
dynamic takes over, where the enthusiasm of each participant is
increased by the enthusiasm of the others.&lt;br/&gt;&lt;br/&gt;The most notorious 20th century case may have been the Cultural
Revolution. Though initiated by Mao to undermine his rivals, the
Cultural Revolution was otherwise mostly a grass-roots phenomenon.
Mao said in essence: There are heretics among us. Seek them out and
punish them. And that's all the aggressively conventional-minded
ever need to hear. They went at it with the delight of dogs chasing
squirrels.&lt;br/&gt;&lt;br/&gt;To unite the conventional-minded, an ideology must have many of the
features of a religion. In particular it must have strict and
arbitrary rules that adherents can demonstrate their 
&lt;a href="https://www.youtube.com/watch?v=qaHLd8de6nM"&gt;&lt;u&gt;purity&lt;/u&gt;&lt;/a&gt; 
by obeying, and its adherents must believe that anyone who obeys these
rules is ipso facto morally superior to anyone who doesn't.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In the late 1980s a new ideology of this type appeared in US
universities. It had a very strong component of moral purity, and
the aggressively conventional-minded seized upon it with their usual
eagerness — all the more because the relaxation of social norms
in the preceding decades meant there had been less and less to
forbid. The resulting wave of intolerance has been eerily similar
in form to the Cultural Revolution, though fortunately much smaller
in magnitude.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I've deliberately avoided mentioning any specific heresies here.
Partly because one of the universal tactics of heretic hunters, now
as in the past, is to accuse those who disapprove of the way in
which they suppress ideas of being heretics themselves. Indeed,
this tactic is so consistent that you could use it as a way of
detecting witch hunts in any era.&lt;br/&gt;&lt;br/&gt;And that's the second reason I've avoided mentioning any specific
heresies. I want this essay to work in the future, not just now.
And unfortunately it probably will. The aggressively conventional-minded
will always be among us, looking for things to forbid. All they
need is an ideology to tell them what. And it's unlikely the current
one will be the last.&lt;br/&gt;&lt;br/&gt;There are aggressively conventional-minded people on both the right
and the left. The reason the current wave of intolerance comes from
the left is simply because the new unifying ideology happened to
come from the left. The next one might come from the right. Imagine
what that would be like.&lt;br/&gt;&lt;br/&gt;Fortunately in western countries the suppression of heresies is
nothing like as bad as it used to be. Though the window of opinions
you can express publicly has narrowed in the last decade, it's still
much wider than it was a few hundred years ago. The problem is the
derivative. Up till about 1985 the window had been growing ever
wider. Anyone looking into the future in 1985 would have expected
freedom of expression to continue to increase. Instead it has
decreased.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The situation is similar to what's happened with infectious diseases
like measles. Anyone looking into the future in 2010 would have
expected the number of measles cases in the US to continue to
decrease. Instead, thanks to anti-vaxxers, it has increased. The
absolute number is still not that high. The problem is the derivative.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/heresy.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In both cases it's hard to know how much to worry. Is it really
dangerous to society as a whole if a handful of extremists refuse
to get their kids vaccinated, or shout down speakers at universities?
The point to start worrying is presumably when their efforts start
to spill over into everyone else's lives. And in both cases that
does seem to be happening.&lt;br/&gt;&lt;br/&gt;So it's probably worth spending some amount of effort on pushing
back to keep open the window of free expression. My hope is that
this essay will help form social antibodies not just against current
efforts to suppress ideas, but against the concept of heresy in
general. That's the real prize. How do you disable the concept of
heresy? Since the Enlightenment, western societies have discovered
many techniques for doing that, but there are surely more to be
discovered.&lt;br/&gt;&lt;br/&gt;Overall I'm optimistic. Though the trend in freedom of expression
has been bad over the last decade, it's been good over the longer
term. And there are signs that the current wave of intolerance is
peaking. Independent-minded people I talk to seem more confident
than they did a few years ago. On the other side, even some of the
&lt;a href="https://www.nytimes.com/2022/03/18/opinion/cancel-culture-free-speech-poll.html"&gt;&lt;u&gt;leaders&lt;/u&gt;&lt;/a&gt; are starting to wonder if things have 
gone too far. And popular culture among the young has already moved on. 
All we have
to do is keep pushing back, and the wave collapses. And then we'll
be net ahead, because as well as having defeated this wave, we'll
also have developed new tactics for resisting the next one.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;] 
Or more accurately, biographies of Newton, since Westfall wrote
two: a long version called &lt;i&gt;Never at Rest&lt;/i&gt;, and a shorter one called
&lt;i&gt;The Life of Isaac Newton&lt;/i&gt;. Both are great. The short version moves
faster, but the long one is full of interesting and often very funny
details. This passage is the same in both.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
Another more subtle but equally damning bit of evidence is
that claims of x-ism are never qualified. You never hear anyone say
that a statement is "probably x-ist" or "almost certainly y-ist."
If claims of x-ism were actually claims about truth, you'd expect
to see "probably" in front of "x-ist" as often as you see it in
front of "fallacious."&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;] 
The rules must be strict, but they need not be demanding. So
the most effective type of rules are those about superficial matters,
like doctrinal minutiae, or the precise words adherents must use.
Such rules can be made extremely complicated, and yet don't repel
potential converts by requiring significant sacrifice.&lt;br/&gt;&lt;br/&gt;The superficial demands of orthodoxy make it an inexpensive substitute
for virtue. And that in turn is one of the reasons orthodoxy is so
attractive to bad people. You could be a horrible person, and yet
as long as you're orthodox, you're better than everyone who isn't.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;] 
Arguably there were two. The first had died down somewhat by
2000, but was followed by a second in the 2010s, probably caused
by social media.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;] 
Fortunately most of those trying to suppress ideas today still
respect Enlightenment principles enough to pay lip service to them.
They know they're not supposed to ban ideas per se, so they have
to recast the ideas as causing "harm," which sounds like something
that can be banned. The more extreme try to claim speech itself is
violence, or even that silence is. But strange as it may sound,
such gymnastics are a good sign. We'll know we're really in trouble
when they stop bothering to invent pretenses for banning ideas —
when, like the medieval church, they say "Damn right we're banning
ideas, and in fact here's a list of them."&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;] 
People only have the luxury of ignoring the medical consensus
about vaccines because vaccines have worked so well. If we didn't
have any vaccines at all, the mortality rate would be so high that
most current anti-vaxxers would be begging for them. And the situation
with freedom of expression is similar. It's only because they live
in a world created by the Enlightenment that kids from the suburbs
can play at banning ideas.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt;&lt;b&gt;Thanks&lt;/b&gt; to Marc Andreessen, Chris Best, 
Trevor Blackwell, Nicholas
Christakis, Daniel Gackle, Jonathan Haidt, Claire Lehmann, Jessica
Livingston, Greg Lukianoff, Robert Morris, and Garry Tan for reading
drafts of this.&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//heresy.html</guid>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Putting Ideas into Words</title>
      <link>https://paulgraham.com//words.html</link>
      <description>&lt;font face="verdana" size="2"&gt;February 2022&lt;br/&gt;&lt;br/&gt;Writing about something, even something you know well, usually shows
you that you didn't know it as well as you thought. Putting ideas
into words is a severe test. The first words you choose are usually
wrong; you have to rewrite sentences over and over &lt;!-- if you want --&gt; to
get them exactly right. And your ideas won't just be imprecise, but
incomplete too. Half the ideas that end up in an essay will be ones
you thought of while you were wri&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;February 2022&lt;br/&gt;&lt;br/&gt;Writing about something, even something you know well, usually shows
you that you didn't know it as well as you thought. Putting ideas
into words is a severe test. The first words you choose are usually
wrong; you have to rewrite sentences over and over &lt;!-- if you want --&gt; to
get them exactly right. And your ideas won't just be imprecise, but
incomplete too. Half the ideas that end up in an essay will be ones
you thought of while you were writing it. Indeed, that's why I write
them.&lt;br/&gt;&lt;br/&gt;Once you publish something, the convention is that whatever you
wrote was what you thought before you wrote it. These were your
ideas, and now you've expressed them. But you know this isn't true.
You know that putting your ideas into words changed them. And not
just the ideas you published. Presumably there were others that
turned out to be too broken to fix, and those you discarded instead.&lt;br/&gt;&lt;br/&gt;It's not just having to commit your ideas to specific words that
makes writing so exacting. The real test is reading what you've
written. You have to pretend to be a neutral reader who knows nothing
of what's in your head, only what you wrote. When he reads what you
wrote, does it seem correct? Does it seem complete? If you make an
effort, you can read your writing as if you were a complete stranger,
and when you do the news is usually bad. It takes me many cycles
before I can get an essay past the stranger. But the stranger is
rational, so you always can, if you ask him what he needs. If he's
not satisfied because you failed to mention x or didn't qualify
some sentence sufficiently, then you mention x or add more
qualifications. Happy now? It may cost you some nice sentences, but
you have to resign yourself to that. You just have to make them as
good as you can and still satisfy the stranger.&lt;br/&gt;&lt;br/&gt;This much, I assume, won't be that controversial. I think it will
accord with the experience of anyone who has tried to write about
anything nontrivial. There may exist people whose thoughts are so
perfectly formed that they just flow straight into words. But I've
never known anyone who could do this, and if I met someone who said
they could, it would seem evidence of their limitations rather than
their ability. Indeed, this is a trope in movies: the guy who claims
to have a plan for doing some difficult thing, and who when questioned
further, taps his head and says "It's all up here." Everyone watching
the movie knows what that means. At best the plan is vague and
incomplete. Very likely there's some undiscovered flaw that invalidates
it completely. At best it's a plan for a plan.&lt;br/&gt;&lt;br/&gt;In precisely defined domains it's possible to form complete ideas
in your head. People can play chess in their heads, for example.
And mathematicians can do some amount of math in their heads, though
they don't seem to feel sure of a proof over a certain length till
they write it down. But this only seems possible with ideas you can
express in a formal language.  &lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/words.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt; Arguably what such people are
doing is putting ideas into words in their heads. I can to some
extent write essays in my head. I'll sometimes think of a paragraph
while walking or lying in bed that survives nearly unchanged in the
final version. But really I'm writing when I do this. I'm doing the
mental part of writing; my fingers just aren't moving as I do it.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/words.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;You can know a great deal about something without writing about it.
Can you ever know so much that you wouldn't learn more from trying
to explain what you know? I don't think so. I've written about at
least two subjects I know well — Lisp hacking and startups
— and in both cases I learned a lot from writing about them.
In both cases there were things I didn't consciously realize till
I had to explain them. And I don't think my experience was anomalous.
A great deal of knowledge is unconscious, and experts have if
anything a higher proportion of unconscious knowledge than beginners.&lt;br/&gt;&lt;br/&gt;I'm not saying that writing is the best way to explore all ideas.
If you have ideas about architecture, presumably the best way to
explore them is to build actual buildings. What I'm saying is that
however much you learn from exploring ideas in other ways, you'll
still learn new things from writing about them.&lt;br/&gt;&lt;br/&gt;Putting ideas into words doesn't have to mean writing, of course.
You can also do it the old way, by talking. But in my experience,
writing is the stricter test. You have to commit to a single, optimal
sequence of words. Less can go unsaid when you don't have tone of
voice to carry meaning. And you can focus in a way that would seem
excessive in conversation. I'll often spend 2 weeks on an essay and
reread drafts 50 times. If you did that in conversation
it would seem evidence of some kind of
mental disorder. 
If you're lazy,
of course, writing and talking are equally useless. But if you want
to push yourself to get things right, writing is the steeper hill.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/words.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The reason I've spent so long establishing this rather obvious point
is that it leads to another that many people will find shocking.
If writing down your ideas always makes them more precise and more
complete, then no one who hasn't written about a topic has fully
formed ideas about it. And someone who never writes has no fully
formed ideas about anything nontrivial.&lt;br/&gt;&lt;br/&gt;It feels to them as if they do, especially if they're not in the
habit of critically examining their own thinking. Ideas can feel
complete. It's only when you try to put them into words that you
discover they're not. So if you never subject your ideas to that
test, you'll not only never have fully formed ideas, but also never
realize it.&lt;br/&gt;&lt;br/&gt;Putting ideas into words is certainly no guarantee that they'll be
right. Far from it. But though it's not a sufficient condition, it
is a necessary one.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;] Machinery and
circuits are formal languages.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;] I thought of this
sentence as I was walking down the street in Palo Alto.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;] There are two
senses of talking to someone: a strict sense in which the conversation
is verbal, and a more general sense in which it can take any form,
including writing. In the limit case (e.g. Seneca's letters),
conversation in the latter sense becomes essay writing.&lt;br/&gt;&lt;br/&gt;It can be very useful to talk (in either sense) with other people
as you're writing something. But a verbal conversation will never
be more exacting than when you're talking about something you're
writing.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt; &lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Patrick
Collison, and Robert Morris for reading drafts of this.  &lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//words.html</guid>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Is There Such a Thing as Good Taste?</title>
      <link>https://paulgraham.com//goodtaste.html</link>
      <description>&lt;font face="verdana" size="2"&gt;November 2021&lt;br/&gt;&lt;br/&gt;&lt;i&gt;(This essay is derived from a talk at the Cambridge Union.)&lt;/i&gt;&lt;br/&gt;&lt;br/&gt;When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?&lt;br/&gt;&lt;br/&gt;It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;November 2021&lt;br/&gt;&lt;br/&gt;&lt;i&gt;(This essay is derived from a talk at the Cambridge Union.)&lt;/i&gt;&lt;br/&gt;&lt;br/&gt;When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?&lt;br/&gt;&lt;br/&gt;It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.&lt;br/&gt;&lt;br/&gt;We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kind. The strongest proof
would be to show that taste exists in the narrowest sense, so I'm
going to talk about taste in art. You have better taste than me if
the art you like is better than the art I like.&lt;br/&gt;&lt;br/&gt;If there's no such thing as good taste, then there's no such thing
as &lt;a href="https://paulgraham.com/goodart.html"&gt;&lt;u&gt;good art&lt;/u&gt;&lt;/a&gt;. Because if there is such a
thing as good art, it's
easy to tell which of two people has better taste. Show them a lot
of works by artists they've never seen before and ask them to
choose the best, and whoever chooses the better art has better
taste.&lt;br/&gt;&lt;br/&gt;So if you want to discard the concept of good taste, you also have
to discard the concept of good art. And that means you have to
discard the possibility of people being good at making it. Which
means there's no way for artists to be good at their jobs. And not
just visual artists, but anyone who is in any sense an artist. You
can't have good actors, or novelists, or composers, or dancers
either. You can have popular novelists, but not good ones.&lt;br/&gt;&lt;br/&gt;We don't realize how far we'd have to go if we discarded the concept
of good taste, because we don't even debate the most obvious cases.
But it doesn't just mean we can't say which of two famous painters
is better. It means we can't say that any painter is better than a
randomly chosen eight year old.&lt;br/&gt;&lt;br/&gt;That was how I realized my father was wrong. I started studying
painting. And it was just like other kinds of work I'd done: you
could do it well, or badly, and if you tried hard, you could get
better at it. And it was obvious that Leonardo and Bellini were
much better at it than me. That gap between us was not imaginary.
They were so good. And if they could be good, then art could be
good, and there was such a thing as good taste after all.&lt;br/&gt;&lt;br/&gt;Now that I've explained how to show there is such a thing as good
taste, I should also explain why people think there isn't. There
are two reasons. One is that there's always so much disagreement
about taste. Most people's response to art is a tangle of unexamined
impulses. Is the artist famous? Is the subject attractive? Is this
the sort of art they're supposed to like? Is it hanging in a famous
museum, or reproduced in a big, expensive book? In practice most
people's response to art is dominated by such extraneous factors.&lt;br/&gt;&lt;br/&gt;And the people who do claim to have good taste are so often mistaken.
The paintings admired by the so-called experts in one generation
are often so different from those admired a few generations later.
It's easy to conclude there's nothing real there at all. It's only
when you isolate this force, for example by trying to paint and
comparing your work to Bellini's, that you can see that it does in
fact exist.&lt;br/&gt;&lt;br/&gt;The other reason people doubt that art can be good is that there
doesn't seem to be any room in the art for this goodness. The
argument goes like this. Imagine several people looking at a work
of art and judging how good it is. If being good art really is a
property of objects, it should be in the object somehow. But it
doesn't seem to be; it seems to be something happening in the heads
of each of the observers. And if they disagree, how do you choose
between them?&lt;br/&gt;&lt;br/&gt;The solution to this puzzle is to realize that the purpose of art
is to work on its human audience, and humans have a lot in common.
And to the extent the things an object acts upon respond in the
same way, that's arguably what it means for the object to have the
corresponding property. If everything a particle interacts with
behaves as if the particle had a mass of &lt;i&gt;m&lt;/i&gt;, then it has a mass of
&lt;i&gt;m&lt;/i&gt;. So the distinction between "objective" and "subjective" is not
binary, but a matter of degree, depending on how much the subjects
have in common. Particles interacting with one another are at one
pole, but people interacting with art are not all the way at the
other; their reactions aren't &lt;i&gt;random&lt;/i&gt;.&lt;br/&gt;&lt;br/&gt;Because people's responses to art aren't random, art can be designed
to operate on people, and be good or bad depending on how effectively
it does so. Much as a vaccine can be. If someone were talking about
the ability of a vaccine to confer immunity, it would seem very
frivolous to object that conferring immunity wasn't really a property
of vaccines, because acquiring immunity is something that happens
in the immune system of each individual person. Sure, people's
immune systems vary, and a vaccine that worked on one might not
work on another, but that doesn't make it meaningless to talk about
the effectiveness of a vaccine.&lt;br/&gt;&lt;br/&gt;The situation with art is messier, of course. You can't measure
effectiveness by simply taking a vote, as you do with vaccines.
You have to imagine the responses of subjects with a deep knowledge
of art, and enough clarity of mind to be able to ignore extraneous
influences like the fame of the artist. And even then you'd still
see some disagreement. People do vary, and judging art is hard,
especially recent art. There is definitely not a total order either
of works or of people's ability to judge them. But there is equally
definitely a partial order of both. So while it's not possible to
have perfect taste, it is possible to have good taste.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt;
&lt;b&gt;Thanks&lt;/b&gt; to the Cambridge Union for inviting me, and to Trevor
Blackwell, Jessica Livingston, and Robert Morris for reading drafts
of this.
&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//goodtaste.html</guid>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Beyond Smart</title>
      <link>https://paulgraham.com//smart.html</link>
      <description>&lt;font face="verdana" size="2"&gt;October 2021&lt;br/&gt;&lt;br/&gt;If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn't what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, b&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;October 2021&lt;br/&gt;&lt;br/&gt;If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn't what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, but the two are not
identical.&lt;br/&gt;&lt;br/&gt;It may seem a hair-splitting distinction to point out that intelligence
and its consequences are not identical, but it isn't. There's a big
gap between them. Anyone who's spent time around universities and
research labs knows how big. There are a lot of genuinely smart
people who don't achieve very much.&lt;br/&gt;&lt;br/&gt;I grew up thinking that being smart was the thing most to be desired.
Perhaps you did too. But I bet it's not what you really want. Imagine
you had a choice between being really smart but discovering nothing
new, and being less smart but discovering lots of new ideas. Surely
you'd take the latter. I would. The choice makes me uncomfortable,
but when you see the two options laid out explicitly like that,
it's obvious which is better.&lt;br/&gt;&lt;br/&gt;The reason the choice makes me uncomfortable is that being smart
still feels like the thing that matters, even though I know
intellectually that it isn't. I spent so many years thinking it
was. The circumstances of childhood are a perfect storm for fostering
this illusion. Intelligence is much easier to measure than the value
of new ideas, and you're constantly being judged by it. Whereas
even the kids who will ultimately discover new things aren't usually
discovering them yet. For kids that way inclined, intelligence is
the only game in town.&lt;br/&gt;&lt;br/&gt;There are more subtle reasons too, which persist long into adulthood.
Intelligence wins in conversation, and thus becomes the basis of
the dominance hierarchy.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/smart.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
Plus having new ideas is such a new
thing historically, and even now done by so few people, that society
hasn't yet assimilated the fact that this is the actual destination,
and intelligence merely a means to an end.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/smart.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Why do so many smart people fail to discover anything new? Viewed
from that direction, the question seems a rather depressing one.
But there's another way to look at it that's not just more optimistic,
but more interesting as well. Clearly intelligence is not the only
ingredient in having new ideas. What are the other ingredients?
Are they things we could cultivate?&lt;br/&gt;&lt;br/&gt;Because the trouble with intelligence, they say, is that it's mostly
inborn. The evidence for this seems fairly convincing, especially
considering that most of us don't want it to be true, and the
evidence thus has to face a stiff headwind. But I'm not going
to get into that question here, because it's the other ingredients
in new ideas that I care about, and it's clear that many of them
can be cultivated.&lt;br/&gt;&lt;br/&gt;That means the truth is excitingly different from the story I got
as a kid. If intelligence is what matters, and also mostly inborn,
the natural consequence is a sort of &lt;i&gt;Brave New World&lt;/i&gt; fatalism. The
best you can do is figure out what sort of work you have an "aptitude"
for, so that whatever intelligence you were born with will at least
be put to the best use, and then work as hard as you can at it.
Whereas if intelligence isn't what matters, but only one of several
ingredients in what does, and many of those aren't inborn, things
get more interesting. You have a lot more control, but the problem
of how to arrange your life becomes that much more complicated.&lt;br/&gt;&lt;br/&gt;So what are the other ingredients in having new ideas? The fact
that I can even ask this question proves the point I raised earlier
— that society hasn't assimilated the fact that it's this and not
intelligence that matters. Otherwise we'd all know the answers
to such a fundamental question.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/smart.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I'm not going to try to provide a complete catalogue of the other
ingredients here. This is the first time I've posed
the question to myself this way, and I think it may take a while
to answer. But I wrote recently about one of the most important:
an obsessive &lt;a href="https://paulgraham.com/genius.html"&gt;&lt;u&gt;interest&lt;/u&gt;&lt;/a&gt; in a particular topic. 
And this can definitely be cultivated.&lt;br/&gt;&lt;br/&gt;Another quality you need in order to discover new ideas is
&lt;a href="https://paulgraham.com/think.html"&gt;&lt;u&gt;independent-mindedness&lt;/u&gt;&lt;/a&gt;. I wouldn't want to 
claim that this is
distinct from intelligence — I'd be reluctant to call someone smart
who wasn't independent-minded — but though largely inborn, this
quality seems to be something that can be cultivated to some extent.&lt;br/&gt;&lt;br/&gt;There are general techniques for having new ideas — for example,
for working on your own &lt;a href="https://paulgraham.com/own.html"&gt;&lt;u&gt;projects&lt;/u&gt;&lt;/a&gt;
and
for overcoming the obstacles you face with &lt;a href="https://paulgraham.com/early.html"&gt;&lt;u&gt;early&lt;/u&gt;&lt;/a&gt; work
— and these
can all be learned. Some of them can be learned by societies. And
there are also collections of techniques for generating specific types
of new ideas, like &lt;a href="https://paulgraham.com/startupideas.html"&gt;startup ideas&lt;/a&gt; and 
&lt;a href="https://paulgraham.com/essay.html"&gt;essay topics&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;And of course there are a lot of fairly mundane ingredients in
discovering new ideas, like &lt;a href="https://paulgraham.com/hwh.html"&gt;&lt;u&gt;working hard&lt;/u&gt;&lt;/a&gt;, 
getting enough sleep, avoiding certain
kinds of stress, having the right colleagues, and finding tricks
for working on what you want even when it's not what you're supposed
to be working on. Anything that prevents people from doing great
work has an inverse that helps them to. And this class of ingredients
is not as boring as it might seem at first. For example, having new
ideas is generally associated with youth. But perhaps it's not youth
per se that yields new ideas, but specific things that come with
youth, like good health and lack of responsibilities. Investigating
this might lead to strategies that will help people of any age to
have better ideas.&lt;br/&gt;&lt;br/&gt;One of the most surprising ingredients in having new ideas is writing
ability. There's a class of new ideas that are best discovered by
writing essays and books. And that "by" is deliberate: you don't
think of the ideas first, and then merely write them down. There
is a kind of thinking that one does by writing, and if you're clumsy
at writing, or don't enjoy doing it, that will get in your way if
you try to do this kind of thinking.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/smart.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I predict the gap between intelligence and new ideas will turn out
to be an interesting place. If we think of this gap merely as a measure
of unrealized potential, it becomes a sort of wasteland that we try to
hurry through with our eyes averted. But if we flip the question,
and start inquiring into the other ingredients in new ideas that
it implies must exist, we can mine this gap for discoveries about
discovery.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
What wins in conversation depends on who with. It ranges from
mere aggressiveness at the bottom, through quick-wittedness in the
middle, to something closer to actual intelligence at the top,
though probably always with some component of quick-wittedness.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
Just as intelligence isn't the only ingredient in having new
ideas, having new ideas isn't the only thing intelligence is useful
for. It's also useful, for example, in diagnosing problems and figuring
out how to fix them. Both overlap with having new ideas, but both
have an end that doesn't.&lt;br/&gt;&lt;br/&gt;Those ways of using intelligence are much more common than having
new ideas. And in such cases intelligence is even harder to distinguish
from its consequences.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
Some would attribute the difference between intelligence and
having new ideas to "creativity," but this doesn't seem a very
useful term. As well as being pretty vague, it's shifted half a frame
sideways from what we care about: it's neither separable from
intelligence, nor responsible for all the difference between
intelligence and having new ideas.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Curiously enough, this essay is an example. It started out
as an essay about writing ability. But when I came to the distinction
between intelligence and having new ideas, that seemed so much more
important that I turned the original essay inside out, making that
the topic and my original topic one of the points in it. As in many
other fields, that level of reworking is easier to contemplate once
you've had a lot of practice.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Patrick Collison, Jessica Livingston,
Robert Morris, Michael Nielsen, and Lisa Randall for reading drafts
of this.
&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//smart.html</guid>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Weird Languages</title>
      <link>https://paulgraham.com//weird.html</link>
      <description>&lt;font face="verdana" size="2"&gt;August 2021&lt;br/&gt;&lt;br/&gt;When people say that in their experience all programming languages
are basically equivalent, they're making a statement not about
languages but about the kind of programming they've done.&lt;br/&gt;&lt;br/&gt;99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one's whole career operating in the intersection
of popular programming languages.&lt;br/&gt;&lt;br/&gt;But the oth&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;August 2021&lt;br/&gt;&lt;br/&gt;When people say that in their experience all programming languages
are basically equivalent, they're making a statement not about
languages but about the kind of programming they've done.&lt;br/&gt;&lt;br/&gt;99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one's whole career operating in the intersection
of popular programming languages.&lt;br/&gt;&lt;br/&gt;But the other .5% of programming is disproportionately interesting.
If you want to learn what it consists of, the weirdness of weird
languages is a good clue to follow.&lt;br/&gt;&lt;br/&gt;Weird languages aren't weird by accident. Not the good ones, at
least. The weirdness of the good ones usually implies the existence
of some form of programming that's not just the usual gluing together
of library calls.&lt;br/&gt;&lt;br/&gt;A concrete example: Lisp macros. Lisp macros seem weird even to
many Lisp programmers. They're not only not in the intersection of
popular languages, but by their nature would be hard to implement
properly in a language without turning it into a dialect of
Lisp. And macros are definitely evidence of techniques that go
beyond glue programming. For example, solving problems by first
writing a language for problems of that type, and then writing
your specific application in it. Nor is this all you can do with
macros; it's just one region in a space of program-manipulating
techniques that even now is far from fully explored.&lt;br/&gt;&lt;br/&gt;So if you want to expand your concept of what programming can be,
one way to do it is by learning weird languages. Pick a language
that most programmers consider weird but whose median user is smart,
and then focus on the differences between this language and the
intersection of popular languages. What can you say in this language
that would be impossibly inconvenient to say in others? In the
process of learning how to say things you couldn't previously say,
you'll probably be learning how to think things you couldn't
previously think.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;font color="888888"&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad
Masad, and Robert Morris for reading drafts of this.
&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//weird.html</guid>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to Work Hard</title>
      <link>https://paulgraham.com//hwh.html</link>
      <description>&lt;font face="verdana" size="2"&gt;June 2021&lt;br/&gt;&lt;br/&gt;It might not seem there's much to learn about how to work hard.
Anyone who's been to school knows what it entails, even if they
chose not to do it. There are 12 year olds who work amazingly hard. And
yet when I ask if I know more about working hard now than when I
was in school, the answer is definitely yes.&lt;br/&gt;&lt;br/&gt;One thing I know is that if you want to do great things, you'll
have to work very hard. I wasn't sure of that as a kid. Schoolwork
v&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;June 2021&lt;br/&gt;&lt;br/&gt;It might not seem there's much to learn about how to work hard.
Anyone who's been to school knows what it entails, even if they
chose not to do it. There are 12 year olds who work amazingly hard. And
yet when I ask if I know more about working hard now than when I
was in school, the answer is definitely yes.&lt;br/&gt;&lt;br/&gt;One thing I know is that if you want to do great things, you'll
have to work very hard. I wasn't sure of that as a kid. Schoolwork
varied in difficulty; one didn't always have to work super hard to
do well. And some of the things famous adults did, they seemed to
do almost effortlessly. Was there, perhaps, some way to evade hard
work through sheer brilliance? Now I know the answer to that question.
There isn't.&lt;br/&gt;&lt;br/&gt;The reason some subjects seemed easy was that my school had low
standards. And the reason famous adults seemed to do things
effortlessly was years of practice; they made it look easy.&lt;br/&gt;&lt;br/&gt;Of course, those famous adults usually had a lot of natural ability
too. There are three ingredients in great work: natural ability,
practice, and effort. You can do pretty well with just two, but to
do the best work you need all three: you need great natural ability
&lt;i&gt;and&lt;/i&gt; to have practiced a lot &lt;i&gt;and&lt;/i&gt; to be trying very hard. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Bill Gates, for example, was among the smartest people in business
in his era, but he was also among the hardest working. "I never
took a day off in my twenties," he said. "Not one." It was similar
with Lionel Messi. He had great natural ability, but when his youth
coaches talk about him, what they remember is not his talent but
his dedication and his desire to win. P. G. Wodehouse would probably
get my vote for best English writer of the 20th century, if I had
to choose. Certainly no one ever made it look easier. But no one
ever worked harder. At 74, he wrote
&lt;blockquote&gt;
  with each new book of mine I have, as I say, the feeling that
  this time I have picked a lemon in the garden of literature. A
  good thing, really, I suppose. Keeps one up on one's toes and
  makes one rewrite every sentence ten times. Or in many cases
  twenty times.
&lt;/blockquote&gt;
Sounds a bit extreme, you think. And yet Bill Gates sounds even
more extreme. Not one day off in ten years?  These two had about
as much natural ability as anyone could have, and yet they also
worked about as hard as anyone could work. You need both.&lt;br/&gt;&lt;br/&gt;That seems so obvious, and yet in practice we find it slightly hard
to grasp. There's a faint xor between talent and hard work. It comes
partly from popular culture, where it seems to run very deep, and
partly from the fact that the outliers are so rare. If great talent
and great drive are both rare, then people with both are rare
squared. Most people you meet who have a lot of one will have less
of the other. But you'll need both if you want to be an outlier
yourself. And since you can't really change how much natural talent
you have, in practice doing great work, insofar as you can, reduces
to working very hard.&lt;br/&gt;&lt;br/&gt;It's straightforward to work hard if you have clearly defined,
externally imposed goals, as you do in school. There is some technique
to it: you have to learn not to lie to yourself, not to procrastinate
(which is a form of lying to yourself), not to get distracted, and
not to give up when things go wrong. But this level of discipline
seems to be within the reach of quite young children, if they want
it.&lt;br/&gt;&lt;br/&gt;What I've learned since I was a kid is how to work toward goals
that are neither clearly defined nor externally imposed. You'll
probably have to learn both if you want to do really great things.&lt;br/&gt;&lt;br/&gt;The most basic level of which is simply to feel you should be working
without anyone telling you to. Now, when I'm not working hard, alarm
bells go off. I can't be sure I'm getting anywhere when I'm working
hard, but I can be sure I'm getting nowhere when I'm not, and it
feels awful.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;There wasn't a single point when I learned this. Like most little
kids, I enjoyed the feeling of achievement when I learned or did
something new. As I grew older, this morphed into a feeling of
disgust when I wasn't achieving anything. The one precisely dateable
landmark I have is when I stopped watching TV, at age 13.&lt;br/&gt;&lt;br/&gt;Several people I've talked to remember getting serious about work
around this age. When I asked Patrick Collison when he started to
find idleness distasteful, he said
&lt;blockquote&gt;
  I think around age 13 or 14. I have a clear memory from around
  then of sitting in the sitting room, staring outside, and wondering
  why I was wasting my summer holiday.
&lt;/blockquote&gt;
Perhaps something changes at adolescence. That would make sense.&lt;br/&gt;&lt;br/&gt;Strangely enough, the biggest obstacle to getting serious about
work was probably school, which made work (what they called work)
seem boring and pointless. I had to learn what real work was before
I could wholeheartedly desire to do it. That took a while, because
even in college a lot of the work is pointless; there are entire
departments that are pointless. But as I learned the shape of real
work, I found that my desire to do it slotted into it as if they'd
been made for each other.&lt;br/&gt;&lt;br/&gt;I suspect most people have to learn what work is before they can
love it. Hardy wrote eloquently about this in &lt;i&gt;A Mathematician's
Apology&lt;/i&gt;:
&lt;blockquote&gt;
  I do not remember having felt, as a boy, any &lt;i&gt;passion&lt;/i&gt; for
  mathematics, and such notions as I may have had of the career of
  a mathematician were far from noble. I thought of mathematics in
  terms of examinations and scholarships: I wanted to beat other
  boys, and this seemed to be the way in which I could do so most
  decisively.
&lt;/blockquote&gt;
He didn't learn what math was really about till part way through
college, when he read Jordan's &lt;i&gt;Cours d'analyse&lt;/i&gt;.
&lt;blockquote&gt;
  I shall never forget the astonishment with which I read that
  remarkable work, the first inspiration for so many mathematicians
  of my generation, and learnt for the first time as I read it what
  mathematics really meant.
&lt;/blockquote&gt;
There are two separate kinds of fakeness you need to learn to
discount in order to understand what real work is. One is the kind
Hardy encountered in school. Subjects get distorted when they're
adapted to be taught to kids — often so distorted that they're
nothing like the work done by actual practitioners.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
The other
kind of fakeness is intrinsic to certain types of work. Some types
of work are inherently bogus, or at best mere busywork.&lt;br/&gt;&lt;br/&gt;There's a kind of solidity to real work. It's not all writing the
&lt;i&gt;Principia&lt;/i&gt;, but it all feels necessary. That's a vague criterion,
but it's deliberately vague, because it has to cover a lot of
different types.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Once you know the shape of real work, you have to learn how many
hours a day to spend on it. You can't solve this problem by simply
working every waking hour, because in many kinds of work there's a
point beyond which the quality of the result will start to decline.&lt;br/&gt;&lt;br/&gt;That limit varies depending on the type of work and the person.
I've done several different kinds of work, and the limits were
different for each. My limit for the harder types of writing or
programming is about five hours a day. Whereas when I was running
a startup, I could
work all the time. At least for the three years I did it; if I'd
kept going much longer, I'd probably have needed to take occasional
vacations.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The only way to find the limit is by crossing it. Cultivate a
sensitivity to the quality of the work you're doing, and then you'll
notice if it decreases because you're working too hard. Honesty is
critical here, in both directions: you have to notice when you're
being lazy, but also when you're working too hard. And if you think
there's something admirable about working too hard, get that idea
out of your head. You're not merely getting worse results, but
getting them because you're showing off — if not to other people,
then to yourself.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Finding the limit of working hard is a constant, ongoing process,
not something you do just once. Both the difficulty of the work and
your ability to do it can vary hour to hour, so you need to be
constantly judging both how hard you're trying and how well you're
doing.&lt;br/&gt;&lt;br/&gt;Trying hard doesn't mean constantly pushing yourself to work, though.
There may be some people who do, but I think my experience is fairly
typical, and I only have to push myself occasionally when I'm
starting a project or when I encounter some sort of check. That's
when I'm in danger of procrastinating. But once I get rolling, I
tend to keep going.&lt;br/&gt;&lt;br/&gt;What keeps me going depends on the type of work. When I was working
on Viaweb, I was driven by fear of failure. I barely procrastinated
at all then, because there was always something that needed doing,
and if I could put more distance between me and the pursuing beast
by doing it, why wait? &lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f7n"&gt;&lt;font color="#dddddd"&gt;7&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
Whereas what drives me now, writing
essays, is the flaws in them. Between essays I fuss for a few days,
like a dog circling while it decides exactly where to lie down. But
once I get started on one, I don't have to push myself to work,
because there's always some error or omission already pushing me.&lt;br/&gt;&lt;br/&gt;I do make some amount of effort to focus on important topics. Many
problems have a hard core at the center, surrounded by easier stuff
at the edges. Working hard means aiming toward the center to the
extent you can. Some days you may not be able to; some days you'll
only be able to work on the easier, peripheral stuff. But you should
always be aiming as close to the center as you can without stalling.&lt;br/&gt;&lt;br/&gt;The bigger question of what to do with your life is one of these
problems with a hard core. There are important problems at the
center, which tend to be hard, and less important, easier ones at
the edges. So as well as the small, daily adjustments involved in
working on a specific problem, you'll occasionally have to make
big, lifetime-scale adjustments about which type of work to do.
And the rule is the same: working hard means aiming toward the
center — toward the most ambitious problems.&lt;br/&gt;&lt;br/&gt;By center, though, I mean the actual center, not merely the current
consensus about the center. The consensus about which problems are
most important is often mistaken, both in general and within specific
fields. If you disagree with it, and you're right, that could
represent a valuable opportunity to do something new.&lt;br/&gt;&lt;br/&gt;The more ambitious types of work will usually be harder, but although
you should not be in denial about this, neither should you treat
difficulty as an infallible guide in deciding what to do. If you
discover some ambitious type of work that's a bargain in the sense
of being easier for you than other people, either because of the
abilities you happen to have, or because of some new way you've
found to approach it, or simply because you're more excited about
it, by all means work on that. Some of the best work is done by
people who find an easy way to do something hard.&lt;br/&gt;&lt;br/&gt;As well as learning the shape of real work, you need to figure out
which kind you're suited for. And that doesn't just mean figuring
out which kind your natural abilities match the best; it doesn't
mean that if you're 7 feet tall, you have to play basketball. What
you're suited for depends not just on your talents but perhaps even
more on your interests. A &lt;a href="https://paulgraham.com/genius.html"&gt;&lt;u&gt;deep interest&lt;/u&gt;&lt;/a&gt; 
in a topic makes people
work harder than any amount of discipline can.&lt;br/&gt;&lt;br/&gt;It can be harder to discover your interests than your talents.
There are fewer types of talent than interest, and they start to
be judged early in childhood, whereas interest in a topic is a
subtle thing that may not mature till your twenties, or even later.
The topic may not even exist earlier. Plus there are some powerful
sources of error you need to learn to discount. Are you really
interested in x, or do you want to work on it because you'll make
a lot of money, or because other people will be impressed with you,
or because your parents want you to?
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f8n"&gt;&lt;font color="#dddddd"&gt;8&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The difficulty of figuring out what to work on varies enormously
from one person to another. That's one of the most important things
I've learned about work since I was a kid. As a kid, you get the
impression that everyone has a calling, and all they have to do is
figure out what it is. That's how it works in movies, and in the
streamlined biographies fed to kids. Sometimes it works that way
in real life. Some people figure out what to do as children and
just do it, like Mozart. But others, like Newton, turn restlessly
from one kind of work to another. Maybe in retrospect we can identify
one as their calling — we can wish Newton spent more time on math
and physics and less on alchemy and theology — but this is an
&lt;a href="https://paulgraham.com/disc.html"&gt;&lt;u&gt;illusion&lt;/u&gt;&lt;/a&gt; induced by hindsight bias. 
There was no voice calling to him that he could have heard.&lt;br/&gt;&lt;br/&gt;So while some people's lives converge fast, there will be others
whose lives never converge. And for these people, figuring out what
to work on is not so much a prelude to working hard as an ongoing
part of it, like one of a set of simultaneous equations. For these
people, the process I described earlier has a third component: along
with measuring both how hard you're working and how well you're
doing, you have to think about whether you should keep working in
this field or switch to another. If you're working hard but not
getting good enough results, you should switch. It sounds simple
expressed that way, but in practice it's very difficult. You shouldn't
give up on the first day just because you work hard and don't get
anywhere. You need to give yourself time to get going. But how much
time? And what should you do if work that was going well stops going
well? How much time do you give yourself then?
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/hwh.html#f9n"&gt;&lt;font color="#dddddd"&gt;9&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;What even counts as good results? That can be really hard to decide.
If you're exploring an area few others have worked in, you may not
even know what good results look like. History is full of examples
of people who misjudged the importance of what they were working
on.&lt;br/&gt;&lt;br/&gt;The best test of whether it's worthwhile to work on something is
whether you find it interesting. That may sound like a dangerously
subjective measure, but it's probably the most accurate one you're
going to get. You're the one working on the stuff. Who's in a better
position than you to judge whether it's important, and what's a
better predictor of its importance than whether it's interesting?&lt;br/&gt;&lt;br/&gt;For this test to work, though, you have to be honest with yourself.
Indeed, that's the most striking thing about the whole question of
working hard: how at each point it depends on being honest with
yourself.&lt;br/&gt;&lt;br/&gt;Working hard is not just a dial you turn up to 11. It's a complicated,
dynamic system that has to be tuned just right at each point. You
have to understand the shape of real work, see clearly what kind
you're best suited for, aim as close to the true core of it as you
can, accurately judge at each moment both what you're capable of
and how you're doing, and put in as many hours each day as you can
without harming the quality of the result. This network is too
complicated to trick. But if you're consistently honest and
clear-sighted, it will automatically assume an optimal shape, and
you'll be productive in a way few people are.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
In "The Bus Ticket Theory of Genius" I said the three ingredients
in great work were natural ability, determination, and interest.
That's the formula in the preceding stage; determination and interest
yield practice and effort.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
I mean this at a resolution of days, not hours. You'll often
get somewhere while not working in the sense that the solution to
a problem comes to you while taking a 
&lt;a href="https://paulgraham.com/top.html"&gt;&lt;u&gt;shower&lt;/u&gt;&lt;/a&gt;, or even in your sleep,
but only because you were working hard on it the day before.&lt;br/&gt;&lt;br/&gt;It's good to go on vacation occasionally, but when I go on vacation,
I like to learn new things. I wouldn't like just sitting on a beach.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
The thing kids do in school that's most like the real version
is sports. Admittedly because many sports originated as games played
in schools. But in this one area, at least, kids are doing exactly
what adults do.&lt;br/&gt;&lt;br/&gt;In the average American high school, you have a choice of pretending
to do something serious, or seriously doing something pretend.
Arguably the latter is no worse.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Knowing what you want to work on doesn't mean you'll be able
to. Most people have to spend a lot of their time working on things
they don't want to, especially early on. But if you know what you
want to do, you at least know what direction to nudge your life in.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
The lower time limits for intense work suggest a solution to
the problem of having less time to work after you have kids: switch
to harder problems. In effect I did that, though not deliberately.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]
Some cultures have a tradition of performative hard work. I
don't love this idea, because (a) it makes a parody of something
important and (b) it causes people to wear themselves out doing
things that don't matter. I don't know enough to say for sure whether
it's net good or bad, but my guess is bad.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f7n"&gt;&lt;font color="#000000"&gt;7&lt;/font&gt;&lt;/a&gt;]
One of the reasons people work so hard on startups is that
startups can fail, and when they do, that failure tends to be both
decisive and conspicuous.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f8n"&gt;&lt;font color="#000000"&gt;8&lt;/font&gt;&lt;/a&gt;]
It's ok to work on something to make a lot of money. You need
to solve the money problem somehow, and there's nothing wrong with
doing that efficiently by trying to make a lot at once. I suppose
it would even be ok to be interested in money for its own sake;
whatever floats your boat. Just so long as you're conscious of your
motivations. The thing to avoid is &lt;i&gt;unconsciously&lt;/i&gt; letting the need
for money warp your ideas about what kind of work you find most
interesting.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f9n"&gt;&lt;font color="#000000"&gt;9&lt;/font&gt;&lt;/a&gt;]
Many people face this question on a smaller scale with
individual projects. But it's easier both to recognize and to accept
a dead end in a single project than to abandon some type of work
entirely. The more determined you are, the harder it gets. Like a
Spanish Flu victim, you're fighting your own immune system: Instead
of giving up, you tell yourself, I should just try harder. And who
can say you're not right?&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, John Carmack, John Collison, Patrick Collison,
Robert Morris, Geoff Ralston, and Harj Taggar for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//hwh.html</guid>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Project of One's Own</title>
      <link>https://paulgraham.com//own.html</link>
      <description>&lt;font face="verdana" size="2"&gt;June 2021&lt;br/&gt;&lt;br/&gt;A few days ago, on the way home from school, my nine year old son
told me he couldn't wait to get home to write more of the story he
was working on. This made me as happy as anything I've heard him
say — not just because he was excited about his story, but because
he'd discovered this way of working. Working on a project of your
own is as different from ordinary work as skating is from walking.
It's more fun, but also much more productive.&lt;br/&gt;&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;June 2021&lt;br/&gt;&lt;br/&gt;A few days ago, on the way home from school, my nine year old son
told me he couldn't wait to get home to write more of the story he
was working on. This made me as happy as anything I've heard him
say — not just because he was excited about his story, but because
he'd discovered this way of working. Working on a project of your
own is as different from ordinary work as skating is from walking.
It's more fun, but also much more productive.&lt;br/&gt;&lt;br/&gt;What proportion of great work has been done by people who were
skating in this sense? If not all of it, certainly a lot.&lt;br/&gt;&lt;br/&gt;There is something special about working on a project of your own.
I wouldn't say exactly that you're happier. A better word would be
excited, or engaged. You're happy when things are going well, but
often they aren't. When I'm writing an essay, most of the time I'm
worried and puzzled: worried that the essay will turn out badly,
and puzzled because I'm groping for some idea that I can't see
clearly enough. Will I be able to pin it down with words? In the
end I usually can, if I take long enough, but I'm never sure; the
first few attempts often fail.&lt;br/&gt;&lt;br/&gt;You have moments of happiness when things work out, but they don't
last long, because then you're on to the next problem. So why do
it at all? Because to the kind of people who like working this way,
nothing else feels as right. You feel as if you're an animal in its
natural habitat, doing what you were meant to do — not always
happy, maybe, but awake and alive.&lt;br/&gt;&lt;br/&gt;Many kids experience the excitement of working on projects of their
own. The hard part is making this converge with the work you do as
an adult. And our customs make it harder. We treat "playing" and
"hobbies" as qualitatively different from "work". It's not clear
to a kid building a treehouse that there's a direct (though long)
route from that to architecture or engineering. And instead of
pointing out the route, we conceal it, by implicitly treating the
stuff kids do as different from real work.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/own.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Instead of telling kids that their treehouses could be on the path
to the work they do as adults, we tell them the path goes through
school. And unfortunately schoolwork tends to be very different from
working on projects of one's own. It's usually neither a project,
nor one's own. So as school gets more serious, working on projects
of one's own is something that survives, if at all, as a thin thread
off to the side.&lt;br/&gt;&lt;br/&gt;It's a bit sad to think of all the high school kids turning their
backs on building treehouses and sitting in class dutifully learning
about Darwin or Newton to pass some exam, when the work that made
Darwin and Newton famous was actually closer in spirit to building
treehouses than studying for exams.&lt;br/&gt;&lt;br/&gt;If I had to choose between my kids getting good grades and 
working on ambitious projects of their own, I'd pick
the projects. And not because I'm an indulgent parent, but because
I've been on the other end and I know which has more predictive
value. When I was picking startups for Y Combinator, I didn't care
about applicants' grades. But if they'd worked on projects of their
own, I wanted to hear all about those.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/own.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;It may be inevitable that school is the way it is. I'm not saying
we have to redesign it (though I'm not saying we don't), just that
we should understand what it does to our attitudes to work — that
it steers us toward the dutiful plodding kind of work, often using
competition as bait, and away from skating.&lt;br/&gt;&lt;br/&gt;There are occasionally times when schoolwork becomes a project of
one's own. Whenever I had to write a paper, that would become a
project of my own — except in English classes, ironically, because
the things one has to write in English classes are so 
&lt;a href="https://paulgraham.com/essay.html"&gt;&lt;u&gt;bogus&lt;/u&gt;&lt;/a&gt;. And
when I got to college and started taking CS classes, the programs
I had to write became projects of my own. Whenever I was writing
or programming, I was usually skating, and that has been true ever
since.&lt;br/&gt;&lt;br/&gt;So where exactly is the edge of projects of one's own? That's an
interesting question, partly because the answer is so complicated,
and partly because there's so much at stake. There turn out to be
two senses in which work can be one's own: 1) that you're doing it
voluntarily, rather than merely because someone told you to, and
2) that you're doing it by yourself.&lt;br/&gt;&lt;br/&gt;The edge of the former is quite sharp. People who care a lot about
their work are usually very sensitive to the difference between
pulling, and being pushed, and work tends to fall into one category
or the other. But the test isn't simply whether you're told to do
something. You can choose to do something you're told to do. Indeed,
you can own it far more thoroughly than the person who told you to
do it.&lt;br/&gt;&lt;br/&gt;For example, math homework is for most people something they're
told to do. But for my father, who was a mathematician, it wasn't.
Most of us think of the problems in a math book as a way to test
or develop our knowledge of the material explained in each section.
But to my father the problems were the part that mattered, and the
text was merely a sort of annotation. Whenever he got a new math
book it was to him like being given a puzzle: here was a new set
of problems to solve, and he'd immediately set about solving all
of them.&lt;br/&gt;&lt;br/&gt;The other sense of a project being one's own — working on it by
oneself — has a much softer edge. It shades gradually into
collaboration. And interestingly, it shades into collaboration in
two different ways. One way to collaborate is to share a single
project. For example, when two mathematicians collaborate on a proof
that takes shape in the course of a conversation between them. The
other way is when multiple people work on separate projects of their
own that fit together like a jigsaw puzzle. For example, when one
person writes the text of a book and another does the graphic design.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/own.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;These two paths into collaboration can of course be combined. But
under the right conditions, the excitement of working on a project
of one's own can be preserved for quite a while before disintegrating
into the turbulent flow of work in a large organization. Indeed,
the history of successful organizations is partly the history of
techniques for preserving that excitement.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/own.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The team that made the original Macintosh were a great example of
this phenomenon. People like Burrell Smith and Andy Hertzfeld and
Bill Atkinson and Susan Kare were not just following orders. They
were not tennis balls hit by Steve Jobs, but rockets let loose by
Steve Jobs. There was a lot of collaboration between them, but
they all seem to have individually felt the excitement of
working on a project of one's own.&lt;br/&gt;&lt;br/&gt;In Andy Hertzfeld's book on the Macintosh, he describes how they'd
come back into the office after dinner and work late into the night.
People who've never experienced the thrill of working on a project
they're excited about can't distinguish this kind of working long
hours from the kind that happens in sweatshops and boiler rooms,
but they're at opposite ends of the spectrum. That's why it's a
mistake to insist dogmatically on "work/life balance." Indeed, the
mere expression "work/life" embodies a mistake: it assumes work and
life are distinct. For those to whom the word "work" automatically
implies the dutiful plodding kind, they are. But for the skaters,
the relationship between work and life would be better represented
by a dash than a slash. I wouldn't want to work on anything that I didn't
want to take over my life.&lt;br/&gt;&lt;br/&gt;Of course, it's easier to achieve this level of motivation when
you're making something like the Macintosh. It's easy for something
new to feel like a project of your own. That's one of the reasons
for the tendency programmers have to rewrite things that don't need
rewriting, and to write their own versions of things that already
exist. This sometimes alarms managers, and measured by total number
of characters typed, it's rarely the optimal solution. But it's not
always driven simply by arrogance or cluelessness.
Writing code from scratch is also much more rewarding — so much
more rewarding that a good programmer can end up net ahead, despite
the shocking waste of characters. Indeed, it may be one of the
advantages of capitalism that it encourages such rewriting. A company
that needs software to do something can't use the software already
written to do it at another company, and thus has to write their
own, which often turns out better.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/own.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The natural alignment between skating and solving new problems is
one of the reasons the payoffs from startups are so high. Not only
is the market price of unsolved problems higher, you also get a
discount on productivity when you work on them. In fact, you get a
double increase in productivity: when you're doing a clean-sheet
design, it's easier to recruit skaters, and they get to spend all
their time skating.&lt;br/&gt;&lt;br/&gt;Steve Jobs knew a thing or two about skaters from having watched
Steve Wozniak. If you can find the right people, you only have to
tell them what to do at the highest level. They'll handle the
details. Indeed, they insist on it. For a project to feel like your
own, you must have sufficient autonomy. You can't be working to
order, or &lt;a href="https://paulgraham.com/artistsship.html"&gt;&lt;u&gt;slowed down&lt;/u&gt;&lt;/a&gt; 
by bureaucracy.&lt;br/&gt;&lt;br/&gt;One way to ensure autonomy is not to have a boss at all. There are
two ways to do that: to be the boss yourself, and to work on projects
outside of work. Though they're at opposite ends of the scale
financially, startups and open source projects have a lot in common,
including the fact that they're often run by skaters. And indeed,
there's a wormhole from one end of the scale to the other: one of
the best ways to discover 
&lt;a href="https://paulgraham.com/startupideas.html"&gt;&lt;u&gt;startup ideas&lt;/u&gt;&lt;/a&gt; is to work on a project
just for fun.&lt;br/&gt;&lt;br/&gt;If your projects are the kind that make money, it's easy to work
on them. It's harder when they're not. And the hardest part, usually,
is morale. That's where adults have it harder than kids. Kids just
plunge in and build their treehouse without worrying about whether
they're wasting their time, or how it compares to other treehouses.
And frankly we could learn a lot from kids here. The high standards
most grownups have for "real" work do not always serve us well.&lt;br/&gt;&lt;br/&gt;The most important phase in a project of one's own is at the
beginning: when you go from thinking it might be cool to do x to
actually doing x. And at that point high standards are not merely
useless but positively harmful. There are a few people who start
too many new projects, but far more, I suspect, who are deterred
by fear of failure from starting projects that would have succeeded
if they had.&lt;br/&gt;&lt;br/&gt;But if we couldn't benefit as kids from the knowledge that our
treehouses were on the path to grownup projects, we can at least
benefit as grownups from knowing that our projects are on a path
that stretches back to treehouses. Remember that careless confidence
you had as a kid when starting something new? That would be a
powerful thing to recapture.&lt;br/&gt;&lt;br/&gt;If it's harder as adults to retain that kind of confidence, we at
least tend to be more aware of what we're doing. Kids bounce, or
are herded, from one kind of work to the next, barely realizing
what's happening to them. Whereas we know more about different types
of work and have more control over which we do. Ideally we can have
the best of both worlds: to be deliberate in choosing to work on
projects of our own, and carelessly confident in starting new ones.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
"Hobby" is a curious word. Now it means work that isn't &lt;i&gt;real&lt;/i&gt;
work — work that one is not to be judged by — but originally it just
meant an obsession in a fairly general sense (even a political
opinion, for example) that one metaphorically rode as a child rides
a hobby-horse. It's hard to say if its recent, narrower meaning is
a change for the better or the worse. For sure there are lots of
false positives — lots of projects that end up being important but
are dismissed initially as mere hobbies. But on the other hand, the
concept provides valuable cover for projects in the early, ugly
duckling phase.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
Tiger parents, as parents so often do, are fighting the last
war. Grades mattered more in the old days when the route to success
was to acquire 
&lt;a href="https://paulgraham.com/credentials.html"&gt;&lt;u&gt;credentials&lt;/u&gt;&lt;/a&gt; 
while ascending some predefined ladder.
But it's just as well that their tactics are focused on grades. How
awful it would be if they invaded the territory of projects, and
thereby gave their kids a distaste for this kind of work by forcing
them to do it. Grades are already a grim, fake world, and aren't
harmed much by parental interference, but working on one's own
projects is a more delicate, private thing that could be damaged
very easily.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
The complicated, gradual edge between working on one's own
projects and collaborating with others is one reason there is so
much disagreement about the idea of the "lone genius." In practice
people collaborate (or not) in all kinds of different ways, but the
idea of the lone genius is definitely not a myth. There's a core
of truth to it that goes with a certain way of working.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Collaboration is powerful too. The optimal organization would
combine collaboration and ownership in such a way as to do the least
damage to each. Interestingly, companies and university departments
approach this ideal from opposite directions: companies insist on
collaboration, and occasionally also manage both to recruit skaters
and allow them to skate, and university departments insist on the
ability to do independent research (which is by custom treated as
skating, whether it is or not), and the people they hire collaborate
as much as they choose.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
If a company could design its software in such a way that the
best newly arrived programmers always got a clean sheet, it could
have a kind of eternal youth. That might not be impossible. If you
had a software backbone defining a game with sufficiently clear
rules, individual programmers could write their own players.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Paul Buchheit, Andy Hertzfeld, Jessica
Livingston, and Peter Norvig for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//own.html</guid>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Fierce Nerds</title>
      <link>https://paulgraham.com//fn.html</link>
      <description>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;Most people think of nerds as quiet, diffident people. In ordinary
social situations they are — as quiet and diffident as the star
quarterback would be if he found himself in the middle of a physics
symposium. And for the same reason: they are fish out of water.
But the apparent diffidence of nerds is an illusion due to the fact
that when non-nerds observe them, it's usually in ordinary social
situations. In fact some nerds are quite fierce.&lt;br/&gt;&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;Most people think of nerds as quiet, diffident people. In ordinary
social situations they are — as quiet and diffident as the star
quarterback would be if he found himself in the middle of a physics
symposium. And for the same reason: they are fish out of water.
But the apparent diffidence of nerds is an illusion due to the fact
that when non-nerds observe them, it's usually in ordinary social
situations. In fact some nerds are quite fierce.&lt;br/&gt;&lt;br/&gt;The fierce nerds are a small but interesting group. They are as a
rule extremely competitive — more competitive, I'd say, than highly
competitive non-nerds. Competition is more personal for them. Partly
perhaps because they're not emotionally mature enough to distance
themselves from it, but also because there's less randomness in the
kinds of competition they engage in, and they are thus more justified
in taking the results personally.&lt;br/&gt;&lt;br/&gt;Fierce nerds also tend to be somewhat overconfident, especially
when young. It might seem like it would be a disadvantage to be
mistaken about one's abilities, but empirically it isn't. Up to a
point, confidence is a self-fullfilling prophecy.&lt;br/&gt;&lt;br/&gt;Another quality you find in most fierce nerds is intelligence. Not
all nerds are smart, but the fierce ones are always at least
moderately so. If they weren't, they wouldn't have the confidence
to be fierce.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/fn.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;There's also a natural connection between nerdiness and
&lt;a href="https://paulgraham.com/think.html"&gt;&lt;u&gt;independent-mindedness&lt;/u&gt;&lt;/a&gt;. It's hard to be 
independent-minded without
being somewhat socially awkward, because conventional beliefs are
so often mistaken, or at least arbitrary. No one who was both
independent-minded and ambitious would want to waste the effort it
takes to fit in. And the independent-mindedness of the fierce nerds
will obviously be of the &lt;a href="https://paulgraham.com/conformism.html"&gt;&lt;u&gt;aggressive&lt;/u&gt;&lt;/a&gt; 
rather than the passive type:
they'll be annoyed by rules, rather than dreamily unaware of them.&lt;br/&gt;&lt;br/&gt;I'm less sure why fierce nerds are impatient, but most seem to be.
You notice it first in conversation, where they tend to interrupt
you. This is merely annoying, but in the more promising fierce nerds
it's connected to a deeper impatience about solving problems. Perhaps
the competitiveness and impatience of fierce nerds are not separate 
qualities, but two manifestations of a single underlying drivenness.&lt;br/&gt;&lt;br/&gt;When you combine all these qualities in sufficient quantities, the
result is quite formidable. The most vivid example of fierce nerds
in action may be James Watson's &lt;i&gt;The Double Helix&lt;/i&gt;. The first sentence
of the book is "I have never seen Francis Crick in a modest mood,"
and the portrait he goes on to paint of Crick is the quintessential
fierce nerd: brilliant, socially awkward, competitive, independent-minded,
overconfident. But so is the implicit portrait he paints of himself.
Indeed, his lack of social awareness makes both portraits that much
more realistic, because he baldly states all sorts of opinions and
motivations that a smoother person would conceal. And moreover it's
clear from the story that Crick and Watson's fierce nerdiness was
integral to their success. Their independent-mindedness caused them
to consider approaches that most others ignored, their overconfidence
allowed them to work on problems they only half understood (they
were literally described as "clowns" by one eminent insider), and
their impatience and competitiveness got them to the answer ahead
of two other groups that would otherwise have found it within the
next year, if not the next several months.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/fn.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The idea that there could be fierce nerds is an unfamiliar one not
just to many normal people but even to some young nerds. Especially
early on, nerds spend so much of their time in ordinary social
situations and so little doing real work that they get a lot more
evidence of their awkwardness than their power. So there will be
some who read this description of the fierce nerd and realize "Hmm,
that's me." And it is to you, young fierce nerd, that I now turn.&lt;br/&gt;&lt;br/&gt;I have some good news, and some bad news. The good news is that
your fierceness will be a great help in solving difficult problems.
And not just the kind of scientific and technical problems that
nerds have traditionally solved. As the world progresses, the number
of things you can win at by getting the right answer increases.
Recently &lt;a href="https://paulgraham.com/richnow.html"&gt;&lt;u&gt;getting rich&lt;/u&gt;&lt;/a&gt; became 
one of them: 7 of the 8 richest people
in America are now fierce nerds.&lt;br/&gt;&lt;br/&gt;Indeed, being a fierce nerd is probably even more helpful in business
than in nerds' original territory of scholarship. Fierceness seems
optional there. Darwin for example doesn't seem to have been
especially fierce. Whereas it's impossible to be the CEO of a company
over a certain size without being fierce, so now that nerds can win
at business, fierce nerds will increasingly monopolize the really
big successes.&lt;br/&gt;&lt;br/&gt;The bad news is that if it's not exercised, your fierceness will
turn to bitterness, and you will become an intellectual playground
bully: the grumpy sysadmin, the forum troll, the 
&lt;a href="https://paulgraham.com/fh.html"&gt;&lt;u&gt;hater&lt;/u&gt;&lt;/a&gt;, the shooter
down of &lt;a href="https://paulgraham.com/newideas.html"&gt;&lt;u&gt;new ideas&lt;/u&gt;&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;How do you avoid this fate? Work on ambitious projects. If you
succeed, it will bring you a kind of satisfaction that neutralizes
bitterness. But you don't need to have succeeded to feel this;
merely working on hard projects gives most fierce nerds some
feeling of satisfaction. And those it doesn't, it at least keeps
busy.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/fn.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Another solution may be to somehow turn off your fierceness, by
devoting yourself to meditation or psychotherapy or something like
that. Maybe that's the right answer for some people. I have no idea.
But it doesn't seem the optimal solution to me. If you're given a
sharp knife, it seems to me better to use it than to blunt its edge
to avoid cutting yourself.&lt;br/&gt;&lt;br/&gt;If you do choose the ambitious route, you'll have a tailwind behind
you. There has never been a better time to be a nerd. In the past
century we've seen a continuous transfer of power from dealmakers
to technicians — from the charismatic to the competent — and I
don't see anything on the horizon that will end it. At least not
till the nerds end it themselves by bringing about the singularity.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
To be a nerd is to be socially awkward, and there are two
distinct ways to do that: to be playing the same game as everyone
else, but badly, and to be playing a different game. The smart nerds
are the latter type.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
The same qualities that make fierce nerds so effective can
also make them very annoying. Fierce nerds would do well to remember
this, and (a) try to keep a lid on it, and (b) seek out organizations
and types of work where getting the right answer matters more than
preserving social harmony. In practice that means small groups
working on hard problems. Which fortunately is the most fun kind
of environment anyway.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
If success neutralizes bitterness, why are there some people
who are at least moderately successful and yet still quite bitter?
Because people's potential bitterness varies depending on how
naturally bitter their personality is, and how ambitious they are:
someone who's naturally very bitter will still have a lot left after
success neutralizes some of it, and someone who's very ambitious
will need proportionally more success to satisfy that ambition.&lt;br/&gt;&lt;br/&gt;So the worst-case scenario is someone who's both naturally bitter
and extremely ambitious, and yet only moderately successful.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Steve Blank, Patrick Collison, Jessica
Livingston, Amjad Masad, and Robert Morris for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//fn.html</guid>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Crazy New Ideas</title>
      <link>https://paulgraham.com//newideas.html</link>
      <description>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;There's one kind of opinion I'd be very afraid to express publicly.
If someone I knew to be both a domain expert and a reasonable person
proposed an idea that sounded preposterous, I'd be very reluctant
to say "That will never work."&lt;br/&gt;&lt;br/&gt;Anyone who has studied the history of ideas, and especially the
history of science, knows that's how big things start. Someone
proposes an idea that sounds crazy, most people dismiss it, then
it gradually take&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;There's one kind of opinion I'd be very afraid to express publicly.
If someone I knew to be both a domain expert and a reasonable person
proposed an idea that sounded preposterous, I'd be very reluctant
to say "That will never work."&lt;br/&gt;&lt;br/&gt;Anyone who has studied the history of ideas, and especially the
history of science, knows that's how big things start. Someone
proposes an idea that sounds crazy, most people dismiss it, then
it gradually takes over the world.&lt;br/&gt;&lt;br/&gt;Most implausible-sounding ideas are in fact bad and could be safely
dismissed. But not when they're proposed by reasonable domain
experts. If the person proposing the idea is reasonable, then they
know how implausible it sounds. And yet they're proposing it anyway.
That suggests they know something you don't. And if they have deep
domain expertise, that's probably the source of it.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/newideas.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Such ideas are not merely unsafe to dismiss, but disproportionately
likely to be interesting. When the average person proposes an
implausible-sounding idea, its implausibility is evidence of their
incompetence. But when a reasonable domain expert does it, the
situation is reversed. There's something like an efficient market
here: on average the ideas that seem craziest will, if correct,
have the biggest effect. So if you can eliminate the theory that
the person proposing an implausible-sounding idea is incompetent,
its implausibility switches from evidence that it's boring to
evidence that it's exciting.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/newideas.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Such ideas are not guaranteed to work. But they don't have to be.
They just have to be sufficiently good bets — to have sufficiently
high expected value. And I think on average they do. I think if you
bet on the entire set of implausible-sounding ideas proposed by
reasonable domain experts, you'd end up net ahead.&lt;br/&gt;&lt;br/&gt;The reason is that everyone is too conservative. The word "paradigm"
is overused, but this is a case where it's warranted. Everyone is
too much in the grip of the current paradigm. Even the people who
have the new ideas undervalue them initially. Which means that
before they reach the stage of proposing them publicly, they've
already subjected them to an excessively strict filter.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/newideas.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The wise response to such an idea is not to make statements, but
to ask questions, because there's a real mystery here. Why has this
smart and reasonable person proposed an idea that seems so wrong?
Are they mistaken, or are you? One of you has to be. If you're the
one who's mistaken, that would be good to know, because it means
there's a hole in your model of the world. But even if they're
mistaken, it should be interesting to learn why. A trap that an
expert falls into is one you have to worry about too.&lt;br/&gt;&lt;br/&gt;This all seems pretty obvious. And yet there are clearly a lot of
people who don't share my fear of dismissing new ideas. Why do they
do it? Why risk looking like a jerk now and a fool later, instead
of just reserving judgement?&lt;br/&gt;&lt;br/&gt;One reason they do it is envy. If you propose a radical new idea
and it succeeds, your reputation (and perhaps also your wealth)
will increase proportionally. Some people would be envious if that
happened, and this potential envy propagates back into a conviction
that you must be wrong.&lt;br/&gt;&lt;br/&gt;Another reason people dismiss new ideas is that it's an easy way
to seem sophisticated. When a new idea first emerges, it usually
seems pretty feeble. It's a mere hatchling. Received wisdom is a
full-grown eagle by comparison. So it's easy to launch a devastating
attack on a new idea, and anyone who does will seem clever to those
who don't understand this asymmetry.&lt;br/&gt;&lt;br/&gt;This phenomenon is exacerbated by the difference between how those
working on new ideas and those attacking them are rewarded. The
rewards for working on new ideas are weighted by the value of the
outcome. So it's worth working on something that only has a 10%
chance of succeeding if it would make things more than 10x better.
Whereas the rewards for attacking new ideas are roughly constant;
such attacks seem roughly equally clever regardless of the target.&lt;br/&gt;&lt;br/&gt;People will also attack new ideas when they have a vested interest
in the old ones. It's not surprising, for example, that some of
Darwin's harshest critics were churchmen. People build whole careers
on some ideas. When someone claims they're false or obsolete, they
feel threatened.&lt;br/&gt;&lt;br/&gt;The lowest form of dismissal is mere factionalism: to automatically
dismiss any idea associated with the opposing faction. The lowest
form of all is to dismiss an idea because of who proposed it.&lt;br/&gt;&lt;br/&gt;But the main thing that leads reasonable people to dismiss new ideas
is the same thing that holds people back from proposing them: the
sheer pervasiveness of the current paradigm. It doesn't just affect
the way we think; it is the Lego blocks we build thoughts out of.
Popping out of the current paradigm is something only a few people
can do. And even they usually have to suppress their intuitions at
first, like a pilot flying through cloud who has to trust his
instruments over his sense of balance.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/newideas.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Paradigms don't just define our present thinking. They also vacuum
up the trail of crumbs that led to them, making our standards for
new ideas impossibly high. The current paradigm seems so perfect
to us, its offspring, that we imagine it must have been accepted
completely as soon as it was discovered — that whatever the church thought
of the heliocentric model, astronomers must have been convinced as
soon as Copernicus proposed it. Far, in fact, from it. Copernicus
published the heliocentric model in 1532, but it wasn't till the
mid seventeenth century that the balance of scientific opinion
shifted in its favor.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/newideas.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Few understand how feeble new ideas look when they first appear.
So if you want to have new ideas yourself, one of the most valuable
things you can do is to learn what they look like when they're born.
Read about how new ideas happened, and try to get yourself into the
heads of people at the time. How did things look to them, when the
new idea was only half-finished, and even the person who had it was
only half-convinced it was right?&lt;br/&gt;&lt;br/&gt;But you don't have to stop at history. You can observe big new ideas
being born all around you right now. Just look for a reasonable
domain expert proposing something that sounds wrong.&lt;br/&gt;&lt;br/&gt;If you're nice, as well as wise, you won't merely resist attacking
such people, but encourage them. Having new ideas is a lonely
business. Only those who've tried it know how lonely. These people
need your help. And if you help them, you'll probably learn something
in the process.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
This domain expertise could be in another field. Indeed,
such crossovers tend to be particularly promising.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
I'm not claiming this principle extends much beyond math,
engineering, and the hard sciences. In politics, for example,
crazy-sounding ideas generally are as bad as they sound. Though
arguably this is not an exception, because the people who propose
them are not in fact domain experts; politicians are domain experts
in political tactics, like how to get elected and how to get
legislation passed, but not in the world that policy acts upon.
Perhaps no one could be.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
This sense of "paradigm" was defined by Thomas Kuhn in his
&lt;i&gt;Structure of Scientific Revolutions&lt;/i&gt;, but I also recommend his
&lt;i&gt;Copernican Revolution&lt;/i&gt;, where you can see him at work developing the
idea.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
This is one reason people with a touch of Asperger's may have
an advantage in discovering new ideas. They're always flying on
instruments.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
Hall, Rupert. &lt;i&gt;From Galileo to Newton.&lt;/i&gt; Collins, 1963. This
book is particularly good at getting into contemporaries' heads.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Patrick Collison, Suhail Doshi, Daniel
Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//newideas.html</guid>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>An NFT That Saves Lives</title>
      <link>https://paulgraham.com//nft.html</link>
      <description>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;&lt;a href="https://www.noorahealth.org/"&gt;Noora Health&lt;/a&gt;, a nonprofit I've 
supported for years, just launched
a new NFT. It has a dramatic name, &lt;a href="http://bit.ly/NooraNFT"&gt;&lt;u&gt;Save Thousands of Lives&lt;/u&gt;&lt;/a&gt;,
because that's what the proceeds will do.&lt;br/&gt;&lt;br/&gt;Noora has been saving lives for 7 years. They run programs in
hospitals in South Asia to teach new mothers how to take care of
their babies once they get home. They're in 165 hospitals no&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;May 2021&lt;br/&gt;&lt;br/&gt;&lt;a href="https://www.noorahealth.org/"&gt;Noora Health&lt;/a&gt;, a nonprofit I've 
supported for years, just launched
a new NFT. It has a dramatic name, &lt;a href="http://bit.ly/NooraNFT"&gt;&lt;u&gt;Save Thousands of Lives&lt;/u&gt;&lt;/a&gt;,
because that's what the proceeds will do.&lt;br/&gt;&lt;br/&gt;Noora has been saving lives for 7 years. They run programs in
hospitals in South Asia to teach new mothers how to take care of
their babies once they get home. They're in 165 hospitals now. And
because they know the numbers before and after they start at a new
hospital, they can measure the impact they have. It is massive.
For every 1000 live births, they save 9 babies.&lt;br/&gt;&lt;br/&gt;This number comes from a &lt;a href="http://bit.ly/NFT-research"&gt;&lt;u&gt;study&lt;/u&gt;&lt;/a&gt;
of 133,733 families at 28 different
hospitals that Noora conducted in collaboration with the Better
Birth team at Ariadne Labs, a joint center for health systems
innovation at Brigham and Womens Hospital and Harvard T.H. Chan
School of Public Health.&lt;br/&gt;&lt;br/&gt;Noora is so effective that even if you measure their costs in the
most conservative way, by dividing their entire budget by the number
of lives saved, the cost of saving a life is the lowest I've seen.
$1,235.&lt;br/&gt;&lt;br/&gt;For this NFT, they're going to issue a public report tracking how
this specific tranche of money is spent, and estimating the number
of lives saved as a result.&lt;br/&gt;&lt;br/&gt;NFTs are a new territory, and this way of using them is especially
new, but I'm excited about its potential. And I'm excited to see
what happens with this particular auction, because unlike an NFT
representing something that has already happened,
this NFT gets better as the price gets higher.&lt;br/&gt;&lt;br/&gt;The reserve price was about $2.5 million, because that's what it
takes for the name to be accurate: that's what it costs to save
2000 lives. But the higher the price of this NFT goes, the more
lives will be saved. What a sentence to be able to write.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//nft.html</guid>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The Real Reason to End the Death Penalty</title>
      <link>https://paulgraham.com//real.html</link>
      <description>&lt;font face="verdana" size="2"&gt;April 2021&lt;br/&gt;&lt;br/&gt;When intellectuals talk about the death penalty, they talk about
things like whether it's permissible for the state to take someone's
life, whether the death penalty acts as a deterrent, and whether
more death sentences are given to some groups than others. But in
practice the debate about the death penalty is not about whether
it's ok to kill murderers. It's about whether it's ok to kill
innocent people, because at least 4% of people on death ro&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;April 2021&lt;br/&gt;&lt;br/&gt;When intellectuals talk about the death penalty, they talk about
things like whether it's permissible for the state to take someone's
life, whether the death penalty acts as a deterrent, and whether
more death sentences are given to some groups than others. But in
practice the debate about the death penalty is not about whether
it's ok to kill murderers. It's about whether it's ok to kill
innocent people, because at least 4% of people on death row are
&lt;a href="https://www.pnas.org/content/111/20/7230"&gt;&lt;u&gt;innocent&lt;/u&gt;&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;When I was a kid I imagined that it was unusual for people to be
convicted of crimes they hadn't committed, and that in murder cases
especially this must be very rare. Far from it. Now, thanks to
organizations like the
&lt;a href="https://innocenceproject.org/all-cases"&gt;&lt;u&gt;Innocence Project&lt;/u&gt;&lt;/a&gt;,
we see a constant stream
of stories about murder convictions being overturned after new
evidence emerges. Sometimes the police and prosecutors were just
very sloppy. Sometimes they were crooked, and knew full well they
were convicting an innocent person.&lt;br/&gt;&lt;br/&gt;Kenneth Adams and three other men spent 18 years in prison on a
murder conviction. They were exonerated after DNA testing implicated
three different men, two of whom later confessed. The police had
been told about the other men early in the investigation, but never
followed up the lead.&lt;br/&gt;&lt;br/&gt;Keith Harward spent 33 years in prison on a murder conviction. He
was convicted because "experts" said his teeth matched photos of
bite marks on one victim. He was exonerated after DNA testing showed
the murder had been committed by another man, Jerry Crotty.&lt;br/&gt;&lt;br/&gt;Ricky Jackson and two other men spent 39 years in prison after being
convicted of murder on the testimony of a 12 year old boy, who later
recanted and said he'd been coerced by police. Multiple people have
confirmed the boy was elsewhere at the time. The three men were
exonerated after the county prosecutor dropped the charges, saying
"The state is conceding the obvious."&lt;br/&gt;&lt;br/&gt;Alfred Brown spent 12 years in prison on a murder conviction,
including 10 years on death row. He was exonerated after it was
discovered that the assistant district attorney had concealed phone
records proving he could not have committed the crimes.&lt;br/&gt;&lt;br/&gt;Glenn Ford spent 29 years on death row after having been convicted
of murder. He was exonerated after new evidence proved he was not
even at the scene when the murder occurred. The attorneys assigned
to represent him had never tried a jury case before.&lt;br/&gt;&lt;br/&gt;Cameron Willingham was actually executed in 2004 by lethal injection.
The "expert" who testified that he deliberately set fire to his
house has since been discredited. A re-examination of the case
ordered by the state of Texas in 2009 concluded that "a finding of
arson could not be sustained."&lt;br/&gt;&lt;br/&gt;&lt;a href="https://saverichardglossip.com/facts"&gt;&lt;u&gt;Rich Glossip&lt;/u&gt;&lt;/a&gt; 
has spent 20 years on death row after being convicted
of murder on the testimony of the actual killer, who escaped with
a life sentence in return for implicating him. In 2015 he came
within minutes of execution before it emerged that Oklahoma had
been planning to kill him with an illegal combination of drugs.
They still plan to go ahead with the execution, perhaps as soon as
this summer, despite 
&lt;a href="https://www.usnews.com/news/best-states/oklahoma/articles/2020-10-14/attorney-for-oklahoma-death-row-inmate-claims-new-evidence"&gt;&lt;u&gt;new 
evidence&lt;/u&gt;&lt;/a&gt; exonerating him.&lt;br/&gt;&lt;br/&gt;I could go on. There are hundreds of similar cases. In Florida
alone, 29 death row prisoners have been exonerated so far.&lt;br/&gt;&lt;br/&gt;Far from being rare, wrongful murder convictions are 
&lt;a href="https://deathpenaltyinfo.org/policy-issues/innocence/description-of-innocence-cases"&gt;&lt;u&gt;very common&lt;/u&gt;&lt;/a&gt;.
Police are under pressure to solve a crime that has gotten a lot
of attention. When they find a suspect, they want to believe he's
guilty, and ignore or even destroy evidence suggesting otherwise.
District attorneys want to be seen as effective and tough on crime,
and in order to win convictions are willing to manipulate witnesses
and withhold evidence. Court-appointed defense attorneys are
overworked and often incompetent. There's a ready supply of criminals
willing to give false testimony in return for a lighter sentence,
suggestible witnesses who can be made to say whatever police want,
and bogus "experts" eager to claim that science proves the defendant
is guilty. And juries want to believe them, since otherwise some
terrible crime remains unsolved.&lt;br/&gt;&lt;br/&gt;This circus of incompetence and dishonesty is the real issue with
the death penalty. We don't even reach the point where theoretical
questions about the moral justification or effectiveness of capital
punishment start to matter, because so many of the people sentenced
to death are actually innocent. Whatever it means in theory, in
practice capital punishment means killing innocent people.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Jessica Livingston, and Don Knight for
reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Related:&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//real.html</guid>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How People Get Rich Now</title>
      <link>https://paulgraham.com//richnow.html</link>
      <description>&lt;font face="verdana" size="2"&gt;April 2021&lt;br/&gt;&lt;br/&gt;Every year since 1982, &lt;i&gt;Forbes&lt;/i&gt; magazine has published a list of the&#13;
richest Americans. If we compare the 100 richest people in 1982 to&#13;
the 100 richest in 2020, we notice some big differences.&lt;br/&gt;&lt;br/&gt;In 1982 the most common source of wealth was inheritance. Of the&#13;
100 richest people, 60 inherited from an ancestor. There were 10&#13;
du Pont heirs alone. By 2020 the number of heirs had been cut in&#13;
half, accounting for only 27 of the biggest&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;April 2021&lt;br/&gt;&lt;br/&gt;Every year since 1982, &lt;i&gt;Forbes&lt;/i&gt; magazine has published a list of the&#13;
richest Americans. If we compare the 100 richest people in 1982 to&#13;
the 100 richest in 2020, we notice some big differences.&lt;br/&gt;&lt;br/&gt;In 1982 the most common source of wealth was inheritance. Of the&#13;
100 richest people, 60 inherited from an ancestor. There were 10&#13;
du Pont heirs alone. By 2020 the number of heirs had been cut in&#13;
half, accounting for only 27 of the biggest 100 fortunes.&lt;br/&gt;&lt;br/&gt;Why would the percentage of heirs decrease? Not because inheritance&#13;
taxes increased. In fact, they decreased significantly during this&#13;
period. The reason the percentage of heirs has decreased is not&#13;
that fewer people are inheriting great fortunes, but that more&#13;
people are making them.&lt;br/&gt;&lt;br/&gt;How are people making these new fortunes? Roughly 3/4 by starting&#13;
companies and 1/4 by investing. Of the 73 new fortunes in 2020, 56&#13;
derive from founders' or early employees' equity (52 founders, 2&#13;
early employees, and 2 wives of founders), and 17 from managing&#13;
investment funds.&lt;br/&gt;&lt;br/&gt;There were no fund managers among the 100 richest Americans in 1982.&#13;
Hedge funds and private equity firms existed in 1982, but none of&#13;
their founders were rich enough yet to make it into the top 100.&#13;
Two things changed: fund managers discovered new ways to generate&#13;
high returns, and more investors were willing to trust them with&#13;
their money.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;But the main source of new fortunes now is starting companies, and&#13;
when you look at the data, you see big changes there too. People&#13;
get richer from starting companies now than they did in 1982, because&#13;
the companies do different things.&lt;br/&gt;&lt;br/&gt;In 1982, there were two dominant sources of new wealth: oil and&#13;
real estate. Of the 40 new fortunes in 1982, at least 24 were due&#13;
primarily to oil or real estate. Now only a small number are: of&#13;
the 73 new fortunes in 2020, 4 were due to real estate and only 2&#13;
to oil.&lt;br/&gt;&lt;br/&gt;By 2020 the biggest source of new wealth was what are sometimes&#13;
called "tech" companies. Of the 73 new fortunes, about 30 derive&#13;
from such companies. These are particularly common among the richest&#13;
of the rich: 8 of the top 10 fortunes in 2020 were new fortunes of&#13;
this type.&lt;br/&gt;&lt;br/&gt;Arguably it's slightly misleading to treat tech as a category.&#13;
Isn't Amazon really a retailer, and Tesla a car maker? Yes and no.&#13;
Maybe in 50 years, when what we call tech is taken for granted, it&#13;
won't seem right to put these two businesses in the same category.&#13;
But at the moment at least, there is definitely something they share&#13;
in common that distinguishes them. What retailer starts AWS? What&#13;
car maker is run by someone who also has a rocket company?&lt;br/&gt;&lt;br/&gt;The tech companies behind the top 100 fortunes also form a&#13;
well-differentiated group in the sense that they're all companies&#13;
that venture capitalists would readily invest in, and the others&#13;
mostly not. And there's a reason why: these are mostly companies&#13;
that win by having better technology, rather than just a CEO who's&#13;
really driven and good at making deals.&lt;br/&gt;&lt;br/&gt;To that extent, the rise of the tech companies represents a qualitative&#13;
change. The oil and real estate magnates of the 1982 Forbes 400&#13;
didn't win by making better technology. They won by being really&#13;
driven and good at making deals. &#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&#13;
And indeed, that way of&#13;
getting rich is so old that it predates the Industrial Revolution.&#13;
The courtiers who got rich in the (nominal) service of European&#13;
royal houses in the 16th and 17th centuries were also, as a rule,&#13;
really driven and good at making deals.&lt;br/&gt;&lt;br/&gt;People who don't look any deeper than the Gini coefficient look&#13;
back on the world of 1982 as the good old days, because those who&#13;
got rich then didn't get as rich. But if you dig into &lt;i&gt;how&lt;/i&gt; they&#13;
got rich, the old days don't look so good. In 1982, 84% of the&#13;
richest 100 people got rich by inheritance, extracting natural&#13;
resources, or doing real estate deals. Is that really better than&#13;
a world in which the richest people get rich by starting tech&#13;
companies?&lt;br/&gt;&lt;br/&gt;Why are people starting so many more new companies than they used&#13;
to, and why are they getting so rich from it? The answer to the&#13;
first question, curiously enough, is that it's misphrased. We&#13;
shouldn't be asking why people are starting companies, but why&#13;
they're starting companies &lt;i&gt;again&lt;/i&gt;.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In 1892, the &lt;i&gt;New York Herald Tribune&lt;/i&gt; compiled a list of all the&#13;
millionaires in America. They found 4047 of them. How many had&#13;
inherited their wealth then? Only about 20%, which is less than the&#13;
proportion of heirs today. And when you investigate the sources of&#13;
the new fortunes, 1892 looks even more like today. Hugh Rockoff&#13;
found that "many of the richest ... gained their initial edge from&#13;
the new technology of mass production."&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;So it's not 2020 that's the anomaly here, but 1982. The real question&#13;
is why so few people had gotten rich from starting companies in&#13;
1982. And the answer is that even as the &lt;i&gt;Herald Tribune&lt;/i&gt;'s list was&#13;
being compiled, a wave of &lt;a href="https://paulgraham.com/re.html"&gt;&lt;u&gt;consolidation&lt;/u&gt;&lt;/a&gt; &#13;
was sweeping through the&#13;
American economy. In the late 19th and early 20th centuries,&#13;
financiers like J. P. Morgan combined thousands of smaller companies&#13;
into a few hundred giant ones with commanding economies of scale.&#13;
By the end of World War II, as Michael Lind writes, "the major&#13;
sectors of the economy were either organized as government-backed&#13;
cartels or dominated by a few oligopolistic corporations."&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In 1960, most of the people who start startups today would have&#13;
gone to work for one of them. You could get rich from starting your&#13;
own company in 1890 and in 2020, but in 1960 it was not really a&#13;
viable option. You couldn't break through the oligopolies to get&#13;
at the markets. So the prestigious route in 1960 was not to start&#13;
your own company, but to work your way up the corporate ladder at&#13;
an existing one.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Making everyone a corporate employee decreased economic inequality&#13;
(and every other kind of variation), but if your model of normal&#13;
is the mid 20th century, you have a very misleading model in that&#13;
respect. J. P. Morgan's economy turned out to be just a phase, and&#13;
starting in the 1970s, it began to break up.&lt;br/&gt;&lt;br/&gt;Why did it break up? Partly senescence. The big companies that&#13;
seemed models of scale and efficiency in 1930 had by 1970 become&#13;
slack and bloated. By 1970 the rigid structure of the economy was&#13;
full of cosy nests that various groups had built to insulate&#13;
themselves from market forces. During the Carter administration the&#13;
federal government realized something was amiss and began, in a&#13;
process they called "deregulation," to roll back the policies that&#13;
propped up the oligopolies.&lt;br/&gt;&lt;br/&gt;But it wasn't just decay from within that broke up J. P. Morgan's&#13;
economy. There was also pressure from without, in the form of new&#13;
technology, and particularly microelectronics. The best way to&#13;
envision what happened is to imagine a pond with a crust of ice on&#13;
top. Initially the only way from the bottom to the surface is around&#13;
the edges. But as the ice crust weakens, you start to be able to&#13;
punch right through the middle.&lt;br/&gt;&lt;br/&gt;The edges of the pond were pure tech: companies that actually&#13;
described themselves as being in the electronics or software business.&#13;
When you used the word "startup" in 1990, that was what you meant.&#13;
But now startups are punching right through the middle of the ice&#13;
crust and displacing incumbents like retailers and TV networks and&#13;
car companies.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f7n"&gt;&lt;font color="#dddddd"&gt;7&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;But though the breakup of J. P. Morgan's economy created a new world&#13;
in the technological sense, it was a reversion to the norm in the&#13;
social sense. If you only look back as far as the mid 20th century,&#13;
it seems like people getting rich by starting their own companies&#13;
is a recent phenomenon. But if you look back further, you realize&#13;
it's actually the default. So what we should expect in the future&#13;
is more of the same. Indeed, we should expect both the number and&#13;
wealth of founders to grow, because every decade it gets easier to&#13;
start a startup.&lt;br/&gt;&lt;br/&gt;Part of the reason it's getting easier to start a startup is social.&#13;
Society is (re)assimilating the concept. If you start one now, your&#13;
parents won't freak out the way they would have a generation ago,&#13;
and knowledge about how to do it is much more widespread. But the&#13;
main reason it's easier to start a startup now is that it's cheaper.&#13;
Technology has driven down the cost of both building products and&#13;
acquiring customers.&lt;br/&gt;&lt;br/&gt;The decreasing cost of starting a startup has in turn changed the&#13;
balance of power between founders and investors. Back when starting&#13;
a startup meant building a factory, you needed investors' permission&#13;
to do it at all. But now investors need founders more than founders&#13;
need investors, and that, combined with the increasing amount of&#13;
venture capital available, has driven up valuations.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f8n"&gt;&lt;font color="#dddddd"&gt;8&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;So the decreasing cost of starting a startup increases the number&#13;
of rich people in two ways: it means that more people start them,&#13;
and that those who do can raise money on better terms.&lt;br/&gt;&lt;br/&gt;But there's also a third factor at work: the companies themselves&#13;
are more valuable, because newly founded companies grow faster than&#13;
they used to. Technology hasn't just made it cheaper to build and&#13;
distribute things, but faster too.&lt;br/&gt;&lt;br/&gt;This trend has been running for a long time. IBM, founded in 1896,&#13;
took 45 years to reach a billion 2020 dollars in revenue.&#13;
Hewlett-Packard, founded in 1939, took 25 years. Microsoft, founded&#13;
in 1975, took 13 years. Now the norm for fast-growing companies is&#13;
7 or 8 years.&#13;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/richnow.html#f9n"&gt;&lt;font color="#dddddd"&gt;9&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Fast growth has a double effect on the value of founders' stock.&#13;
The value of a company is a function of its revenue and its growth&#13;
rate. So if a company grows faster, you not only get to a billion&#13;
dollars in revenue sooner, but the company is more valuable when&#13;
it reaches that point than it would be if it were growing slower.&lt;br/&gt;&lt;br/&gt;That's why founders sometimes get so rich so young now. The low&#13;
initial cost of starting a startup means founders can start young,&#13;
and the fast growth of companies today means that if they succeed&#13;
they could be surprisingly rich just a few years later.&lt;br/&gt;&lt;br/&gt;It's easier now to start and grow a company than it has ever been.&#13;
That means more people start them, that those who do get better&#13;
terms from investors, and that the resulting companies become more&#13;
valuable. Once you understand how these mechanisms work, and that&#13;
startups were suppressed for most of the 20th century, you don't&#13;
have to resort to some vague right turn the country took under&#13;
Reagan to explain why America's Gini coefficient is increasing. Of&#13;
course the Gini coefficient is increasing. With more people starting&#13;
more valuable companies, how could it not be?&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]&#13;
Investment firms grew rapidly after a regulatory change by&#13;
the Labor Department in 1978 allowed pension funds to invest in&#13;
them, but the effects of this growth were not yet visible in the&#13;
top 100 fortunes in 1982.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]&#13;
George Mitchell deserves mention as an exception. Though&#13;
really driven and good at making deals, he was also the first to&#13;
figure out how to use fracking to get natural gas out of shale.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]&#13;
When I say people are starting more companies, I mean the&#13;
type of company meant to &lt;a href="https://paulgraham.com/growth.html"&gt;&lt;u&gt;grow&lt;/u&gt;&lt;/a&gt; &#13;
very big. There has actually been a&#13;
decrease in the last couple decades in the overall number of new&#13;
companies. But the vast majority of companies are small retail and&#13;
service businesses. So what the statistics about the decreasing&#13;
number of new businesses mean is that people are starting fewer&#13;
shoe stores and barber shops.&lt;br/&gt;&lt;br/&gt;People sometimes get &lt;a href="https://www.inc.com/magazine/201505/leigh-buchanan/the-vanishing-startups-in-decline.html"&gt;&lt;u&gt;confused&lt;/u&gt;&lt;/a&gt; when they see a graph labelled&#13;
"startups" that's going down, because there are two senses of the&#13;
word "startup": (1) the founding of a company, and (2) a particular&#13;
type of company designed to grow big fast. The statistics mean&#13;
startup in sense (1), not sense (2).&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]&#13;
Rockoff, Hugh. "Great Fortunes of the Gilded Age." NBER Working&#13;
Paper 14555, 2008.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]&#13;
Lind, Michael. &lt;i&gt;Land of Promise.&lt;/i&gt; HarperCollins, 2012.&lt;br/&gt;&lt;br/&gt;It's also likely that the high tax rates in the mid 20th century&#13;
deterred people from starting their own companies. Starting one's&#13;
own company is risky, and when risk isn't rewarded, people opt for&#13;
&lt;a href="https://paulgraham.com/inequality.html"&gt;&lt;u&gt;safety&lt;/u&gt;&lt;/a&gt; instead.&lt;br/&gt;&lt;br/&gt;But it wasn't simply cause and effect. The oligopolies and high tax&#13;
rates of the mid 20th century were all of a piece. Lower taxes are&#13;
not just a cause of entrepreneurship, but an effect as well: the&#13;
people getting rich in the mid 20th century from real estate and&#13;
oil exploration lobbied for and got huge tax loopholes that made&#13;
their effective tax rate much lower, and presumably if it had been&#13;
more common to grow big companies by building new technology, the&#13;
people doing that would have lobbied for their own loopholes as&#13;
well.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]&#13;
That's why the people who did get rich in the mid 20th century&#13;
so often got rich from oil exploration or real estate. Those were&#13;
the two big areas of the economy that weren't susceptible to&#13;
consolidation.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f7n"&gt;&lt;font color="#000000"&gt;7&lt;/font&gt;&lt;/a&gt;]&#13;
The pure tech companies used to be called "high technology" startups.&#13;
But now that startups can punch through the middle of the ice crust,&#13;
we don't need a separate name for the edges, and the term "high-tech"&#13;
has a decidedly &lt;a href="https://books.google.com/ngrams/graph?content=high+tech&amp;amp;year_start=1900&amp;amp;year_end=2019&amp;amp;corpus=en-2019&amp;amp;smoothing=3"&gt;&lt;u&gt;retro&lt;/u&gt;&lt;/a&gt; &#13;
sound.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f8n"&gt;&lt;font color="#000000"&gt;8&lt;/font&gt;&lt;/a&gt;]&#13;
Higher valuations mean you either sell less stock to get a&#13;
given amount of money, or get more money for a given amount of&#13;
stock. The typical startup does some of each. Obviously you end up&#13;
richer if you keep more stock, but you should also end up richer&#13;
if you raise more money, because (a) it should make the company&#13;
more successful, and (b) you should be able to last longer before&#13;
the next round, or not even need one. Notice all those shoulds&#13;
though. In practice a lot of money slips through them.&lt;br/&gt;&lt;br/&gt;It might seem that the huge rounds raised by startups nowadays&#13;
contradict the claim that it has become cheaper to start one. But&#13;
there's no contradiction here; the startups that raise the most are&#13;
the ones doing it by choice, in order to grow faster, not the ones&#13;
doing it because they need the money to survive. There's nothing&#13;
like not needing money to make people offer it to you.&lt;br/&gt;&lt;br/&gt;You would think, after having been on the side of labor in its fight&#13;
with capital for almost two centuries, that the far left would be&#13;
happy that labor has finally prevailed. But none of them seem to&#13;
be. You can almost hear them saying "No, no, not &lt;i&gt;that&lt;/i&gt; way."&lt;br/&gt;&lt;br/&gt;[&lt;a name="f9n"&gt;&lt;font color="#000000"&gt;9&lt;/font&gt;&lt;/a&gt;]&#13;
IBM was created in 1911 by merging three companies, the most&#13;
important of which was Herman Hollerith's Tabulating Machine Company,&#13;
founded in 1896. In 1941 its revenues were $60 million.&lt;br/&gt;&lt;br/&gt;Hewlett-Packard's revenues in 1964 were $125 million.&lt;br/&gt;&lt;br/&gt;Microsoft's revenues in 1988 were $590 million.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Jessica Livingston, Bob Lesko, Robert Morris, &#13;
Russ Roberts, and Alex Tabarrok for reading drafts of this, and to Jon Erlichman for growth data.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//richnow.html</guid>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Write Simply</title>
      <link>https://paulgraham.com//simply.html</link>
      <description>&lt;font face="verdana" size="2"&gt;March 2021&lt;br/&gt;&lt;br/&gt;I try to write using ordinary words and simple sentences.&lt;br/&gt;&lt;br/&gt;That kind of writing is easier to read, and the easier something
is to read, the more deeply readers will engage with it. The less
energy they expend on your prose, the more they'll have left for
your ideas.&lt;br/&gt;&lt;br/&gt;And the further they'll read. Most readers' energy tends to flag
part way through an article or essay. If the friction of reading
is low enough, more keep going till &lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;March 2021&lt;br/&gt;&lt;br/&gt;I try to write using ordinary words and simple sentences.&lt;br/&gt;&lt;br/&gt;That kind of writing is easier to read, and the easier something
is to read, the more deeply readers will engage with it. The less
energy they expend on your prose, the more they'll have left for
your ideas.&lt;br/&gt;&lt;br/&gt;And the further they'll read. Most readers' energy tends to flag
part way through an article or essay. If the friction of reading
is low enough, more keep going till the end.&lt;br/&gt;&lt;br/&gt;There's an Italian dish called &lt;i&gt;saltimbocca&lt;/i&gt;, which means "leap
into the mouth." My goal when writing might be called &lt;i&gt;saltintesta&lt;/i&gt;:
the ideas leap into your head and you barely notice the words that
got them there.&lt;br/&gt;&lt;br/&gt;It's too much to hope that writing could ever be pure ideas. You
might not even want it to be. But for most writers, most of the
time, that's the goal to aim for. The gap between most writing and
pure ideas is not filled with poetry.&lt;br/&gt;&lt;br/&gt;Plus it's more considerate to write simply. When you write in a
fancy way to impress people, you're making them do extra work just
so you can seem cool. It's like trailing a long train behind you
that readers have to carry.&lt;br/&gt;&lt;br/&gt;And remember, if you're writing in English, that a lot of your
readers won't be native English speakers. Their understanding of
ideas may be way ahead of their understanding of English. So you
can't assume that writing about a difficult topic means you can
use difficult words.&lt;br/&gt;&lt;br/&gt;Of course, fancy writing doesn't just conceal ideas. It can also
conceal the lack of them. That's why some people write that way,
to conceal the fact that they have 
&lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=hermeneutic+dialectics+hegemonic+modalities"&gt;&lt;u&gt;&lt;/u&gt;&lt;/a&gt; nothing to say. Whereas writing
simply keeps you honest. If you say nothing simply, it will be
obvious to everyone, including you.&lt;br/&gt;&lt;br/&gt;Simple writing also lasts better. People reading your stuff in the
future will be in much the same position as people from other
countries reading it today. The culture and the language will have
changed. It's not vain to care about that, any more than it's vain
for a woodworker to build a chair to last.&lt;br/&gt;&lt;br/&gt;Indeed, lasting is not merely an accidental quality of chairs, or
writing. It's a sign you did a good job.&lt;br/&gt;&lt;br/&gt;But although these are all real advantages of writing simply, none
of them are why I do it. The main reason I write simply is that it
offends me not to. When I write a sentence that seems too complicated,
or that uses unnecessarily intellectual words, it doesn't seem fancy
to me. It seems clumsy.&lt;br/&gt;&lt;br/&gt;There are of course times when you want to use a complicated sentence
or fancy word for effect. But you should never do it by accident.&lt;br/&gt;&lt;br/&gt;The other reason my writing ends up being simple is the way I do
it. I write the first draft fast, then spend days editing it, trying
to get everything just right. Much of this editing is cutting, and
that makes simple writing even simpler.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//simply.html</guid>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Donate Unrestricted</title>
      <link>https://paulgraham.com//donate.html</link>
      <description>&lt;font face="verdana" size="2"&gt;March 2021&lt;br/&gt;&lt;br/&gt;The secret curse of the nonprofit world is restricted donations.
If you haven't been involved with nonprofits, you may never have
heard this phrase before. But if you have been, it probably made
you wince.&lt;br/&gt;&lt;br/&gt;Restricted donations mean donations where the donor limits what can
be done with the money. This is common with big donations, perhaps
the default. And yet it's usually a bad idea. Usually the way the
donor wants the money spent is not&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;March 2021&lt;br/&gt;&lt;br/&gt;The secret curse of the nonprofit world is restricted donations.
If you haven't been involved with nonprofits, you may never have
heard this phrase before. But if you have been, it probably made
you wince.&lt;br/&gt;&lt;br/&gt;Restricted donations mean donations where the donor limits what can
be done with the money. This is common with big donations, perhaps
the default. And yet it's usually a bad idea. Usually the way the
donor wants the money spent is not the way the nonprofit would have
chosen. Otherwise there would have been no need to restrict the
donation. But who has a better understanding of where money needs
to be spent, the nonprofit or the donor?&lt;br/&gt;&lt;br/&gt;If a nonprofit doesn't understand better than its donors where money
needs to be spent, then it's incompetent and you shouldn't be
donating to it at all.&lt;br/&gt;&lt;br/&gt;Which means a restricted donation is inherently suboptimal. It's
either a donation to a bad nonprofit, or a donation for the wrong
things.&lt;br/&gt;&lt;br/&gt;There are a couple exceptions to this principle. One is when the
nonprofit is an umbrella organization. It's reasonable to make a
restricted donation to a university, for example, because a university
is only nominally a single nonprofit. Another exception is when the
donor actually does know as much as the nonprofit about where money
needs to be spent. The Gates Foundation, for example, has specific
goals and often makes restricted donations to individual nonprofits
to accomplish them. But unless you're a domain expert yourself or
donating to an umbrella organization, your donation would do more
good if it were unrestricted.&lt;br/&gt;&lt;br/&gt;If restricted donations do less good than unrestricted ones, why
do donors so often make them? Partly because doing good isn't donors'
only motive. They often have other motives as well — to make a mark,
or to generate good publicity
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/donate.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;,
or to comply with regulations
or corporate policies. Many donors may simply never have considered
the distinction between restricted and unrestricted donations. They
may believe that donating money for some specific purpose is just
how donation works. And to be fair, nonprofits don't try very hard
to discourage such illusions. They can't afford to. People running
nonprofits are almost always anxious about money. They can't afford
to talk back to big donors.&lt;br/&gt;&lt;br/&gt;You can't expect candor in a relationship so asymmetric. So I'll
tell you what nonprofits wish they could tell you. If you want to
donate to a nonprofit, donate unrestricted. If you trust them to
spend your money, trust them to decide how.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Note&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
Unfortunately restricted donations tend to generate more
publicity than unrestricted ones. "X donates money to build a school
in Africa" is not only more interesting than "X donates money to Y
nonprofit to spend as Y chooses," but also focuses more attention
on X.&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Chase Adam, Ingrid Bassett, Trevor Blackwell, and Edith
Elliot for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//donate.html</guid>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What I Worked On</title>
      <link>https://paulgraham.com//worked.html</link>
      <description>&lt;font face="verdana" size="2"&gt;February 2021&lt;br/&gt;&lt;br/&gt;Before college the two main things I worked on, outside of school,
were writing and programming. I didn't write essays. I wrote what
beginning writers were supposed to write then, and probably still
are: short stories. My stories were awful. They had hardly any plot,
just characters with strong feelings, which I imagined made them
deep.&lt;br/&gt;&lt;br/&gt;The first programs I tried writing were on the IBM 1401 that our
school district used for what was &lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;February 2021&lt;br/&gt;&lt;br/&gt;Before college the two main things I worked on, outside of school,
were writing and programming. I didn't write essays. I wrote what
beginning writers were supposed to write then, and probably still
are: short stories. My stories were awful. They had hardly any plot,
just characters with strong feelings, which I imagined made them
deep.&lt;br/&gt;&lt;br/&gt;The first programs I tried writing were on the IBM 1401 that our
school district used for what was then called "data processing."
This was in 9th grade, so I was 13 or 14. The school district's
1401 happened to be in the basement of our junior high school, and
my friend Rich Draves and I got permission to use it. It was like
a mini Bond villain's lair down there, with all these alien-looking
machines  CPU, disk drives, printer, card reader  sitting up
on a raised floor under bright fluorescent lights.&lt;br/&gt;&lt;br/&gt;The language we used was an early version of Fortran. You had to
type programs on punch cards, then stack them in the card reader
and press a button to load the program into memory and run it. The
result would ordinarily be to print something on the spectacularly
loud printer.&lt;br/&gt;&lt;br/&gt;I was puzzled by the 1401. I couldn't figure out what to do with
it. And in retrospect there's not much I could have done with it.
The only form of input to programs was data stored on punched cards,
and I didn't have any data stored on punched cards. The only other
option was to do things that didn't rely on any input, like calculate
approximations of pi, but I didn't know enough math to do anything
interesting of that type. So I'm not surprised I can't remember any
programs I wrote, because they can't have done much. My clearest
memory is of the moment I learned it was possible for programs not
to terminate, when one of mine didn't. On a machine without
time-sharing, this was a social as well as a technical error, as
the data center manager's expression made clear.&lt;br/&gt;&lt;br/&gt;With microcomputers, everything changed. Now you could have a
computer sitting right in front of you, on a desk, that could respond
to your keystrokes as it was running instead of just churning through
a stack of punch cards and then stopping. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The first of my friends to get a microcomputer built it himself.
It was sold as a kit by Heathkit. I remember vividly how impressed
and envious I felt watching him sitting in front of it, typing
programs right into the computer.&lt;br/&gt;&lt;br/&gt;Computers were expensive in those days and it took me years of
nagging before I convinced my father to buy one, a TRS-80, in about
1980. The gold standard then was the Apple II, but a TRS-80 was
good enough. This was when I really started programming. I wrote
simple games, a program to predict how high my model rockets would
fly, and a word processor that my father used to write at least one
book. There was only room in memory for about 2 pages of text, so
he'd write 2 pages at a time and then print them out, but it was a
lot better than a typewriter.&lt;br/&gt;&lt;br/&gt;Though I liked programming, I didn't plan to study it in college.
In college I was going to study philosophy, which sounded much more
powerful. It seemed, to my naive high school self, to be the study
of the ultimate truths, compared to which the things studied in
other fields would be mere domain knowledge. What I discovered when
I got to college was that the other fields took up so much of the
space of ideas that there wasn't much left for these supposed
ultimate truths. All that seemed left for philosophy were edge cases
that people in other fields felt could safely be ignored.&lt;br/&gt;&lt;br/&gt;I couldn't have put this into words when I was 18. All I knew at
the time was that I kept taking philosophy courses and they kept
being boring. So I decided to switch to AI.&lt;br/&gt;&lt;br/&gt;AI was in the air in the mid 1980s, but there were two things
especially that made me want to work on it: a novel by Heinlein
called &lt;i&gt;The Moon is a Harsh Mistress&lt;/i&gt;, which featured an intelligent
computer called Mike, and a PBS documentary that showed Terry
Winograd using SHRDLU. I haven't tried rereading &lt;i&gt;The Moon is a Harsh
Mistress&lt;/i&gt;, so I don't know how well it has aged, but when I read it
I was drawn entirely into its world. It seemed only a matter of
time before we'd have Mike, and when I saw Winograd using SHRDLU,
it seemed like that time would be a few years at most. All you had
to do was teach SHRDLU more words.&lt;br/&gt;&lt;br/&gt;There weren't any classes in AI at Cornell then, not even graduate
classes, so I started trying to teach myself. Which meant learning
Lisp, since in those days Lisp was regarded as the language of AI.
The commonly used programming languages then were pretty primitive,
and programmers' ideas correspondingly so. The default language at
Cornell was a Pascal-like language called PL/I, and the situation
was similar elsewhere. Learning Lisp expanded my concept of a program
so fast that it was years before I started to have a sense of where
the new limits were. This was more like it; this was what I had
expected college to do. It wasn't happening in a class, like it was
supposed to, but that was ok. For the next couple years I was on a
roll. I knew what I was going to do.&lt;br/&gt;&lt;br/&gt;For my undergraduate thesis, I reverse-engineered SHRDLU. My God
did I love working on that program. It was a pleasing bit of code,
but what made it even more exciting was my belief  hard to imagine
now, but not unique in 1985  that it was already climbing the
lower slopes of intelligence.&lt;br/&gt;&lt;br/&gt;I had gotten into a program at Cornell that didn't make you choose
a major. You could take whatever classes you liked, and choose
whatever you liked to put on your degree. I of course chose "Artificial
Intelligence." When I got the actual physical diploma, I was dismayed
to find that the quotes had been included, which made them read as
scare-quotes. At the time this bothered me, but now it seems amusingly
accurate, for reasons I was about to discover.&lt;br/&gt;&lt;br/&gt;I applied to 3 grad schools: MIT and Yale, which were renowned for
AI at the time, and Harvard, which I'd visited because Rich Draves
went there, and was also home to Bill Woods, who'd invented the
type of parser I used in my SHRDLU clone. Only Harvard accepted me,
so that was where I went.&lt;br/&gt;&lt;br/&gt;I don't remember the moment it happened, or if there even was a
specific moment, but during the first year of grad school I realized
that AI, as practiced at the time, was a hoax. By which I mean the
sort of AI in which a program that's told "the dog is sitting on
the chair" translates this into some formal representation and adds
it to the list of things it knows.&lt;br/&gt;&lt;br/&gt;What these programs really showed was that there's a subset of
natural language that's a formal language. But a very proper subset.
It was clear that there was an unbridgeable gap between what they
could do and actually understanding natural language. It was not,
in fact, simply a matter of teaching SHRDLU more words. That whole
way of doing AI, with explicit data structures representing concepts,
was not going to work. Its brokenness did, as so often happens,
generate a lot of opportunities to write papers about various
band-aids that could be applied to it, but it was never going to
get us Mike.&lt;br/&gt;&lt;br/&gt;So I looked around to see what I could salvage from the wreckage
of my plans, and there was Lisp. I knew from experience that Lisp
was interesting for its own sake and not just for its association
with AI, even though that was the main reason people cared about
it at the time. So I decided to focus on Lisp. In fact, I decided
to write a book about Lisp hacking. It's scary to think how little
I knew about Lisp hacking when I started writing that book. But
there's nothing like writing a book about something to help you
learn it. The book, &lt;i&gt;On Lisp&lt;/i&gt;, wasn't published till 1993, but I wrote
much of it in grad school.&lt;br/&gt;&lt;br/&gt;Computer Science is an uneasy alliance between two halves, theory
and systems. The theory people prove things, and the systems people
build things. I wanted to build things. I had plenty of respect for
theory  indeed, a sneaking suspicion that it was the more admirable
of the two halves  but building things seemed so much more exciting.&lt;br/&gt;&lt;br/&gt;The problem with systems work, though, was that it didn't last.
Any program you wrote today, no matter how good, would be obsolete
in a couple decades at best. People might mention your software in
footnotes, but no one would actually use it. And indeed, it would
seem very feeble work. Only people with a sense of the history of
the field would even realize that, in its time, it had been good.&lt;br/&gt;&lt;br/&gt;There were some surplus Xerox Dandelions floating around the computer
lab at one point. Anyone who wanted one to play around with could
have one. I was briefly tempted, but they were so slow by present
standards; what was the point? No one else wanted one either, so
off they went. That was what happened to systems work.&lt;br/&gt;&lt;br/&gt;I wanted not just to build things, but to build things that would
last.&lt;br/&gt;&lt;br/&gt;In this dissatisfied state I went in 1988 to visit Rich Draves at
CMU, where he was in grad school. One day I went to visit the
Carnegie Institute, where I'd spent a lot of time as a kid. While
looking at a painting there I realized something that might seem
obvious, but was a big surprise to me. There, right on the wall,
was something you could make that would last. Paintings didn't
become obsolete. Some of the best ones were hundreds of years old.&lt;br/&gt;&lt;br/&gt;And moreover this was something you could make a living doing. Not
as easily as you could by writing software, of course, but I thought
if you were really industrious and lived really cheaply, it had to
be possible to make enough to survive. And as an artist you could
be truly independent. You wouldn't have a boss, or even need to get
research funding.&lt;br/&gt;&lt;br/&gt;I had always liked looking at paintings. Could I make them? I had
no idea. I'd never imagined it was even possible. I knew intellectually
that people made art  that it didn't just appear spontaneously
 but it was as if the people who made it were a different species.
They either lived long ago or were mysterious geniuses doing strange
things in profiles in &lt;i&gt;Life&lt;/i&gt; magazine. The idea of actually being
able to make art, to put that verb before that noun, seemed almost
miraculous.&lt;br/&gt;&lt;br/&gt;That fall I started taking art classes at Harvard. Grad students
could take classes in any department, and my advisor, Tom Cheatham,
was very easy going. If he even knew about the strange classes I
was taking, he never said anything.&lt;br/&gt;&lt;br/&gt;So now I was in a PhD program in computer science, yet planning to
be an artist, yet also genuinely in love with Lisp hacking and
working away at &lt;i&gt;On Lisp&lt;/i&gt;. In other words, like many a grad student,
I was working energetically on multiple projects that were not my
thesis.&lt;br/&gt;&lt;br/&gt;I didn't see a way out of this situation. I didn't want to drop out
of grad school, but how else was I going to get out? I remember
when my friend Robert Morris got kicked out of Cornell for writing
the internet worm of 1988, I was envious that he'd found such a
spectacular way to get out of grad school.&lt;br/&gt;&lt;br/&gt;Then one day in April 1990 a crack appeared in the wall. I ran into
professor Cheatham and he asked if I was far enough along to graduate
that June. I didn't have a word of my dissertation written, but in
what must have been the quickest bit of thinking in my life, I
decided to take a shot at writing one in the 5 weeks or so that
remained before the deadline, reusing parts of &lt;i&gt;On Lisp&lt;/i&gt; where I
could, and I was able to respond, with no perceptible delay "Yes,
I think so. I'll give you something to read in a few days."&lt;br/&gt;&lt;br/&gt;I picked applications of continuations as the topic. In retrospect
I should have written about macros and embedded languages. There's
a whole world there that's barely been explored. But all I wanted
was to get out of grad school, and my rapidly written dissertation
sufficed, just barely.&lt;br/&gt;&lt;br/&gt;Meanwhile I was applying to art schools. I applied to two: RISD in
the US, and the Accademia di Belli Arti in Florence, which, because
it was the oldest art school, I imagined would be good. RISD accepted
me, and I never heard back from the Accademia, so off to Providence
I went.&lt;br/&gt;&lt;br/&gt;I'd applied for the BFA program at RISD, which meant in effect that
I had to go to college again. This was not as strange as it sounds,
because I was only 25, and art schools are full of people of different
ages. RISD counted me as a transfer sophomore and said I had to do
the foundation that summer. The foundation means the classes that
everyone has to take in fundamental subjects like drawing, color,
and design.&lt;br/&gt;&lt;br/&gt;Toward the end of the summer I got a big surprise: a letter from
the Accademia, which had been delayed because they'd sent it to
Cambridge England instead of Cambridge Massachusetts, inviting me
to take the entrance exam in Florence that fall. This was now only
weeks away. My nice landlady let me leave my stuff in her attic. I
had some money saved from consulting work I'd done in grad school;
there was probably enough to last a year if I lived cheaply. Now
all I had to do was learn Italian.&lt;br/&gt;&lt;br/&gt;Only &lt;i&gt;stranieri&lt;/i&gt; (foreigners) had to take this entrance exam. In
retrospect it may well have been a way of excluding them, because
there were so many &lt;i&gt;stranieri&lt;/i&gt; attracted by the idea of studying
art in Florence that the Italian students would otherwise have been
outnumbered. I was in decent shape at painting and drawing from the
RISD foundation that summer, but I still don't know how I managed
to pass the written exam. I remember that I answered the essay
question by writing about Cezanne, and that I cranked up the
intellectual level as high as I could to make the most of my limited
vocabulary. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I'm only up to age 25 and already there are such conspicuous patterns.
Here I was, yet again about to attend some august institution in
the hopes of learning about some prestigious subject, and yet again
about to be disappointed. The students and faculty in the painting
department at the Accademia were the nicest people you could imagine,
but they had long since arrived at an arrangement whereby the
students wouldn't require the faculty to teach anything, and in
return the faculty wouldn't require the students to learn anything.
And at the same time all involved would adhere outwardly to the
conventions of a 19th century atelier. We actually had one of those
little stoves, fed with kindling, that you see in 19th century
studio paintings, and a nude model sitting as close to it as possible
without getting burned. Except hardly anyone else painted her besides
me. The rest of the students spent their time chatting or occasionally
trying to imitate things they'd seen in American art magazines.&lt;br/&gt;&lt;br/&gt;Our model turned out to live just down the street from me. She made
a living from a combination of modelling and making fakes for a
local antique dealer. She'd copy an obscure old painting out of a
book, and then he'd take the copy and maltreat it to make it look
old. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;While I was a student at the Accademia I started painting still
lives in my bedroom at night. These paintings were tiny, because
the room was, and because I painted them on leftover scraps of
canvas, which was all I could afford at the time. Painting still
lives is different from painting people, because the subject, as
its name suggests, can't move. People can't sit for more than about
15 minutes at a time, and when they do they don't sit very still.
So the traditional m.o. for painting people is to know how to paint
a generic person, which you then modify to match the specific person
you're painting. Whereas a still life you can, if you want, copy
pixel by pixel from what you're seeing. You don't want to stop
there, of course, or you get merely photographic accuracy, and what
makes a still life interesting is that it's been through a head.
You want to emphasize the visual cues that tell you, for example,
that the reason the color changes suddenly at a certain point is
that it's the edge of an object. By subtly emphasizing such things
you can make paintings that are more realistic than photographs not
just in some metaphorical sense, but in the strict information-theoretic
sense. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I liked painting still lives because I was curious about what I was
seeing. In everyday life, we aren't consciously aware of much we're
seeing. Most visual perception is handled by low-level processes
that merely tell your brain "that's a water droplet" without telling
you details like where the lightest and darkest points are, or
"that's a bush" without telling you the shape and position of every
leaf. This is a feature of brains, not a bug. In everyday life it
would be distracting to notice every leaf on every bush. But when
you have to paint something, you have to look more closely, and
when you do there's a lot to see. You can still be noticing new
things after days of trying to paint something people usually take
for granted, just as you can &lt;!-- still be noticing new things --&gt; after
days of trying to write an essay about something people usually
take for granted.&lt;br/&gt;&lt;br/&gt;This is not the only way to paint. I'm not 100% sure it's even a
good way to paint. But it seemed a good enough bet to be worth
trying.&lt;br/&gt;&lt;br/&gt;Our teacher, professor Ulivi, was a nice guy. He could see I worked
hard, and gave me a good grade, which he wrote down in a sort of
passport each student had. But the Accademia wasn't teaching me
anything except Italian, and my money was running out, so at the
end of the first year I went back to the US.&lt;br/&gt;&lt;br/&gt;I wanted to go back to RISD, but I was now broke and RISD was very
expensive, so I decided to get a job for a year and then return to
RISD the next fall. I got one at a company called Interleaf, which
made software for creating documents. You mean like Microsoft Word?
Exactly. That was how I learned that low end software tends to eat
high end software. But Interleaf still had a few years to live yet.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Interleaf had done something pretty bold. Inspired by Emacs, they'd
added a scripting language, and even made the scripting language a
dialect of Lisp. Now they wanted a Lisp hacker to write things in
it. This was the closest thing I've had to a normal job, and I
hereby apologize to my boss and coworkers, because I was a bad
employee. Their Lisp was the thinnest icing on a giant C cake, and
since I didn't know C and didn't want to learn it, I never understood
most of the software. Plus I was terribly irresponsible. This was
back when a programming job meant showing up every day during certain
working hours. That seemed unnatural to me, and on this point the
rest of the world is coming around to my way of thinking, but at
the time it caused a lot of friction. Toward the end of the year I
spent much of my time surreptitiously working on &lt;i&gt;On Lisp&lt;/i&gt;, which I
had by this time gotten a contract to publish.&lt;br/&gt;&lt;br/&gt;The good part was that I got paid huge amounts of money, especially
by art student standards. In Florence, after paying my part of the
rent, my budget for everything else had been $7 a day. Now I was
getting paid more than 4 times that every hour, even when I was
just sitting in a meeting. By living cheaply I not only managed to
save enough to go back to RISD, but also paid off my college loans.&lt;br/&gt;&lt;br/&gt;I learned some useful things at Interleaf, though they were mostly
about what not to do. I learned that it's better for technology
companies to be run by product people than sales people (though
sales is a real skill and people who are good at it are really good
at it), that it leads to bugs when code is edited by too many people,
that cheap office space is no bargain if it's depressing, that
planned meetings are inferior to corridor conversations, that big,
bureaucratic customers are a dangerous source of money, and that
there's not much overlap between conventional office hours and the
optimal time for hacking, or conventional offices and the optimal
place for it.&lt;br/&gt;&lt;br/&gt;But the most important thing I learned, and which I used in both
Viaweb and Y Combinator, is that the low end eats the high end:
that it's good to be the "entry level" option, even though that
will be less prestigious, because if you're not, someone else will
be, and will squash you against the ceiling. Which in turn means
that prestige is a danger sign.&lt;br/&gt;&lt;br/&gt;When I left to go back to RISD the next fall, I arranged to do
freelance work for the group that did projects for customers, and
this was how I survived for the next several years. When I came
back to visit for a project later on, someone told me about a new
thing called HTML, which was, as he described it, a derivative of
SGML. Markup language enthusiasts were an occupational hazard at
Interleaf and I ignored him, but this HTML thing later became a big
part of my life.&lt;br/&gt;&lt;br/&gt;In the fall of 1992 I moved back to Providence to continue at RISD.
The foundation had merely been intro stuff, and the Accademia had
been a (very civilized) joke. Now I was going to see what real art
school was like. But alas it was more like the Accademia than not.
Better organized, certainly, and a lot more expensive, but it was
now becoming clear that art school did not bear the same relationship
to art that medical school bore to medicine. At least not the
painting department. The textile department, which my next door
neighbor belonged to, seemed to be pretty rigorous. No doubt
illustration and architecture were too. But painting was post-rigorous.
Painting students were supposed to express themselves, which to the
more worldly ones meant to try to cook up some sort of distinctive
signature style.&lt;br/&gt;&lt;br/&gt;A signature style is the visual equivalent of what in show business
is known as a "schtick": something that immediately identifies the
work as yours and no one else's. For example, when you see a painting
that looks like a certain kind of cartoon, you know it's by Roy
Lichtenstein. So if you see a big painting of this type hanging in
the apartment of a hedge fund manager, you know he paid millions
of dollars for it. That's not always why artists have a signature
style, but it's usually why buyers pay a lot for such work.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;There were plenty of earnest students too: kids who "could draw"
in high school, and now had come to what was supposed to be the
best art school in the country, to learn to draw even better. They
tended to be confused and demoralized by what they found at RISD,
but they kept going, because painting was what they did. I was not
one of the kids who could draw in high school, but at RISD I was
definitely closer to their tribe than the tribe of signature style
seekers.&lt;br/&gt;&lt;br/&gt;I learned a lot in the color class I took at RISD, but otherwise I
was basically teaching myself to paint, and I could do that for
free. So in 1993 I dropped out. I hung around Providence for a bit,
and then my college friend Nancy Parmet did me a big favor. A
rent-controlled apartment in a building her mother owned in New
York was becoming vacant. Did I want it? It wasn't much more than
my current place, and New York was supposed to be where the artists
were. So yes, I wanted it!
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f7n"&gt;&lt;font color="#dddddd"&gt;7&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Asterix comics begin by zooming in on a tiny corner of Roman Gaul
that turns out not to be controlled by the Romans. You can do
something similar on a map of New York City: if you zoom in on the
Upper East Side, there's a tiny corner that's not rich, or at least
wasn't in 1993. It's called Yorkville, and that was my new home.
Now I was a New York artist  in the strictly technical sense of
making paintings and living in New York.&lt;br/&gt;&lt;br/&gt;I was nervous about money, because I could sense that Interleaf was
on the way down. Freelance Lisp hacking work was very rare, and I
didn't want to have to program in another language, which in those
days would have meant C++ if I was lucky. So with my unerring nose
for financial opportunity, I decided to write another book on Lisp.
This would be a popular book, the sort of book that could be used
as a textbook. I imagined myself living frugally off the royalties
and spending all my time painting. (The painting on the cover of
this book, &lt;i&gt;ANSI Common Lisp&lt;/i&gt;, is one that I painted around this
time.)&lt;br/&gt;&lt;br/&gt;The best thing about New York for me was the presence of Idelle and
Julian Weber. Idelle Weber was a painter, one of the early
photorealists, and I'd taken her painting class at Harvard. I've
never known a teacher more beloved by her students. Large numbers
of former students kept in touch with her, including me. After I
moved to New York I became her de facto studio assistant.&lt;br/&gt;&lt;br/&gt;She liked to paint on big, square canvases, 4 to 5 feet on a side.
One day in late 1994 as I was stretching one of these monsters there
was something on the radio about a famous fund manager. He wasn't
that much older than me, and was super rich. The thought suddenly
occurred to me: why don't I become rich? Then I'll be able to work
on whatever I want.&lt;br/&gt;&lt;br/&gt;Meanwhile I'd been hearing more and more about this new thing called
the World Wide Web. Robert Morris showed it to me when I visited
him in Cambridge, where he was now in grad school at Harvard. It
seemed to me that the web would be a big deal. I'd seen what graphical
user interfaces had done for the popularity of microcomputers. It
seemed like the web would do the same for the internet.&lt;br/&gt;&lt;br/&gt;If I wanted to get rich, here was the next train leaving the station.
I was right about that part. What I got wrong was the idea. I decided
we should start a company to put art galleries online. I can't
honestly say, after reading so many Y Combinator applications, that
this was the worst startup idea ever, but it was up there. Art
galleries didn't want to be online, and still don't, not the fancy
ones. That's not how they sell. I wrote some software to generate
web sites for galleries, and Robert wrote some to resize images and
set up an http server to serve the pages. Then we tried to sign up
galleries. To call this a difficult sale would be an understatement.
It was difficult to give away. A few galleries let us make sites
for them for free, but none paid us.&lt;br/&gt;&lt;br/&gt;Then some online stores started to appear, and I realized that
except for the order buttons they were identical to the sites we'd
been generating for galleries. This impressive-sounding thing called
an "internet storefront" was something we already knew how to build.&lt;br/&gt;&lt;br/&gt;So in the summer of 1995, after I submitted the camera-ready copy
of &lt;i&gt;ANSI Common Lisp&lt;/i&gt; to the publishers, we started trying to write
software to build online stores. At first this was going to be
normal desktop software, which in those days meant Windows software.
That was an alarming prospect, because neither of us knew how to
write Windows software or wanted to learn. We lived in the Unix
world. But we decided we'd at least try writing a prototype store
builder on Unix. Robert wrote a shopping cart, and I wrote a new
site generator for stores  in Lisp, of course.&lt;br/&gt;&lt;br/&gt;We were working out of Robert's apartment in Cambridge. His roommate
was away for big chunks of time, during which I got to sleep in his
room. For some reason there was no bed frame or sheets, just a
mattress on the floor. One morning as I was lying on this mattress
I had an idea that made me sit up like a capital L. What if we ran
the software on the server, and let users control it by clicking
on links? Then we'd never have to write anything to run on users'
computers. We could generate the sites on the same server we'd serve
them from. Users wouldn't need anything more than a browser.&lt;br/&gt;&lt;br/&gt;This kind of software, known as a web app, is common now, but at
the time it wasn't clear that it was even possible. To find out,
we decided to try making a version of our store builder that you
could control through the browser. A couple days later, on August
12, we had one that worked. The UI was horrible, but it proved you
could build a whole store through the browser, without any client
software or typing anything into the command line on the server.&lt;br/&gt;&lt;br/&gt;Now we felt like we were really onto something. I had visions of a
whole new generation of software working this way. You wouldn't
need versions, or ports, or any of that crap. At Interleaf there
had been a whole group called Release Engineering that seemed to
be at least as big as the group that actually wrote the software.
Now you could just update the software right on the server.&lt;br/&gt;&lt;br/&gt;We started a new company we called Viaweb, after the fact that our
software worked via the web, and we got $10,000 in seed funding
from Idelle's husband Julian. In return for that and doing the
initial legal work and giving us business advice, we gave him 10%
of the company. Ten years later this deal became the model for Y
Combinator's. We knew founders needed something like this, because
we'd needed it ourselves.&lt;br/&gt;&lt;br/&gt;At this stage I had a negative net worth, because the thousand
dollars or so I had in the bank was more than counterbalanced by
what I owed the government in taxes. (Had I diligently set aside
the proper proportion of the money I'd made consulting for Interleaf?
No, I had not.) So although Robert had his graduate student stipend,
I needed that seed funding to live on.&lt;br/&gt;&lt;br/&gt;We originally hoped to launch in September, but we got more ambitious
about the software as we worked on it. Eventually we managed to
build a WYSIWYG site builder, in the sense that as you were creating
pages, they looked exactly like the static ones that would be
generated later, except that instead of leading to static pages,
the links all referred to closures stored in a hash table on the
server.&lt;br/&gt;&lt;br/&gt;It helped to have studied art, because the main goal of an online
store builder is to make users look legit, and the key to looking
legit is high production values. If you get page layouts and fonts
and colors right, you can make a guy running a store out of his
bedroom look more legit than a big company.&lt;br/&gt;&lt;br/&gt;(If you're curious why my site looks so old-fashioned, it's because
it's still made with this software. It may look clunky today, but
in 1996 it was the last word in slick.)&lt;br/&gt;&lt;br/&gt;In September, Robert rebelled. "We've been working on this for a
month," he said, "and it's still not done." This is funny in
retrospect, because he would still be working on it almost 3 years
later. But I decided it might be prudent to recruit more programmers,
and I asked Robert who else in grad school with him was really good.
He recommended Trevor Blackwell, which surprised me at first, because
at that point I knew Trevor mainly for his plan to reduce everything
in his life to a stack of notecards, which he carried around with
him. But Rtm was right, as usual. Trevor turned out to be a
frighteningly effective hacker.&lt;br/&gt;&lt;br/&gt;It was a lot of fun working with Robert and Trevor. They're the two
most &lt;a href="https://paulgraham.com/think.html"&gt;&lt;u&gt;independent-minded&lt;/u&gt;&lt;/a&gt; people 
I know, and in completely different
ways. If you could see inside Rtm's brain it would look like a
colonial New England church, and if you could see inside Trevor's
it would look like the worst excesses of Austrian Rococo.&lt;br/&gt;&lt;br/&gt;We opened for business, with 6 stores, in January 1996. It was just
as well we waited a few months, because although we worried we were
late, we were actually almost fatally early. There was a lot of
talk in the press then about ecommerce, but not many people actually
wanted online stores.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f8n"&gt;&lt;font color="#dddddd"&gt;8&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;There were three main parts to the software: the editor, which
people used to build sites and which I wrote, the shopping cart,
which Robert wrote, and the manager, which kept track of orders and
statistics, and which Trevor wrote. In its time, the editor was one
of the best general-purpose site builders. I kept the code tight
and didn't have to integrate with any other software except Robert's
and Trevor's, so it was quite fun to work on. If all I'd had to do
was work on this software, the next 3 years would have been the
easiest of my life. Unfortunately I had to do a lot more, all of
it stuff I was worse at than programming, and the next 3 years were
instead the most stressful.&lt;br/&gt;&lt;br/&gt;There were a lot of startups making ecommerce software in the second
half of the 90s. We were determined to be the Microsoft Word, not
the Interleaf. Which meant being easy to use and inexpensive. It
was lucky for us that we were poor, because that caused us to make
Viaweb even more inexpensive than we realized. We charged $100 a
month for a small store and $300 a month for a big one. This low
price was a big attraction, and a constant thorn in the sides of
competitors, but it wasn't because of some clever insight that we
set the price low. We had no idea what businesses paid for things.
$300 a month seemed like a lot of money to us.&lt;br/&gt;&lt;br/&gt;We did a lot of things right by accident like that. For example,
we did what's now called "doing things that 
&lt;a href="https://paulgraham.com/ds.html"&gt;&lt;u&gt;don't scale&lt;/u&gt;&lt;/a&gt;," although
at the time we would have described it as "being so lame that we're
driven to the most desperate measures to get users." The most common
of which was building stores for them. This seemed particularly
humiliating, since the whole raison d'etre of our software was that
people could use it to make their own stores. But anything to get
users.&lt;br/&gt;&lt;br/&gt;We learned a lot more about retail than we wanted to know. For
example, that if you could only have a small image of a man's shirt
(and all images were small then by present standards), it was better
to have a closeup of the collar than a picture of the whole shirt.
The reason I remember learning this was that it meant I had to
rescan about 30 images of men's shirts. My first set of scans were
so beautiful too.&lt;br/&gt;&lt;br/&gt;Though this felt wrong, it was exactly the right thing to be doing.
Building stores for users taught us about retail, and about how it
felt to use our software. I was initially both mystified and repelled
by "business" and thought we needed a "business person" to be in
charge of it, but once we started to get users, I was converted,
in much the same way I was converted to 
&lt;a href="https://paulgraham.com/kids.html"&gt;&lt;u&gt;fatherhood&lt;/u&gt;&lt;/a&gt; once I had kids.
Whatever users wanted, I was all theirs. Maybe one day we'd have
so many users that I couldn't scan their images for them, but in
the meantime there was nothing more important to do.&lt;br/&gt;&lt;br/&gt;Another thing I didn't get at the time is that 
&lt;a href="https://paulgraham.com/growth.html"&gt;&lt;u&gt;growth rate&lt;/u&gt;&lt;/a&gt; is the
ultimate test of a startup. Our growth rate was fine. We had about
70 stores at the end of 1996 and about 500 at the end of 1997. I
mistakenly thought the thing that mattered was the absolute number
of users. And that is the thing that matters in the sense that
that's how much money you're making, and if you're not making enough,
you might go out of business. But in the long term the growth rate
takes care of the absolute number. If we'd been a startup I was
advising at Y Combinator, I would have said: Stop being so stressed
out, because you're doing fine. You're growing 7x a year. Just don't
hire too many more people and you'll soon be profitable, and then
you'll control your own destiny.&lt;br/&gt;&lt;br/&gt;Alas I hired lots more people, partly because our investors wanted
me to, and partly because that's what startups did during the
Internet Bubble. A company with just a handful of employees would
have seemed amateurish. So we didn't reach breakeven until about
when Yahoo bought us in the summer of 1998. Which in turn meant we
were at the mercy of investors for the entire life of the company.
And since both we and our investors were noobs at startups, the
result was a mess even by startup standards.&lt;br/&gt;&lt;br/&gt;It was a huge relief when Yahoo bought us. In principle our Viaweb
stock was valuable. It was a share in a business that was profitable
and growing rapidly. But it didn't feel very valuable to me; I had
no idea how to value a business, but I was all too keenly aware of
the near-death experiences we seemed to have every few months. Nor
had I changed my grad student lifestyle significantly since we
started. So when Yahoo bought us it felt like going from rags to
riches. Since we were going to California, I bought a car, a yellow
1998 VW GTI. I remember thinking that its leather seats alone were
by far the most luxurious thing I owned.&lt;br/&gt;&lt;br/&gt;The next year, from the summer of 1998 to the summer of 1999, must
have been the least productive of my life. I didn't realize it at
the time, but I was worn out from the effort and stress of running
Viaweb. For a while after I got to California I tried to continue
my usual m.o. of programming till 3 in the morning, but fatigue
combined with Yahoo's prematurely aged
&lt;a href="https://paulgraham.com/yahoo.html"&gt;&lt;u&gt;culture&lt;/u&gt;&lt;/a&gt; and grim cube farm
in Santa Clara gradually dragged me down. After a few months it
felt disconcertingly like working at Interleaf.&lt;br/&gt;&lt;br/&gt;Yahoo had given us a lot of options when they bought us. At the
time I thought Yahoo was so overvalued that they'd never be worth
anything, but to my astonishment the stock went up 5x in the next
year. I hung on till the first chunk of options vested, then in the
summer of 1999 I left. It had been so long since I'd painted anything
that I'd half forgotten why I was doing this. My brain had been
entirely full of software and men's shirts for 4 years. But I had
done this to get rich so I could paint, I reminded myself, and now
I was rich, so I should go paint.&lt;br/&gt;&lt;br/&gt;When I said I was leaving, my boss at Yahoo had a long conversation
with me about my plans. I told him all about the kinds of pictures
I wanted to paint. At the time I was touched that he took such an
interest in me. Now I realize it was because he thought I was lying.
My options at that point were worth about $2 million a month. If I
was leaving that kind of money on the table, it could only be to
go and start some new startup, and if I did, I might take people
with me. This was the height of the Internet Bubble, and Yahoo was
ground zero of it. My boss was at that moment a billionaire. Leaving
then to start a new startup must have seemed to him an insanely,
and yet also plausibly, ambitious plan.&lt;br/&gt;&lt;br/&gt;But I really was quitting to paint, and I started immediately.
There was no time to lose. I'd already burned 4 years getting rich.
Now when I talk to founders who are leaving after selling their
companies, my advice is always the same: take a vacation. That's
what I should have done, just gone off somewhere and done nothing
for a month or two, but the idea never occurred to me.&lt;br/&gt;&lt;br/&gt;So I tried to paint, but I just didn't seem to have any energy or
ambition. Part of the problem was that I didn't know many people
in California. I'd compounded this problem by buying a house up in
the Santa Cruz Mountains, with a beautiful view but miles from
anywhere. I stuck it out for a few more months, then in desperation
I went back to New York, where unless you understand about rent
control you'll be surprised to hear I still had my apartment, sealed
up like a tomb of my old life. Idelle was in New York at least, and
there were other people trying to paint there, even though I didn't
know any of them.&lt;br/&gt;&lt;br/&gt;When I got back to New York I resumed my old life, except now I was
rich. It was as weird as it sounds. I resumed all my old patterns,
except now there were doors where there hadn't been. Now when I was
tired of walking, all I had to do was raise my hand, and (unless
it was raining) a taxi would stop to pick me up. Now when I walked
past charming little restaurants I could go in and order lunch. It
was exciting for a while. Painting started to go better. I experimented
with a new kind of still life where I'd paint one painting in the
old way, then photograph it and print it, blown up, on canvas, and
then use that as the underpainting for a second still life, painted
from the same objects (which hopefully hadn't rotted yet).&lt;br/&gt;&lt;br/&gt;Meanwhile I looked for an apartment to buy. Now I could actually
choose what neighborhood to live in. Where, I asked myself and
various real estate agents, is the Cambridge of New York? Aided by
occasional visits to actual Cambridge, I gradually realized there
wasn't one. Huh.&lt;br/&gt;&lt;br/&gt;Around this time, in the spring of 2000, I had an idea. It was clear
from our experience with Viaweb that web apps were the future. Why
not build a web app for making web apps? Why not let people edit
code on our server through the browser, and then host the resulting
applications for them?
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f9n"&gt;&lt;font color="#dddddd"&gt;9&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
You could run all sorts of services
on the servers that these applications could use just by making an
API call: making and receiving phone calls, manipulating images,
taking credit card payments, etc.&lt;br/&gt;&lt;br/&gt;I got so excited about this idea that I couldn't think about anything
else. It seemed obvious that this was the future. I didn't particularly
want to start another company, but it was clear that this idea would
have to be embodied as one, so I decided to move to Cambridge and
start it. I hoped to lure Robert into working on it with me, but
there I ran into a hitch. Robert was now a postdoc at MIT, and
though he'd made a lot of money the last time I'd lured him into
working on one of my schemes, it had also been a huge time sink.
So while he agreed that it sounded like a plausible idea, he firmly
refused to work on it.&lt;br/&gt;&lt;br/&gt;Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had
worked for Viaweb, and two undergrads who wanted summer jobs, and
we got to work trying to build what it's now clear is about twenty
companies and several open source projects worth of software. The
language for defining applications would of course be a dialect of
Lisp. But I wasn't so naive as to assume I could spring an overt
Lisp on a general audience; we'd hide the parentheses, like Dylan
did.&lt;br/&gt;&lt;br/&gt;By then there was a name for the kind of company Viaweb was, an
"application service provider," or ASP. This name didn't last long
before it was replaced by "software as a service," but it was current
for long enough that I named this new company after it: it was going
to be called Aspra.&lt;br/&gt;&lt;br/&gt;I started working on the application builder, Dan worked on network
infrastructure, and the two undergrads worked on the first two
services (images and phone calls). But about halfway through the
summer I realized I really didn't want to run a company  especially
not a big one, which it was looking like this would have to be. I'd
only started Viaweb because I needed the money. Now that I didn't
need money anymore, why was I doing this? If this vision had to be
realized as a company, then screw the vision. I'd build a subset
that could be done as an open source project.&lt;br/&gt;&lt;br/&gt;Much to my surprise, the time I spent working on this stuff was not
wasted after all. After we started Y Combinator, I would often
encounter startups working on parts of this new architecture, and
it was very useful to have spent so much time thinking about it and
even trying to write some of it.&lt;br/&gt;&lt;br/&gt;The subset I would build as an open source project was the new Lisp,
whose parentheses I now wouldn't even have to hide. A lot of Lisp
hackers dream of building a new Lisp, partly because one of the
distinctive features of the language is that it has dialects, and
partly, I think, because we have in our minds a Platonic form of
Lisp that all existing dialects fall short of. I certainly did. So
at the end of the summer Dan and I switched to working on this new
dialect of Lisp, which I called Arc, in a house I bought in Cambridge.&lt;br/&gt;&lt;br/&gt;The following spring, lightning struck. I was invited to give a
talk at a Lisp conference, so I gave one about how we'd used Lisp
at Viaweb. Afterward I put a postscript file of this talk online,
on paulgraham.com, which I'd created years before using Viaweb but
had never used for anything. In one day it got 30,000 page views.
What on earth had happened? The referring urls showed that someone
had posted it on Slashdot.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f10n"&gt;&lt;font color="#dddddd"&gt;10&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Wow, I thought, there's an audience. If I write something and put
it on the web, anyone can read it. That may seem obvious now, but
it was surprising then. In the print era there was a narrow channel
to readers, guarded by fierce monsters known as editors. The only
way to get an audience for anything you wrote was to get it published
as a book, or in a newspaper or magazine. Now anyone could publish
anything.&lt;br/&gt;&lt;br/&gt;This had been possible in principle since 1993, but not many people
had realized it yet. I had been intimately involved with building
the infrastructure of the web for most of that time, and a writer
as well, and it had taken me 8 years to realize it. Even then it
took me several years to understand the implications. It meant there
would be a whole new generation of 
&lt;a href="https://paulgraham.com/essay.html"&gt;&lt;u&gt;essays&lt;/u&gt;&lt;/a&gt;.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f11n"&gt;&lt;font color="#dddddd"&gt;11&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In the print era, the channel for publishing essays had been
vanishingly small. Except for a few officially anointed thinkers
who went to the right parties in New York, the only people allowed
to publish essays were specialists writing about their specialties.
There were so many essays that had never been written, because there
had been no way to publish them. Now they could be, and I was going
to write them.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f12n"&gt;&lt;font color="#dddddd"&gt;12&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I've worked on several different things, but to the extent there
was a turning point where I figured out what to work on, it was
when I started publishing essays online. From then on I knew that
whatever else I did, I'd always write essays too.&lt;br/&gt;&lt;br/&gt;I knew that online essays would be a 
&lt;a href="https://paulgraham.com/marginal.html"&gt;&lt;u&gt;marginal&lt;/u&gt;&lt;/a&gt; medium at first.
Socially they'd seem more like rants posted by nutjobs on their
GeoCities sites than the genteel and beautifully typeset compositions
published in &lt;i&gt;The New Yorker&lt;/i&gt;. But by this point I knew enough to
find that encouraging instead of discouraging.&lt;br/&gt;&lt;br/&gt;One of the most conspicuous patterns I've noticed in my life is how
well it has worked, for me at least, to work on things that weren't
prestigious. Still life has always been the least prestigious form
of painting. Viaweb and Y Combinator both seemed lame when we started
them. I still get the glassy eye from strangers when they ask what
I'm writing, and I explain that it's an essay I'm going to publish
on my web site. Even Lisp, though prestigious intellectually in
something like the way Latin is, also seems about as hip.&lt;br/&gt;&lt;br/&gt;It's not that unprestigious types of work are good per se. But when
you find yourself drawn to some kind of work despite its current
lack of prestige, it's a sign both that there's something real to
be discovered there, and that you have the right kind of motives.
Impure motives are a big danger for the ambitious. If anything is
going to lead you astray, it will be the desire to impress people.
So while working on things that aren't prestigious doesn't guarantee
you're on the right track, it at least guarantees you're not on the
most common type of wrong one.&lt;br/&gt;&lt;br/&gt;Over the next several years I wrote lots of essays about all kinds
of different topics. O'Reilly reprinted a collection of them as a
book, called &lt;i&gt;Hackers &amp;amp; Painters&lt;/i&gt; after one of the essays in it. I
also worked on spam filters, and did some more painting. I used to
have dinners for a group of friends every thursday night, which
taught me how to cook for groups. And I bought another building in
Cambridge, a former candy factory (and later, twas said, porn
studio), to use as an office.&lt;br/&gt;&lt;br/&gt;One night in October 2003 there was a big party at my house. It was
a clever idea of my friend Maria Daniels, who was one of the thursday
diners. Three separate hosts would all invite their friends to one
party. So for every guest, two thirds of the other guests would be
people they didn't know but would probably like. One of the guests
was someone I didn't know but would turn out to like a lot: a woman
called Jessica Livingston. A couple days later I asked her out.&lt;br/&gt;&lt;br/&gt;Jessica was in charge of marketing at a Boston investment bank.
This bank thought it understood startups, but over the next year,
as she met friends of mine from the startup world, she was surprised
how different reality was. And how colorful their stories were. So
she decided to compile a book of 
&lt;a href="https://www.amazon.com/Founders-Work-Stories-Startups-Early/dp/1430210788"&gt;&lt;u&gt;interviews&lt;/u&gt;&lt;/a&gt; with startup founders.&lt;br/&gt;&lt;br/&gt;When the bank had financial problems and she had to fire half her
staff, she started looking for a new job. In early 2005 she interviewed
for a marketing job at a Boston VC firm. It took them weeks to make
up their minds, and during this time I started telling her about
all the things that needed to be fixed about venture capital. They
should make a larger number of smaller investments instead of a
handful of giant ones, they should be funding younger, more technical
founders instead of MBAs, they should let the founders remain as
CEO, and so on.&lt;br/&gt;&lt;br/&gt;One of my tricks for writing essays had always been to give talks.
The prospect of having to stand up in front of a group of people
and tell them something that won't waste their time is a great
spur to the imagination. When the Harvard Computer Society, the
undergrad computer club, asked me to give a talk, I decided I would
tell them how to start a startup. Maybe they'd be able to avoid the
worst of the mistakes we'd made.&lt;br/&gt;&lt;br/&gt;So I gave this talk, in the course of which I told them that the
best sources of seed funding were successful startup founders,
because then they'd be sources of advice too. Whereupon it seemed
they were all looking expectantly at me. Horrified at the prospect
of having my inbox flooded by business plans (if I'd only known),
I blurted out "But not me!" and went on with the talk. But afterward
it occurred to me that I should really stop procrastinating about
angel investing. I'd been meaning to since Yahoo bought us, and now
it was 7 years later and I still hadn't done one angel investment.&lt;br/&gt;&lt;br/&gt;Meanwhile I had been scheming with Robert and Trevor about projects
we could work on together. I missed working with them, and it seemed
like there had to be something we could collaborate on.&lt;br/&gt;&lt;br/&gt;As Jessica and I were walking home from dinner on March 11, at the
corner of Garden and Walker streets, these three threads converged.
Screw the VCs who were taking so long to make up their minds. We'd
start our own investment firm and actually implement the ideas we'd
been talking about. I'd fund it, and Jessica could quit her job and
work for it, and we'd get Robert and Trevor as partners too.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f13n"&gt;&lt;font color="#dddddd"&gt;13&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Once again, ignorance worked in our favor. We had no idea how to
be angel investors, and in Boston in 2005 there were no Ron Conways
to learn from. So we just made what seemed like the obvious choices,
and some of the things we did turned out to be novel.&lt;br/&gt;&lt;br/&gt;There are multiple components to Y Combinator, and we didn't figure
them all out at once. The part we got first was to be an angel firm.
In those days, those two words didn't go together. There were VC
firms, which were organized companies with people whose job it was
to make investments, but they only did big, million dollar investments.
And there were angels, who did smaller investments, but these were
individuals who were usually focused on other things and made
investments on the side. And neither of them helped founders enough
in the beginning. We knew how helpless founders were in some respects,
because we remembered how helpless we'd been. For example, one thing
Julian had done for us that seemed to us like magic was to get us
set up as a company. We were fine writing fairly difficult software,
but actually getting incorporated, with bylaws and stock and all
that stuff, how on earth did you do that? Our plan was not only to
make seed investments, but to do for startups everything Julian had
done for us.&lt;br/&gt;&lt;br/&gt;YC was not organized as a fund. It was cheap enough to run that we
funded it with our own money. That went right by 99% of readers,
but professional investors are thinking "Wow, that means they got
all the returns." But once again, this was not due to any particular
insight on our part. We didn't know how VC firms were organized.
It never occurred to us to try to raise a fund, and if it had, we
wouldn't have known where to start.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f14n"&gt;&lt;font color="#dddddd"&gt;14&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The most distinctive thing about YC is the batch model: to fund a
bunch of startups all at once, twice a year, and then to spend three
months focusing intensively on trying to help them. That part we
discovered by accident, not merely implicitly but explicitly due
to our ignorance about investing. We needed to get experience as
investors. What better way, we thought, than to fund a whole bunch
of startups at once? We knew undergrads got temporary jobs at tech
companies during the summer. Why not organize a summer program where
they'd start startups instead? We wouldn't feel guilty for being
in a sense fake investors, because they would in a similar sense
be fake founders. So while we probably wouldn't make much money out
of it, we'd at least get to practice being investors on them, and
they for their part would probably have a more interesting summer
than they would working at Microsoft.&lt;br/&gt;&lt;br/&gt;We'd use the building I owned in Cambridge as our headquarters.
We'd all have dinner there once a week  on tuesdays, since I was
already cooking for the thursday diners on thursdays  and after
dinner we'd bring in experts on startups to give talks.&lt;br/&gt;&lt;br/&gt;We knew undergrads were deciding then about summer jobs, so in a
matter of days we cooked up something we called the Summer Founders
Program, and I posted an 
&lt;a href="https://paulgraham.com/summerfounder.html"&gt;&lt;u&gt;announcement&lt;/u&gt;&lt;/a&gt; 
on my site, inviting undergrads
to apply. I had never imagined that writing essays would be a way
to get "deal flow," as investors call it, but it turned out to be
the perfect source.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f15n"&gt;&lt;font color="#dddddd"&gt;15&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
We got 225 applications for the Summer
Founders Program, and we were surprised to find that a lot of them
were from people who'd already graduated, or were about to that
spring. Already this SFP thing was starting to feel more serious
than we'd intended.&lt;br/&gt;&lt;br/&gt;We invited about 20 of the 225 groups to interview in person, and
from those we picked 8 to fund. They were an impressive group. That
first batch included reddit, Justin Kan and Emmett Shear, who went
on to found Twitch, Aaron Swartz, who had already helped write the
RSS spec and would a few years later become a martyr for open access,
and Sam Altman, who would later become the second president of YC.
I don't think it was entirely luck that the first batch was so good.
You had to be pretty bold to sign up for a weird thing like the
Summer Founders Program instead of a summer job at a legit place
like Microsoft or Goldman Sachs.&lt;br/&gt;&lt;br/&gt;The deal for startups was based on a combination of the deal we did
with Julian ($10k for 10%) and what Robert said MIT grad students
got for the summer ($6k). We invested $6k per founder, which in the
typical two-founder case was $12k, in return for 6%. That had to
be fair, because it was twice as good as the deal we ourselves had
taken. Plus that first summer, which was really hot, Jessica brought
the founders free air conditioners.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f16n"&gt;&lt;font color="#dddddd"&gt;16&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Fairly quickly I realized that we had stumbled upon the way to scale
startup funding. Funding startups in batches was more convenient
for us, because it meant we could do things for a lot of startups
at once, but being part of a batch was better for the startups too.
It solved one of the biggest problems faced by founders: the
isolation. Now you not only had colleagues, but colleagues who
understood the problems you were facing and could tell you how they
were solving them.&lt;br/&gt;&lt;br/&gt;As YC grew, we started to notice other advantages of scale. The
alumni became a tight community, dedicated to helping one another,
and especially the current batch, whose shoes they remembered being
in. We also noticed that the startups were becoming one another's
customers. We used to refer jokingly to the "YC GDP," but as YC
grows this becomes less and less of a joke. Now lots of startups
get their initial set of customers almost entirely from among their
batchmates.&lt;br/&gt;&lt;br/&gt;I had not originally intended YC to be a full-time job. I was going
to do three things: hack, write essays, and work on YC. As YC grew,
and I grew more excited about it, it started to take up a lot more
than a third of my attention. But for the first few years I was
still able to work on other things.&lt;br/&gt;&lt;br/&gt;In the summer of 2006, Robert and I started working on a new version
of Arc. This one was reasonably fast, because it was compiled into
Scheme. To test this new Arc, I wrote Hacker News in it. It was
originally meant to be a news aggregator for startup founders and
was called Startup News, but after a few months I got tired of
reading about nothing but startups. Plus it wasn't startup founders
we wanted to reach. It was future startup founders. So I changed
the name to Hacker News and the topic to whatever engaged one's
intellectual curiosity.&lt;br/&gt;&lt;br/&gt;HN was no doubt good for YC, but it was also by far the biggest
source of stress for me. If all I'd had to do was select and help
founders, life would have been so easy. And that implies that HN
was a mistake. Surely the biggest source of stress in one's work
should at least be something close to the core of the work. Whereas
I was like someone who was in pain while running a marathon not
from the exertion of running, but because I had a blister from an
ill-fitting shoe. When I was dealing with some urgent problem during
YC, there was about a 60% chance it had to do with HN, and a 40%
chance it had do with everything else combined.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f17n"&gt;&lt;font color="#dddddd"&gt;17&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;As well as HN, I wrote all of YC's internal software in Arc. But
while I continued to work a good deal &lt;i&gt;in&lt;/i&gt; Arc, I gradually stopped
working &lt;i&gt;on&lt;/i&gt; Arc, partly because I didn't have time to, and partly
because it was a lot less attractive to mess around with the language
now that we had all this infrastructure depending on it. So now my
three projects were reduced to two: writing essays and working on
YC.&lt;br/&gt;&lt;br/&gt;YC was different from other kinds of work I've done. Instead of
deciding for myself what to work on, the problems came to me. Every
6 months there was a new batch of startups, and their problems,
whatever they were, became our problems. It was very engaging work,
because their problems were quite varied, and the good founders
were very effective. If you were trying to learn the most you could
about startups in the shortest possible time, you couldn't have
picked a better way to do it.&lt;br/&gt;&lt;br/&gt;There were parts of the job I didn't like. Disputes between cofounders,
figuring out when people were lying to us, fighting with people who
maltreated the startups, and so on. But I worked hard even at the
parts I didn't like. I was haunted by something Kevin Hale once
said about companies: "No one works harder than the boss." He meant
it both descriptively and prescriptively, and it was the second
part that scared me. I wanted YC to be good, so if how hard I worked
set the upper bound on how hard everyone else worked, I'd better
work very hard.&lt;br/&gt;&lt;br/&gt;One day in 2010, when he was visiting California for interviews,
Robert Morris did something astonishing: he offered me unsolicited
advice. I can only remember him doing that once before. One day at
Viaweb, when I was bent over double from a kidney stone, he suggested
that it would be a good idea for him to take me to the hospital.
That was what it took for Rtm to offer unsolicited advice. So I
remember his exact words very clearly. "You know," he said, "you
should make sure Y Combinator isn't the last cool thing you do."&lt;br/&gt;&lt;br/&gt;At the time I didn't understand what he meant, but gradually it
dawned on me that he was saying I should quit. This seemed strange
advice, because YC was doing great. But if there was one thing rarer
than Rtm offering advice, it was Rtm being wrong. So this set me
thinking. It was true that on my current trajectory, YC would be
the last thing I did, because it was only taking up more of my
attention. It had already eaten Arc, and was in the process of
eating essays too. Either YC was my life's work or I'd have to leave
eventually. And it wasn't, so I would.&lt;br/&gt;&lt;br/&gt;In the summer of 2012 my mother had a stroke, and the cause turned
out to be a blood clot caused by colon cancer. The stroke destroyed
her balance, and she was put in a nursing home, but she really
wanted to get out of it and back to her house, and my sister and I
were determined to help her do it. I used to fly up to Oregon to
visit her regularly, and I had a lot of time to think on those
flights. On one of them I realized I was ready to hand YC over to
someone else.&lt;br/&gt;&lt;br/&gt;I asked Jessica if she wanted to be president, but she didn't, so
we decided we'd try to recruit Sam Altman. We talked to Robert and
Trevor and we agreed to make it a complete changing of the guard.
Up till that point YC had been controlled by the original LLC we
four had started. But we wanted YC to last for a long time, and to
do that it couldn't be controlled by the founders. So if Sam said
yes, we'd let him reorganize YC. Robert and I would retire, and
Jessica and Trevor would become ordinary partners.&lt;br/&gt;&lt;br/&gt;When we asked Sam if he wanted to be president of YC, initially he
said no. He wanted to start a startup to make nuclear reactors.
But I kept at it, and in October 2013 he finally agreed. We decided
he'd take over starting with the winter 2014 batch. For the rest
of 2013 I left running YC more and more to Sam, partly so he could
learn the job, and partly because I was focused on my mother, whose
cancer had returned.&lt;br/&gt;&lt;br/&gt;She died on January 15, 2014. We knew this was coming, but it was
still hard when it did.&lt;br/&gt;&lt;br/&gt;I kept working on YC till March, to help get that batch of startups
through Demo Day, then I checked out pretty completely. (I still
talk to alumni and to new startups working on things I'm interested
in, but that only takes a few hours a week.)&lt;br/&gt;&lt;br/&gt;What should I do next? Rtm's advice hadn't included anything about
that. I wanted to do something completely different, so I decided
I'd paint. I wanted to see how good I could get if I really focused
on it. So the day after I stopped working on YC, I started painting.
I was rusty and it took a while to get back into shape, but it was
at least completely engaging.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f18n"&gt;&lt;font color="#dddddd"&gt;18&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I spent most of the rest of 2014 painting. I'd never been able to
work so uninterruptedly before, and I got to be better than I had
been. Not good enough, but better. Then in November, right in the
middle of a painting, I ran out of steam. Up till that point I'd
always been curious to see how the painting I was working on would
turn out, but suddenly finishing this one seemed like a chore. So
I stopped working on it and cleaned my brushes and haven't painted
since. So far anyway.&lt;br/&gt;&lt;br/&gt;I realize that sounds rather wimpy. But attention is a zero sum
game. If you can choose what to work on, and you choose a project
that's not the best one (or at least a good one) for you, then it's
getting in the way of another project that is. And at 50 there was
some opportunity cost to screwing around.&lt;br/&gt;&lt;br/&gt;I started writing essays again, and wrote a bunch of new ones over
the next few months. I even wrote a couple that 
&lt;a href="https://paulgraham.com/know.html"&gt;&lt;u&gt;weren't&lt;/u&gt;&lt;/a&gt; about
startups. Then in March 2015 I started working on Lisp again.&lt;br/&gt;&lt;br/&gt;The distinctive thing about Lisp is that its core is a language
defined by writing an interpreter in itself. It wasn't originally
intended as a programming language in the ordinary sense. It was
meant to be a formal model of computation, an alternative to the
Turing machine. If you want to write an interpreter for a language
in itself, what's the minimum set of predefined operators you need?
The Lisp that John McCarthy invented, or more accurately discovered,
is an answer to that question.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/worked.html#f19n"&gt;&lt;font color="#dddddd"&gt;19&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;McCarthy didn't realize this Lisp could even be used to program
computers till his grad student Steve Russell suggested it. Russell
translated McCarthy's interpreter into IBM 704 machine language,
and from that point Lisp started also to be a programming language
in the ordinary sense. But its origins as a model of computation
gave it a power and elegance that other languages couldn't match.
It was this that attracted me in college, though I didn't understand
why at the time.&lt;br/&gt;&lt;br/&gt;McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions.
It was missing a lot of things you'd want in a programming language.
So these had to be added, and when they were, they weren't defined
using McCarthy's original axiomatic approach. That wouldn't have
been feasible at the time. McCarthy tested his interpreter by
hand-simulating the execution of programs. But it was already getting
close to the limit of interpreters you could test that way  indeed,
there was a bug in it that McCarthy had overlooked. To test a more
complicated interpreter, you'd have had to run it, and computers
then weren't powerful enough.&lt;br/&gt;&lt;br/&gt;Now they are, though. Now you could continue using McCarthy's
axiomatic approach till you'd defined a complete programming language.
And as long as every change you made to McCarthy's Lisp was a
discoveredness-preserving transformation, you could, in principle,
end up with a complete language that had this quality. Harder to
do than to talk about, of course, but if it was possible in principle,
why not try? So I decided to take a shot at it. It took 4 years,
from March 26, 2015 to October 12, 2019. It was fortunate that I
had a precisely defined goal, or it would have been hard to keep
at it for so long.&lt;br/&gt;&lt;br/&gt;I wrote this new Lisp, called &lt;a href="https://paulgraham.com/bel.html"&gt;&lt;u&gt;Bel&lt;/u&gt;&lt;/a&gt;, 
in itself in Arc. That may sound
like a contradiction, but it's an indication of the sort of trickery
I had to engage in to make this work. By means of an egregious
collection of hacks I managed to make something close enough to an
interpreter written in itself that could actually run. Not fast,
but fast enough to test.&lt;br/&gt;&lt;br/&gt;I had to ban myself from writing essays during most of this time,
or I'd never have finished. In late 2015 I spent 3 months writing
essays, and when I went back to working on Bel I could barely
understand the code. Not so much because it was badly written as
because the problem is so convoluted. When you're working on an
interpreter written in itself, it's hard to keep track of what's
happening at what level, and errors can be practically encrypted
by the time you get them.&lt;br/&gt;&lt;br/&gt;So I said no more essays till Bel was done. But I told few people
about Bel while I was working on it. So for years it must have
seemed that I was doing nothing, when in fact I was working harder
than I'd ever worked on anything. Occasionally after wrestling for
hours with some gruesome bug I'd check Twitter or HN and see someone
asking "Does Paul Graham still code?"&lt;br/&gt;&lt;br/&gt;Working on Bel was hard but satisfying. I worked on it so intensively
that at any given time I had a decent chunk of the code in my head
and could write more there. I remember taking the boys to the
coast on a sunny day in 2015 and figuring out how to deal with some
problem involving continuations while I watched them play in the
tide pools. It felt like I was doing life right. I remember that
because I was slightly dismayed at how novel it felt. The good news
is that I had more moments like this over the next few years.&lt;br/&gt;&lt;br/&gt;In the summer of 2016 we moved to England. We wanted our kids to
see what it was like living in another country, and since I was a
British citizen by birth, that seemed the obvious choice. We only
meant to stay for a year, but we liked it so much that we still
live there. So most of Bel was written in England.&lt;br/&gt;&lt;br/&gt;In the fall of 2019, Bel was finally finished. Like McCarthy's
original Lisp, it's a spec rather than an implementation, although
like McCarthy's Lisp it's a spec expressed as code.&lt;br/&gt;&lt;br/&gt;Now that I could write essays again, I wrote a bunch about topics
I'd had stacked up. I kept writing essays through 2020, but I also
started to think about other things I could work on. How should I
choose what to do? Well, how had I chosen what to work on in the
past? I wrote an essay for myself to answer that question, and I
was surprised how long and messy the answer turned out to be. If
this surprised me, who'd lived it, then I thought perhaps it would
be interesting to other people, and encouraging to those with
similarly messy lives. So I wrote a more detailed version for others
to read, and this is the last sentence of it.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
My experience skipped a step in the evolution of computers:
time-sharing machines with interactive OSes. I went straight from
batch processing to microcomputers, which made microcomputers seem
all the more exciting.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
Italian words for abstract concepts can nearly always be
predicted from their English cognates (except for occasional traps
like &lt;i&gt;polluzione&lt;/i&gt;). It's the everyday words that differ. So if you
string together a lot of abstract concepts with a few simple verbs,
you can make a little Italian go a long way.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
I lived at Piazza San Felice 4, so my walk to the Accademia
went straight down the spine of old Florence: past the Pitti, across
the bridge, past Orsanmichele, between the Duomo and the Baptistery,
and then up Via Ricasoli to Piazza San Marco. I saw Florence at
street level in every possible condition, from empty dark winter
evenings to sweltering summer days when the streets were packed with
tourists.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
You can of course paint people like still lives if you want
to, and they're willing. That sort of portrait is arguably the apex
of still life painting, though the long sitting does tend to produce
pained expressions in the sitters.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
Interleaf was one of many companies that had smart people and
built impressive technology, and yet got crushed by Moore's Law.
In the 1990s the exponential growth in the power of commodity (i.e.
Intel) processors rolled up high-end, special-purpose hardware and
software companies like a bulldozer.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]
The signature style seekers at RISD weren't specifically
mercenary. In the art world, money and coolness are tightly coupled.
Anything expensive comes to be seen as cool, and anything seen as
cool will soon become equally expensive.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f7n"&gt;&lt;font color="#000000"&gt;7&lt;/font&gt;&lt;/a&gt;]
Technically the apartment wasn't rent-controlled but
rent-stabilized, but this is a refinement only New Yorkers would
know or care about. The point is that it was really cheap, less
than half market price.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f8n"&gt;&lt;font color="#000000"&gt;8&lt;/font&gt;&lt;/a&gt;]
Most software you can launch as soon as it's done. But when
the software is an online store builder and you're hosting the
stores, if you don't have any users yet, that fact will be painfully
obvious. So before we could launch publicly we had to launch
privately, in the sense of recruiting an initial set of users and
making sure they had decent-looking stores.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f9n"&gt;&lt;font color="#000000"&gt;9&lt;/font&gt;&lt;/a&gt;]
We'd had a code editor in Viaweb for users to define their
own page styles. They didn't know it, but they were editing Lisp
expressions underneath. But this wasn't an app editor, because the
code ran when the merchants' sites were generated, not when shoppers
visited them.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f10n"&gt;&lt;font color="#000000"&gt;10&lt;/font&gt;&lt;/a&gt;]
This was the first instance of what is now a familiar experience,
and so was what happened next, when I read the comments and found
they were full of angry people. How could I claim that Lisp was
better than other languages? Weren't they all Turing complete?
People who see the responses to essays I write sometimes tell me
how sorry they feel for me, but I'm not exaggerating when I reply
that it has always been like this, since the very beginning. It
comes with the territory. An essay must tell readers things they
&lt;a href="https://paulgraham.com/useful.html"&gt;&lt;u&gt;don't already know&lt;/u&gt;&lt;/a&gt;, and some 
people dislike being told such things.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f11n"&gt;&lt;font color="#000000"&gt;11&lt;/font&gt;&lt;/a&gt;]
People put plenty of stuff on the internet in the 90s of
course, but putting something online is not the same as publishing
it online. Publishing online means you treat the online version as
the (or at least a) primary version.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f12n"&gt;&lt;font color="#000000"&gt;12&lt;/font&gt;&lt;/a&gt;]
There is a general lesson here that our experience with Y
Combinator also teaches: Customs continue to constrain you long
after the restrictions that caused them have disappeared. Customary
VC practice had once, like the customs about publishing essays,
been based on real constraints. Startups had once been much more
expensive to start, and proportionally rare. Now they could be cheap
and common, but the VCs' customs still reflected the old world,
just as customs about writing essays still reflected the constraints
of the print era.&lt;br/&gt;&lt;br/&gt;Which in turn implies that people who are independent-minded (i.e.
less influenced by custom) will have an advantage in fields affected
by rapid change (where customs are more likely to be obsolete).&lt;br/&gt;&lt;br/&gt;Here's an interesting point, though: you can't always predict which
fields will be affected by rapid change. Obviously software and
venture capital will be, but who would have predicted that essay
writing would be?&lt;br/&gt;&lt;br/&gt;[&lt;a name="f13n"&gt;&lt;font color="#000000"&gt;13&lt;/font&gt;&lt;/a&gt;]
Y Combinator was not the original name. At first we were
called Cambridge Seed. But we didn't want a regional name, in case
someone copied us in Silicon Valley, so we renamed ourselves after
one of the coolest tricks in the lambda calculus, the Y combinator.&lt;br/&gt;&lt;br/&gt;I picked orange as our color partly because it's the warmest, and
partly because no VC used it. In 2005 all the VCs used staid colors
like maroon, navy blue, and forest green, because they were trying
to appeal to LPs, not founders. The YC logo itself is an inside
joke: the Viaweb logo had been a white V on a red circle, so I made
the YC logo a white Y on an orange square.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f14n"&gt;&lt;font color="#000000"&gt;14&lt;/font&gt;&lt;/a&gt;]
YC did become a fund for a couple years starting in 2009,
because it was getting so big I could no longer afford to fund it
personally. But after Heroku got bought we had enough money to go
back to being self-funded.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f15n"&gt;&lt;font color="#000000"&gt;15&lt;/font&gt;&lt;/a&gt;]
I've never liked the term "deal flow," because it implies
that the number of new startups at any given time is fixed. This
is not only false, but it's the purpose of YC to falsify it, by
causing startups to be founded that would not otherwise have existed.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f16n"&gt;&lt;font color="#000000"&gt;16&lt;/font&gt;&lt;/a&gt;]
She reports that they were all different shapes and sizes,
because there was a run on air conditioners and she had to get
whatever she could, but that they were all heavier than she could
carry now.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f17n"&gt;&lt;font color="#000000"&gt;17&lt;/font&gt;&lt;/a&gt;]
Another problem with HN was a bizarre edge case that occurs
when you both write essays and run a forum. When you run a forum,
you're assumed to see if not every conversation, at least every
conversation involving you. And when you write essays, people post
highly imaginative misinterpretations of them on forums. Individually
these two phenomena are tedious but bearable, but the combination
is disastrous. You actually have to respond to the misinterpretations,
because the assumption that you're present in the conversation means
that not responding to any sufficiently upvoted misinterpretation
reads as a tacit admission that it's correct. But that in turn
encourages more; anyone who wants to pick a fight with you senses
that now is their chance.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f18n"&gt;&lt;font color="#000000"&gt;18&lt;/font&gt;&lt;/a&gt;]
The worst thing about leaving YC was not working with Jessica
anymore. We'd been working on YC almost the whole time we'd known
each other, and we'd neither tried nor wanted to separate it from
our personal lives, so leaving was like pulling up a deeply rooted
tree.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f19n"&gt;&lt;font color="#000000"&gt;19&lt;/font&gt;&lt;/a&gt;]
One way to get more precise about the concept of invented vs
discovered is to talk about space aliens. Any sufficiently advanced
alien civilization would certainly know about the Pythagorean
theorem, for example. I believe, though with less certainty, that
they would also know about the Lisp in McCarthy's 1960 paper.&lt;br/&gt;&lt;br/&gt;But if so there's no reason to suppose that this is the limit of
the language that might be known to them. Presumably aliens need
numbers and errors and I/O too. So it seems likely there exists at
least one path out of McCarthy's Lisp along which discoveredness
is preserved.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, John Collison, Patrick Collison, Daniel
Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj
Taggar for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//worked.html</guid>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Earnestness</title>
      <link>https://paulgraham.com//earnest.html</link>
      <description>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;Jessica and I have certain words that have special significance
when we're talking about startups. The highest compliment we can
pay to founders is to describe them as "earnest." This is not by
itself a guarantee of success. You could be earnest but incapable.
But when founders are both formidable (another of our words) and
earnest, they're as close to unstoppable as you get.&lt;br/&gt;&lt;br/&gt;Earnestness sounds like a boring, even Victorian virtue. It&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;Jessica and I have certain words that have special significance
when we're talking about startups. The highest compliment we can
pay to founders is to describe them as "earnest." This is not by
itself a guarantee of success. You could be earnest but incapable.
But when founders are both formidable (another of our words) and
earnest, they're as close to unstoppable as you get.&lt;br/&gt;&lt;br/&gt;Earnestness sounds like a boring, even Victorian virtue. It seems
a bit of an anachronism that people in Silicon Valley would care
about it. Why does this matter so much?&lt;br/&gt;&lt;br/&gt;When you call someone earnest, you're making a statement about their
motives. It means both that they're doing something for the right
reasons, and that they're trying as hard as they can. If we imagine
motives as vectors, it means both the direction and the magnitude
are right. Though these are of course related: when people are doing
something for the right reasons, they try harder.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The reason motives matter so much in Silicon Valley is that so many
people there have the wrong ones. Starting a successful startup
makes you rich and famous. So a lot of the people trying to start
them are doing it for those reasons. Instead of what? Instead of
interest in the problem for its own sake. That is the root of
earnestness. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;It's also the hallmark of a nerd. Indeed, when people describe
themselves as "x nerds," what they mean is that they're interested
in x for its own sake, and not because it's cool to be interested
in x, or because of what they can get from it. They're saying they
care so much about x that they're willing to sacrifice seeming cool
for its sake.&lt;br/&gt;&lt;br/&gt;A &lt;a href="https://paulgraham.com/genius.html"&gt;&lt;u&gt;genuine interest&lt;/u&gt;&lt;/a&gt; 
in something is a very powerful motivator  for
some people, the most powerful motivator of all.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
Which is why
it's what Jessica and I look for in founders. But as well as being
a source of strength, it's also a source of vulnerability. Caring
constrains you. The earnest can't easily reply in kind to mocking
banter, or put on a cool facade of nihil admirari. They care too
much. They are doomed to be the straight man. That's a real
disadvantage in your 
&lt;a href="https://paulgraham.com/nerds.html"&gt;&lt;u&gt;teenage years&lt;/u&gt;&lt;/a&gt;, 
when mocking banter and nihil
admirari often have the upper hand. But it becomes an advantage
later.&lt;br/&gt;&lt;br/&gt;It's a commonplace now that the kids who were 
nerds in high school
become the cool kids' bosses later on. But people misunderstand why
this happens. It's not just because the nerds are smarter, but also
because they're more earnest. When the problems get harder than the
fake ones you're given in high school, caring about them starts to
matter.&lt;br/&gt;&lt;br/&gt;Does it always matter? Do the earnest always win? Not always. It
probably doesn't matter much in politics, or in crime, or in certain
types of business that are similar to crime, like gambling, personal
injury law, patent trolling, and so on. Nor does it matter in
academic fields at the more 
&lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=hermeneutic+dialectics+hegemonic+phenomenology+intersectionality"&gt;&lt;u&gt;bogus&lt;/u&gt;&lt;/a&gt; end of the spectrum. And though
I don't know enough to say for sure, it may not matter in some kinds
of humor: it may be possible to be completely cynical and still be
very funny.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Looking at the list of fields I mentioned, there's an obvious
pattern. Except possibly for humor, these are all types of work I'd
avoid like the plague. So that could be a useful heuristic for
deciding which fields to work in: how much does earnestness matter?
Which can in turn presumably be inferred from the prevalence of
nerds at the top.&lt;br/&gt;&lt;br/&gt;Along with "nerd," another word that tends to be associated with
earnestness is "naive." The earnest often seem naive.  It's not
just that they don't have the motives other people have. They often
don't fully grasp that such motives exist. Or they may know
intellectually that they do, but because they don't feel them, they
forget about them.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;It works to be slightly naive not just about motives but also,
believe it or not, about the problems you're working on. Naive
optimism can compensate for the bit rot that 
&lt;a href="https://paulgraham.com/ecw.html"&gt;&lt;u&gt;rapid change&lt;/u&gt;&lt;/a&gt; causes
in established beliefs. You plunge into some problem saying "How
hard can it be?", and then after solving it you learn that it was
till recently insoluble.&lt;br/&gt;&lt;br/&gt;Naivete is an obstacle for anyone who wants to seem sophisticated,
and this is one reason would-be intellectuals find it so difficult
to understand Silicon Valley. It hasn't been safe for such people
to use the word "earnest" outside scare quotes since Oscar Wilde
wrote "The Importance of Being Earnest" in 1895. And yet when you
zoom in on Silicon Valley, right into 
&lt;a href="https://paulgraham.com/jessica.html"&gt;&lt;u&gt;Jessica Livingston's brain&lt;/u&gt;&lt;/a&gt;,
that's what her x-ray vision
is seeking out in founders. Earnestness!
Who'd have guessed? Reporters literally can't believe it when
founders making piles of money say that they started their companies
to make the world better. The situation seems made for mockery.
How can these founders be so naive as not to realize how implausible
they sound?&lt;br/&gt;&lt;br/&gt;Though those asking this question don't realize it, that's not a
rhetorical question.&lt;br/&gt;&lt;br/&gt;A lot of founders are faking it, of course, particularly the smaller
fry, and the soon to be smaller fry. But not all of them. There are
a significant number of founders who really are interested in the
problem they're solving mainly for its own sake.&lt;br/&gt;&lt;br/&gt;Why shouldn't there be? We have no difficulty believing that people
would be interested in history or math or even old bus tickets for
their own sake. Why can't there be people interested in self-driving
cars or social networks for their own sake? When you look at the
question from this side, it seems obvious there would be. And isn't
it likely that having a deep interest in something would be a source
of great energy and resilience? It is in every other field.&lt;br/&gt;&lt;br/&gt;The question really is why we have a blind spot about business.
And the answer to that is obvious if you know enough history. For
most of history, making large amounts of money has not been very
intellectually interesting. In preindustrial times it was never far
from robbery, and some areas of business still retain that character,
except using lawyers instead of soldiers.&lt;br/&gt;&lt;br/&gt;But there are other areas of business where the work is genuinely
interesting. Henry Ford got to spend much of his time working on
interesting technical problems, and for the last several decades
the trend in that direction has been accelerating. It's much easier
now to make a lot of money by working on something you're interested
in than it was &lt;a href="https://paulgraham.com/re.html"&gt;&lt;u&gt;50 years ago&lt;/u&gt;&lt;/a&gt;. 
And that, rather than how fast they
grow, may be the most important change that startups represent.
Though indeed, the fact that the work is genuinely interesting is
a big part of why it gets done so fast.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/earnest.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Can you imagine a more important change than one in the relationship
between intellectual curiosity and money? These are two of the most
powerful forces in the world, and in my lifetime they've become
significantly more aligned. How could you not be fascinated to watch
something like this happening in real time?&lt;br/&gt;&lt;br/&gt;I meant this essay to be about earnestness generally, and now I've
gone and talked about startups again. But I suppose at least it
serves as an example of an x nerd in the wild.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
It's interesting how many different ways there are &lt;i&gt;not&lt;/i&gt; to
be earnest: to be cleverly cynical, to be superficially brilliant,
to be conspicuously virtuous, to be cool, to be sophisticated, to
be orthodox, to be a snob, to bully, to pander, to be on the make.
This pattern suggests that earnestness is not one end of a continuum,
but a target one can fall short of in multiple dimensions.&lt;br/&gt;&lt;br/&gt;Another thing I notice about this list is that it sounds like a
list of the ways people behave on Twitter. Whatever else social
media is, it's a vivid catalogue of ways not to be earnest.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
People's motives are as mixed in Silicon Valley as anywhere
else. Even the founders motivated mostly by money tend to be at
least somewhat interested in the problem they're solving, and even
the founders most interested in the problem they're solving also
like the idea of getting rich. But there's great variation in the
relative proportions of different founders' motivations.&lt;br/&gt;&lt;br/&gt;And when I talk about "wrong" motives, I don't mean morally wrong.
There's nothing morally wrong with starting a startup to make money.
I just mean that those startups don't do as well.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
The most powerful motivator for most people is probably family.
But there are some for whom intellectual curiosity comes first. In
his (wonderful) autobiography, Paul Halmos says explicitly that for
a mathematician, math must come before anything else, including
family. Which at least implies that it did for him.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Interestingly, just as the word "nerd" implies earnestness even
when used as a metaphor, the word "politics" implies the opposite.
It's not only in actual politics that earnestness seems to be a
handicap, but also in office politics and academic politics.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
It's a bigger social error to seem naive in most European
countries than it is in America, and this may be one of subtler
reasons startups are less common there. Founder culture is completely
at odds with sophisticated cynicism.&lt;br/&gt;&lt;br/&gt;The most earnest part of Europe is Scandinavia, and not surprisingly
this is also the region with the highest number of successful
startups per capita.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]
Much of business is schleps, and probably always will be. But
even being a professor is largely schleps. It would be interesting
to collect statistics about the schlep ratios of different jobs,
but I suspect they'd rarely be less than 30%.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Patrick Collison, Suhail Doshi, Jessica
Livingston, Mattias Ljungman, Harj Taggar, and Kyle Vogt for reading
drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//earnest.html</guid>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Billionaires Build</title>
      <link>https://paulgraham.com//ace.html</link>
      <description>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;As I was deciding what to write about next, I was surprised to find
that two separate essays I'd been planning to write were actually
the same.&lt;br/&gt;&lt;br/&gt;The first is about how to ace your Y Combinator interview. There
has been so much nonsense written about this topic that I've been
meaning for years to write something telling founders the truth.&lt;br/&gt;&lt;br/&gt;The second is about something politicians sometimes say  that the
only way to become a b&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;As I was deciding what to write about next, I was surprised to find
that two separate essays I'd been planning to write were actually
the same.&lt;br/&gt;&lt;br/&gt;The first is about how to ace your Y Combinator interview. There
has been so much nonsense written about this topic that I've been
meaning for years to write something telling founders the truth.&lt;br/&gt;&lt;br/&gt;The second is about something politicians sometimes say  that the
only way to become a billionaire is by exploiting people  and why
this is mistaken.&lt;br/&gt;&lt;br/&gt;Keep reading, and you'll learn both simultaneously.&lt;br/&gt;&lt;br/&gt;I know the politicians are mistaken because it was my job to predict
which people will become billionaires. I think I can truthfully say
that I know as much about how to do this as anyone. If the key to
becoming a billionaire  the defining feature of billionaires 
was to exploit people, then I, as a professional billionaire scout,
would surely realize this and look for people who would be good at
it, just as an NFL scout looks for speed in wide receivers.&lt;br/&gt;&lt;br/&gt;But aptitude for exploiting people is not what Y Combinator looks
for at all. In fact, it's the opposite of what they look for. I'll
tell you what they do look for, by explaining how to convince 
Y Combinator to fund you, and you can see for yourself.&lt;br/&gt;&lt;br/&gt;What YC looks for, above all, is founders who understand some group
of users and can make what they want. This is so important that
it's YC's motto: "Make something people want."&lt;br/&gt;&lt;br/&gt;A big company can to some extent force unsuitable products on
unwilling customers, but a startup doesn't have the power to do
that. A startup must sing for its supper, by making things that
genuinely delight its customers. Otherwise it will never get off
the ground.&lt;br/&gt;&lt;br/&gt;Here's where things get difficult, both for you as a founder and
for the YC partners trying to decide whether to fund you. In a
market economy, it's hard to make something people want that they
don't already have. That's the great thing about market economies.
If other people both knew about this need and were able to satisfy
it, they already would be, and there would be no room for your
startup.&lt;br/&gt;&lt;br/&gt;Which means the conversation during your YC interview will have to
be about something new: either a new need, or a new way to satisfy
one. And not just new, but uncertain. If it were certain that the
need existed and that you could satisfy it, that certainty would
be reflected in large and rapidly growing revenues, and you wouldn't
be seeking seed funding.&lt;br/&gt;&lt;br/&gt;So the YC partners have to guess both whether you've discovered a
real need, and whether you'll be able to satisfy it. That's what they
are, at least in this part of their job: professional guessers.
They have 1001 heuristics for doing this, and I'm not going to tell
you all of them, but I'm happy to tell you the most important ones,
because these can't be faked; the only way to "hack" them would be
to do what you should be doing anyway as a founder.&lt;br/&gt;&lt;br/&gt;The first thing the partners will try to figure out, usually, is
whether what you're making will ever be something a lot of people
want. It doesn't have to be something a lot of people want now.
The product and the market will both evolve, and will influence
each other's evolution. But in the end there has to be something
with a huge market. That's what the partners will be trying to
figure out: is there a path to a huge market?
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/ace.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Sometimes it's obvious there will be a huge market. If 
&lt;a href="https://boomsupersonic.com/"&gt;&lt;u&gt;Boom&lt;/u&gt;&lt;/a&gt; manages
to ship an airliner at all, international airlines will have to buy
it. But usually it's not obvious. Usually the path to a huge market
is by growing a small market. This idea is important enough that
it's worth coining a phrase for, so let's call one of these small
but growable markets a "larval market."&lt;br/&gt;&lt;br/&gt;The perfect example of a larval market might be Apple's market when
they were founded in 1976. In 1976, not many people wanted their
own computer. But more and more started to want one, till now every
10 year old on the planet wants a computer (but calls it a "phone").&lt;br/&gt;&lt;br/&gt;The ideal combination is the group of founders who are 
&lt;a href="https://paulgraham.com/startupideas.html"&gt;&lt;u&gt;"living in
the future"&lt;/u&gt;&lt;/a&gt; in the sense of being at the leading edge of some kind
of change, and who are building something they themselves want.
Most super-successful startups are of this type. Steve Wozniak
wanted a computer. Mark Zuckerberg wanted to engage online with his
college friends. Larry and Sergey wanted to find things on the web.
All these founders were building things they and their peers wanted,
and the fact that they were at the leading edge of change meant
that more people would want these things in the future.&lt;br/&gt;&lt;br/&gt;But although the ideal larval market is oneself and one's peers,
that's not the only kind. A larval market might also be regional,
for example. You build something to serve one location, and then
expand to others.&lt;br/&gt;&lt;br/&gt;The crucial feature of the initial market is that it exist. That
may seem like an obvious point, but the lack of it is the biggest
flaw in most startup ideas. There have to be some people who want
what you're building right now, and want it so urgently that they're
willing to use it, bugs and all, even though you're a small company
they've never heard of. There don't have to be many, but there have
to be some. As long as you have some users, there are straightforward
ways to get more: build new features they want, seek out more people
like them, get them to refer you to their friends, and so on. But
these techniques all require some initial seed group of users.&lt;br/&gt;&lt;br/&gt;So this is one thing the YC partners will almost certainly dig into
during your interview. Who are your first users going to be, and
how do you know they want this? If I had to decide whether to fund
startups based on a single question, it would be "How do you know
people want this?"&lt;br/&gt;&lt;br/&gt;The most convincing answer is "Because we and our friends want it."
It's even better when this is followed by the news that you've
already built a prototype, and even though it's very crude, your
friends are using it, and it's spreading by word of mouth. If you
can say that and you're not lying, the partners will switch from
default no to default yes. Meaning you're in unless there's some
other disqualifying flaw.&lt;br/&gt;&lt;br/&gt;That is a hard standard to meet, though. Airbnb didn't meet it.
They had the first part. They had made something they themselves
wanted. But it wasn't spreading. So don't feel bad if you don't hit
this gold standard of convincingness. If Airbnb didn't hit it, it
must be too high.&lt;br/&gt;&lt;br/&gt;In practice, the YC partners will be satisfied if they feel that
you have a deep understanding of your users' needs. And the Airbnbs
did have that. They were able to tell us all about what motivated
hosts and guests. They knew from first-hand experience, because
they'd been the first hosts. We couldn't ask them a question they
didn't know the answer to. We ourselves were not very excited about
the idea as users, but we knew this didn't prove anything, because
there were lots of successful startups we hadn't been excited about
as users. We were able to say to ourselves "They seem to know what
they're talking about. Maybe they're onto something. It's not growing
yet, but maybe they can figure out how to make it grow during YC."
Which they did, about three weeks into the batch.&lt;br/&gt;&lt;br/&gt;The best thing you can do in a YC interview is to teach the partners
about your users. So if you want to prepare for your interview, one of the best 
ways to do it is to go talk to your users and find out exactly what
they're thinking. Which is what you should be doing anyway.&lt;br/&gt;&lt;br/&gt;This may sound strangely credulous, but the YC partners want to
rely on the founders to tell them about the market. Think about
how VCs typically judge the potential market for an idea. They're
not ordinarily domain experts themselves, so they forward the idea
to someone who is, and ask for their opinion. YC doesn't have time
to do this, but if the YC partners can convince themselves that the
founders both (a) know what they're talking about and (b) aren't
lying, they don't need outside domain experts. They can use the
founders themselves as domain experts when evaluating their own
idea.&lt;br/&gt;&lt;br/&gt;This is why YC interviews aren't pitches. To give as many founders
as possible a chance to get funded, we made interviews as short as
we could: 10 minutes. That is not enough time for the partners to
figure out, through the indirect evidence in a pitch, whether you
know what you're talking about and aren't lying. They need to dig
in and ask you questions. There's not enough time for sequential
access. They need random access.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/ace.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The worst advice I ever heard about how to succeed in a YC interview
is that you should take control of the interview and make sure to
deliver the message you want to. In other words, turn the interview
into a pitch. ⟨elaborate expletive⟩. It is so annoying when people
try to do that. You ask them a question, and instead of answering
it, they deliver some obviously prefabricated blob of pitch. It
eats up 10 minutes really fast.&lt;br/&gt;&lt;br/&gt;There is no one who can give you accurate advice about what to do
in a YC interview except a current or former YC partner. People
who've merely been interviewed, even successfully, have no idea of
this, but interviews take all sorts of different forms depending
on what the partners want to know about most. Sometimes they're all
about the founders, other times they're all about the idea. Sometimes
some very narrow aspect of the idea. Founders sometimes walk away
from interviews complaining that they didn't get to explain their
idea completely. True, but they explained enough.&lt;br/&gt;&lt;br/&gt;Since a YC interview consists of questions, the way to do it well
is to answer them well. Part of that is answering them candidly.
The partners don't expect you to know everything. But if you don't
know the answer to a question, don't try to bullshit your way out
of it. The partners, like most experienced investors, are professional
bullshit detectors, and you are (hopefully) an amateur bullshitter.
And if you try to bullshit them and fail, they may not even tell
you that you failed. So it's better to be honest than to try to
sell them. If you don't know the answer to a question, say you
don't, and tell them how you'd go about finding it, or tell them
the answer to some related question.&lt;br/&gt;&lt;br/&gt;If you're asked, for example, what could go wrong, the worst possible
answer is "nothing." Instead of convincing them that your idea is
bullet-proof, this will convince them that you're a fool or a liar.
Far better to go into gruesome detail. That's what experts do when
you ask what could go wrong.  The partners know that your idea is
risky. That's what a good bet looks like at this stage: a tiny
probability of a huge outcome.&lt;br/&gt;&lt;br/&gt;Ditto if they ask about competitors. Competitors are rarely what
kills startups. Poor execution does. But you should know who your
competitors are, and tell the YC partners candidly what your relative
strengths and weaknesses are. Because the YC partners know that
competitors don't kill startups, they won't hold competitors against
you too much. They will, however, hold it against you if you seem
either to be unaware of competitors, or to be minimizing the threat
they pose. They may not be sure whether you're clueless or lying,
but they don't need to be.&lt;br/&gt;&lt;br/&gt;The partners don't expect your idea to be perfect. This is seed
investing. At this stage, all they can expect are promising hypotheses.
But they do expect you to be thoughtful and honest. So if trying
to make your idea seem perfect causes you to come off as glib or
clueless, you've sacrificed something you needed for something you
didn't.&lt;br/&gt;&lt;br/&gt;If the partners are sufficiently convinced that there's a path to
a big market, the next question is whether you'll be able to find
it. That in turn depends on three things: the general qualities of
the founders, their specific expertise in this domain, and the
relationship between them. How determined are the founders? Are
they good at building things? Are they resilient enough to keep
going when things go wrong? How strong is their friendship?&lt;br/&gt;&lt;br/&gt;Though the Airbnbs only did ok in the idea department, they did
spectacularly well in this department. The story of how they'd
funded themselves by making Obama- and McCain-themed breakfast
cereal was the single most important factor in our decision to fund
them. They didn't realize it at the time, but what seemed to them
an irrelevant story was in fact fabulously good evidence of their
qualities as founders. It showed they were resourceful and determined,
and could work together.&lt;br/&gt;&lt;br/&gt;It wasn't just the cereal story that showed that, though. The whole
interview showed that they cared. They weren't doing this just for
the money, or because startups were cool. The reason they were
working so hard on this company was because it was their project.
They had discovered an interesting new idea, and they just couldn't
let it go.&lt;br/&gt;&lt;br/&gt;Mundane as it sounds, that's the most powerful motivator of all,
not just in startups, but in most ambitious undertakings: to be
&lt;a href="https://paulgraham.com/genius.html"&gt;&lt;u&gt;genuinely interested&lt;/u&gt;&lt;/a&gt; in what 
you're building. This is what really
drives billionaires, or at least the ones who become billionaires
from starting companies. The company is their project.&lt;br/&gt;&lt;br/&gt;One thing few people realize about billionaires is that all of them
could have stopped sooner. They could have gotten acquired, or found
someone else to run the company. Many founders do. The ones who
become really rich are the ones who keep working. And what makes
them keep working is not just money. What keeps them working is the
same thing that keeps anyone else working when they could stop if
they wanted to: that there's nothing else they'd rather do.&lt;br/&gt;&lt;br/&gt;That, not exploiting people, is the defining quality of people who
become billionaires from starting companies. So that's what YC looks
for in founders: authenticity. People's motives for starting startups
are usually mixed. They're usually doing it from some combination
of the desire to make money, the desire to seem cool, genuine
interest in the problem, and unwillingness to work for someone else.
The last two are more powerful motivators than the first two. It's
ok for founders to want to make money or to seem cool. Most do.
But if the founders seem like they're doing it &lt;i&gt;just&lt;/i&gt; to make money
or &lt;i&gt;just&lt;/i&gt; to seem cool, they're not likely to succeed on a big
scale. The founders who are doing it for the money will take the
first sufficiently large acquisition offer, and the ones who are
doing it to seem cool will rapidly discover that there are much
less painful ways of seeming cool.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/ace.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Y Combinator certainly sees founders whose m.o. is to exploit people.
YC is a magnet for them, because they want the YC brand. But when
the YC partners detect someone like that, they reject them. If bad
people made good founders, the YC partners would face a moral
dilemma. Fortunately they don't, because bad people make bad founders.
This exploitative type of founder is not going to succeed on a large
scale, and in fact probably won't even succeed on a small one,
because they're always going to be taking shortcuts. They see YC
itself as a shortcut.&lt;br/&gt;&lt;br/&gt;Their exploitation usually begins with their own cofounders, which
is disastrous, since the cofounders' relationship is the foundation
of the company. Then it moves on to the users, which is also
disastrous, because the sort of early adopters a successful startup
wants as its initial users are the hardest to fool. The best this
kind of founder can hope for is to keep the edifice of deception
tottering along until some acquirer can be tricked into buying it.
But that kind of acquisition is never very big.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/ace.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;If professional billionaire scouts know that exploiting people is
not the skill to look for, why do some politicians think this is
the defining quality of billionaires?&lt;br/&gt;&lt;br/&gt;I think they start from the feeling that it's wrong that one person
could have so much more money than another. It's understandable
where that feeling comes from. It's in our DNA, and even in the DNA
of other species.&lt;br/&gt;&lt;br/&gt;If they limited themselves to saying that it made them feel bad
when one person had so much more money than other people, who would
disagree? It makes me feel bad too, and I think people who make a
lot of money have a moral obligation to use it for the common good.
The mistake they make is to jump from feeling bad that some people
are much richer than others to the conclusion that there's no
legitimate way to make a very large amount of money. Now we're
getting into statements that are not only falsifiable, but false.&lt;br/&gt;&lt;br/&gt;There are certainly some people who become rich by doing bad things.
But there are also plenty of people who behave badly and don't make
that much from it. There is no correlation  in fact, probably an
inverse correlation  between how badly you behave and how much
money you make.&lt;br/&gt;&lt;br/&gt;The greatest danger of this nonsense may not even be that it sends
policy astray, but that it misleads ambitious people. Can you imagine
a better way to destroy social mobility than by telling poor kids
that the way to get rich is by exploiting people, while the rich
kids know, from having watched the preceding generation do it, how
it's really done?&lt;br/&gt;&lt;br/&gt;I'll tell you how it's really done, so you can at least tell your
own kids the truth. It's all about users. The most reliable way to
become a billionaire is to start a company that 
&lt;a href="https://paulgraham.com/growth.html"&gt;&lt;u&gt;grows fast&lt;/u&gt;&lt;/a&gt;, and the
way to grow fast is to make what users want. Newly started startups
have no choice but to delight users, or they'll never even get
rolling. But this never stops being the lodestar, and bigger companies
take their eye off it at their peril. Stop delighting users, and
eventually someone else will.&lt;br/&gt;&lt;br/&gt;Users are what the partners want to
know about in YC interviews, and what I want to know about when I
talk to founders that we funded ten years ago and who are billionaires
now. What do users want? What new things could you build for them?
Founders who've become billionaires are always eager to talk about
that topic. That's how they became billionaires.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
The YC partners have so much practice doing this that they
sometimes see paths that the founders themselves haven't seen yet.
The partners don't try to seem skeptical, as buyers in transactions
often do to increase their leverage. Although the founders feel
their job is to convince the partners of the potential of their
idea, these roles are not infrequently reversed, and the founders
leave the interview feeling their idea has more potential than they
realized.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
In practice, 7 minutes would be enough. You rarely change your
mind at minute 8. But 10 minutes is socially convenient.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
I myself took the first sufficiently large acquisition offer
in my first startup, so I don't blame founders for doing this.
There's nothing wrong with starting a startup to make money. You
need to make money somehow, and for some people startups are the
most efficient way to do it. I'm just saying that these are not the
startups that get really big.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Not these days, anyway. There were some big ones during the
Internet Bubble, and indeed some big IPOs.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Jessica Livingston, Robert Morris, Geoff Ralston, and
Harj Taggar for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//ace.html</guid>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The Airbnbs</title>
      <link>https://paulgraham.com//airbnbs.html</link>
      <description>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;To celebrate Airbnb's IPO and to help future founders, I thought
it might be useful to explain what was special about Airbnb.&lt;br/&gt;&lt;br/&gt;What was special about the Airbnbs was how earnest they were. They
did nothing half-way, and we could sense this even in the interview.
Sometimes after we interviewed a startup we'd be uncertain what to
do, and have to talk it over. Other times we'd just look at one
another and smile. The Airbnbs' interview was&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;December 2020&lt;br/&gt;&lt;br/&gt;To celebrate Airbnb's IPO and to help future founders, I thought
it might be useful to explain what was special about Airbnb.&lt;br/&gt;&lt;br/&gt;What was special about the Airbnbs was how earnest they were. They
did nothing half-way, and we could sense this even in the interview.
Sometimes after we interviewed a startup we'd be uncertain what to
do, and have to talk it over. Other times we'd just look at one
another and smile. The Airbnbs' interview was that kind. We didn't
even like the idea that much. Nor did users, at that stage; they
had no growth. But the founders seemed so full of energy that it
was impossible not to like them.&lt;br/&gt;&lt;br/&gt;That first impression was not misleading. During the batch our
nickname for Brian Chesky was The Tasmanian Devil, because like the
&lt;a href="http://www.youtube.com/watch?v=StG2u5qfFRg&amp;amp;t=2m27s"&gt;cartoon
character&lt;/a&gt; he seemed a tornado of energy. All three of them were
like that. No one ever worked harder during YC than the Airbnbs
did. When you talked to the Airbnbs, they took notes. If you suggested
an idea to them in office hours, the next time you talked to them
they'd not only have implemented it, but also implemented two new
ideas they had in the process. "They probably have the best attitude
of any startup we've funded" I wrote to Mike Arrington during the
batch.&lt;br/&gt;&lt;br/&gt;They're still like that. Jessica and I had dinner with Brian in the
summer of 2018, just the three of us. By this point the company is
ten years old. He took a page of notes about ideas for new things
Airbnb could do.&lt;br/&gt;&lt;br/&gt;What we didn't realize when we first met Brian and Joe and Nate was
that Airbnb was on its last legs. After working on the company for
a year and getting no growth, they'd agreed to give it one last
shot. They'd try this Y Combinator thing, and if the company still
didn't take off, they'd give up.&lt;br/&gt;&lt;br/&gt;Any normal person would have given up already. They'd been funding
the company with credit cards. They had a &lt;i&gt;binder&lt;/i&gt; full of
credit cards they'd maxed out. Investors didn't think much of the
idea. One investor they met in a cafe walked out in the middle of
meeting with them. They thought he was going to the bathroom, but
he never came back. "He didn't even finish his smoothie," Brian
said. And now, in late 2008, it was the worst recession in decades.
The stock market was in free fall and wouldn't hit bottom for another
four months.&lt;br/&gt;&lt;br/&gt;Why hadn't they given up? This is a useful question to ask. People,
like matter, reveal their nature under extreme conditions. One thing
that's clear is that they weren't doing this just for the money.
As a money-making scheme, this was pretty lousy: a year's work and
all they had to show for it was a binder full of maxed-out credit
cards. So why were they still working on this startup? Because of
the experience they'd had as the first hosts.&lt;br/&gt;&lt;br/&gt;When they first tried renting out airbeds on their floor during a
design convention, all they were hoping for was to make enough money
to pay their rent that month. But something surprising happened:
they enjoyed having those first three guests staying with them. And
the guests enjoyed it too. Both they and the guests had done it
because they were in a sense forced to, and yet they'd all had a
great experience. Clearly there was something new here: for hosts,
a new way to make money that had literally been right under their
noses, and for guests, a new way to travel that was in many ways
better than hotels.&lt;br/&gt;&lt;br/&gt;That experience was why the Airbnbs didn't give up. They knew they'd
discovered something. They'd seen a glimpse of the future, and they
couldn't let it go.&lt;br/&gt;&lt;br/&gt;They knew that once people tried staying in what is now called "an
airbnb," they would also realize that this was the future. But only
if they tried it, and they weren't. That was the problem during Y
Combinator: to get growth started.&lt;br/&gt;&lt;br/&gt;Airbnb's goal during YC was to reach what we call &lt;a href="http://paulgraham.com/ramenprofitable.html"&gt;ramen profitability&lt;/a&gt;,
which means making enough money that the company can pay the founders'
living expenses, if they live on ramen noodles. Ramen profitability
is not, obviously, the end goal of any startup, but it's the most
important threshold on the way, because this is the point where
you're airborne. This is the point where you no longer need investors'
permission to continue existing. For the Airbnbs, ramen profitability
was $4000 a month: $3500 for rent, and $500 for food. They taped
this goal to the mirror in the bathroom of their apartment.&lt;br/&gt;&lt;br/&gt;The way to get growth started in something like Airbnb is to focus
on the hottest subset of the market. If you can get growth started
there, it will spread to the rest. When I asked the Airbnbs where
there was most demand, they knew from searches: New York City. So
they focused on New York. They went there &lt;a href="http://paulgraham.com/ds.html"&gt;in person&lt;/a&gt; to visit their
hosts and help them make their listings more attractive. A big part
of that was better pictures. So Joe and Brian rented a professional
camera and took pictures of the hosts' places themselves.&lt;br/&gt;&lt;br/&gt;This didn't just make the listings better. It also taught them about
their hosts. When they came back from their first trip to New York,
I asked what they'd noticed about hosts that surprised them, and
they said the biggest surprise was how many of the hosts were in
the same position they'd been in: they needed this money to pay
their rent. This was, remember, the worst recession in decades, and
it had hit New York first. It definitely added to the Airbnbs' sense
of mission to feel that people needed them.&lt;br/&gt;&lt;br/&gt;In late January 2009, about three weeks into Y Combinator, their
efforts started to show results, and their numbers crept upward.
But it was hard to say for sure whether it was growth or just random
fluctuation. By February it was clear that it was real growth. They
made $460 in fees in the first week of February, $897 in the second,
and $1428 in the third. That was it: they were airborne. Brian sent
me an email on February 22 announcing that they were ramen profitable
and giving the last three weeks' numbers.&lt;br/&gt;&lt;br/&gt;"I assume you know what you've now set yourself up for next week,"
I responded.&lt;br/&gt;&lt;br/&gt;Brian's reply was seven words: "We are not going to slow down."&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//airbnbs.html</guid>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to Think for Yourself</title>
      <link>https://paulgraham.com//think.html</link>
      <description>&lt;font face="verdana" size="2"&gt;November 2020&lt;br/&gt;&lt;br/&gt;There are some kinds of work that you can't do well without thinking
differently from your peers. To be a successful scientist, for
example, it's not enough just to be correct. Your ideas have to be
both correct and novel. You can't publish papers saying things other
people already know. You need to say things no one else has realized
yet.&lt;br/&gt;&lt;br/&gt;The same is true for investors. It's not enough for a public market
investor to predict correctl&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;November 2020&lt;br/&gt;&lt;br/&gt;There are some kinds of work that you can't do well without thinking
differently from your peers. To be a successful scientist, for
example, it's not enough just to be correct. Your ideas have to be
both correct and novel. You can't publish papers saying things other
people already know. You need to say things no one else has realized
yet.&lt;br/&gt;&lt;br/&gt;The same is true for investors. It's not enough for a public market
investor to predict correctly how a company will do. If a lot of
other people make the same prediction, the stock price will already
reflect it, and there's no room to make money. The only valuable
insights are the ones most other investors don't share.&lt;br/&gt;&lt;br/&gt;You see this pattern with startup founders too. You don't want to
start a startup to do something that everyone agrees is a good idea,
or there will already be other companies doing it. You have to do
something that sounds to most other people like a bad idea, but
that you know isn't  like writing software for a tiny computer
used by a few thousand hobbyists, or starting a site to let people
rent airbeds on strangers' floors.&lt;br/&gt;&lt;br/&gt;Ditto for essayists. An essay that told people things they already
knew would be boring. You have to tell them something &lt;a href="https://paulgraham.com/useful.html"&gt;&lt;u&gt;new&lt;/u&gt;&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;But this pattern isn't universal. In fact, it doesn't hold for most
kinds of work. In most kinds of work  to be an administrator, for
example  all you need is the first half. All you need is to be
right. It's not essential that everyone else be wrong.&lt;br/&gt;&lt;br/&gt;There's room for a little novelty in most kinds of work, but in
practice there's a fairly sharp distinction between the kinds of
work where it's essential to be independent-minded, and the kinds
where it's not.&lt;br/&gt;&lt;br/&gt;I wish someone had told me about this distinction when I was a kid,
because it's one of the most important things to think about when
you're deciding what kind of work you want to do. Do you want to
do the kind of work where you can only win by thinking differently
from everyone else? I suspect most people's unconscious mind will
answer that question before their conscious mind has a chance to.
I know mine does.&lt;br/&gt;&lt;br/&gt;Independent-mindedness seems to be more a matter of nature than
nurture. Which means if you pick the wrong type of work, you're
going to be unhappy. If you're naturally independent-minded, you're
going to find it frustrating to be a middle manager. And if you're
naturally conventional-minded, you're going to be sailing into a
headwind if you try to do original research.&lt;br/&gt;&lt;br/&gt;One difficulty here, though, is that people are often mistaken about
where they fall on the spectrum from conventional- to independent-minded.
Conventional-minded people don't like to think of themselves as
conventional-minded. And in any case, it genuinely feels to them
as if they make up their own minds about everything. It's just a
coincidence that their beliefs are identical to their peers'. And
the independent-minded, meanwhile, are often unaware how different
their ideas are from conventional ones, at least till they state
them publicly.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;By the time they reach adulthood, most people know roughly how smart
they are (in the narrow sense of ability to solve pre-set problems),
because they're constantly being tested and ranked according to it.
But schools generally ignore independent-mindedness, except to the
extent they try to suppress it. So we don't get anything like the
same kind of feedback about how independent-minded we are.&lt;br/&gt;&lt;br/&gt;There may even be a phenomenon like Dunning-Kruger at work, where
the most conventional-minded people are confident that they're
independent-minded, while the genuinely independent-minded worry
they might not be independent-minded enough.&lt;br/&gt;&lt;br/&gt;
&lt;center&gt;___________&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
Can you make yourself more independent-minded? I think so. This
quality may be largely inborn, but there seem to be ways to magnify
it, or at least not to suppress it.&lt;br/&gt;&lt;br/&gt;One of the most effective techniques is one practiced unintentionally
by most nerds: simply to be less aware what conventional beliefs
are. It's hard to be a conformist if you don't know what you're
supposed to conform to. Though again, it may be that such people
already are independent-minded. A conventional-minded person would
probably feel anxious not knowing what other people thought, and
make more effort to find out.&lt;br/&gt;&lt;br/&gt;It matters a lot who you surround yourself with. If you're surrounded
by conventional-minded people, it will constrain which ideas you
can express, and that in turn will constrain which ideas you have.
But if you surround yourself with independent-minded people, you'll
have the opposite experience: hearing other people say surprising
things will encourage you to, and to think of more.&lt;br/&gt;&lt;br/&gt;Because the independent-minded find it uncomfortable to be surrounded
by conventional-minded people, they tend to self-segregate once
they have a chance to. The problem with high school is that they
haven't yet had a chance to. Plus high school tends to be an
inward-looking little world whose inhabitants lack confidence, both
of which magnify the forces of conformism.  So high school is
often a &lt;a href="https://paulgraham.com/nerds.html"&gt;&lt;u&gt;bad time&lt;/u&gt;&lt;/a&gt; for the
independent-minded. But there is some advantage even here: it
teaches you what to avoid. If you later find yourself in a situation
that makes you think "this is like high school," you know you should
get out.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Another place where the independent- and conventional-minded are
thrown together is in successful startups. The founders and early
employees are almost always independent-minded; otherwise the startup
wouldn't be successful. But conventional-minded people greatly
outnumber independent-minded ones, so as the company grows, the
original spirit of independent-mindedness is inevitably diluted.
This causes all kinds of problems besides the obvious one that the
company starts to suck. One of the strangest is that the founders
find themselves able to speak more freely with founders of other
companies than with their own employees.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Fortunately you don't have to spend all your time with independent-minded
people. It's enough to have one or two you can talk to regularly.
And once you find them, they're usually as eager to talk as you
are; they need you too. Although universities no longer have the
kind of monopoly they used to have on education, good universities
are still an excellent way to meet independent-minded people. Most
students will still be conventional-minded, but you'll at least
find clumps of independent-minded ones, rather than the near zero
you may have found in high school.&lt;br/&gt;&lt;br/&gt;It also works to go in the other direction: as well as cultivating
a small collection of independent-minded friends, to try to meet
as many different types of people as you can. It will decrease the
influence of your immediate peers if you have several other groups
of peers. Plus if you're part of several different worlds, you can
often import ideas from one to another.&lt;br/&gt;&lt;br/&gt;But by different types of people, I don't mean demographically
different. For this technique to work, they have to think differently.
So while it's an excellent idea to go and visit other countries,
you can probably find people who think differently right around the
corner. When I meet someone who knows a lot about something unusual
(which includes practically everyone, if you dig deep enough), I
try to learn what they know that other people don't. There are
almost always surprises here. It's a good way to make conversation
when you meet strangers, but I don't do it to make conversation.
I really want to know.&lt;br/&gt;&lt;br/&gt;You can expand the source of influences in time as well as space,
by reading history. When I read history I do it not just to learn
what happened, but to try to get inside the heads of people who
lived in the past. How did things look to them? This is hard to do,
but worth the effort for the same reason it's worth travelling far
to triangulate a point.&lt;br/&gt;&lt;br/&gt;You can also take more explicit measures to prevent yourself from
automatically adopting conventional opinions. The most general is
to cultivate an attitude of skepticism. When you hear someone say
something, stop and ask yourself "Is that true?" Don't say it out
loud. I'm not suggesting that you impose on everyone who talks to
you the burden of proving what they say, but rather that you take
upon yourself the burden of evaluating what they say.&lt;br/&gt;&lt;br/&gt;Treat it as a puzzle. You know that some accepted ideas will later
turn out to be wrong. See if you can guess which. The end goal is
not to find flaws in the things you're told, but to find the new
ideas that had been concealed by the broken ones. So this game
should be an exciting quest for novelty, not a boring protocol for
intellectual hygiene. And you'll be surprised, when you start asking
"Is this true?", how often the answer is not an immediate yes. If
you have any imagination, you're more likely to have too many leads
to follow than too few.&lt;br/&gt;&lt;br/&gt;More generally your goal should be not to let anything into your
head unexamined, and things don't always enter your head in the
form of statements. Some of the most powerful influences are implicit.
How do you even notice these? By standing back and watching how
other people get their ideas.&lt;br/&gt;&lt;br/&gt;When you stand back at a sufficient distance, you can see ideas
spreading through groups of people like waves. The most obvious are
in fashion: you notice a few people wearing a certain kind of shirt,
and then more and more, until half the people around you are wearing
the same shirt. You may not care much what you wear, but there are
intellectual fashions too, and you definitely don't want to participate
in those. Not just because you want sovereignty over your own
thoughts, but because &lt;a href="https://paulgraham.com/nov.html"&gt;&lt;u&gt;unfashionable&lt;/u&gt;&lt;/a&gt;
ideas are disproportionately likely to lead somewhere interesting.
The best place to find undiscovered ideas is where no one else is
looking.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;
&lt;center&gt;___________&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
To go beyond this general advice, we need to look at the internal
structure of independent-mindedness  at the individual muscles
we need to exercise, as it were. It seems to me that it has three
components: fastidiousness about truth, resistance to being told
what to think, and curiosity.&lt;br/&gt;&lt;br/&gt;Fastidiousness about truth means more than just not believing things
that are false. It means being careful about degree of belief. For
most people, degree of belief rushes unexamined toward the extremes:
the unlikely becomes impossible, and the probable becomes certain.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
To the independent-minded, this seems unpardonably sloppy.
They're willing to have anything in their heads, from highly
speculative hypotheses to (apparent) tautologies, but on subjects
they care about, everything has to be labelled with a carefully
considered degree of belief.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The independent-minded thus have a horror of ideologies, which
require one to accept a whole collection of beliefs at once, and
to treat them as articles of faith. To an independent-minded person
that would seem revolting, just as it would seem to someone fastidious
about food to take a bite of a submarine sandwich filled with a
large variety of ingredients of indeterminate age and provenance.&lt;br/&gt;&lt;br/&gt;Without this fastidiousness about truth, you can't be truly
independent-minded. It's not enough just to have resistance to being
told what to think. Those kind of people reject conventional ideas
only to replace them with the most random conspiracy theories. And
since these conspiracy theories have often been manufactured to
capture them, they end up being less independent-minded than ordinary
people, because they're subject to a much more exacting master than
mere convention.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f7n"&gt;&lt;font color="#dddddd"&gt;7&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Can you increase your fastidiousness about truth? I would think so.
In my experience, merely thinking about something you're fastidious
about causes that fastidiousness to grow. If so, this is one of
those rare virtues we can have more of merely by wanting it. And
if it's like other forms of fastidiousness, it should also be
possible to encourage in children. I certainly got a strong dose
of it from my father.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f8n"&gt;&lt;font color="#dddddd"&gt;8&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The second component of independent-mindedness, resistance to being
told what to think, is the most visible of the three. But even this
is often misunderstood. The big mistake people make about it is to
think of it as a merely negative quality. The language we use
reinforces that idea. You're &lt;i&gt;un&lt;/i&gt;conventional. You &lt;i&gt;don't&lt;/i&gt; care
what other people think. But it's not just a kind of immunity. In
the most independent-minded people, the desire not to be told what
to think is a positive force. It's not mere skepticism, but an
active &lt;a href="https://paulgraham.com/gba.html"&gt;&lt;u&gt;delight&lt;/u&gt;&lt;/a&gt; in ideas that subvert
the conventional wisdom, the more counterintuitive the better.&lt;br/&gt;&lt;br/&gt;Some of the most novel ideas seemed at the time almost like practical
jokes. Think how often your reaction to a novel idea is to laugh.
I don't think it's because novel ideas are funny per se, but because
novelty and humor share a certain kind of surprisingness. But while
not identical, the two are close enough that there is a definite
correlation between having a sense of humor and being independent-minded
 just as there is between being humorless and being conventional-minded.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f9n"&gt;&lt;font color="#dddddd"&gt;9&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;I don't think we can significantly increase our resistance to being
told what to think. It seems the most innate of the three components
of independent-mindedness; people who have this quality as adults
usually showed all too visible signs of it as children. But if we
can't increase our resistance to being told what to think, we can
at least shore it up, by surrounding ourselves with other
independent-minded people.&lt;br/&gt;&lt;br/&gt;The third component of independent-mindedness, curiosity, may be
the most interesting. To the extent that we can give a brief answer
to the question of where novel ideas come from, it's curiosity. That's
what people are usually feeling before having them.&lt;br/&gt;&lt;br/&gt;In my experience, independent-mindedness and curiosity predict one
another perfectly. Everyone I know who's independent-minded is
deeply curious, and everyone I know who's conventional-minded isn't.
Except, curiously, children. All small children are curious. Perhaps
the reason is that even the conventional-minded have to be curious
in the beginning, in order to learn what the conventions are. Whereas
the independent-minded are the gluttons of curiosity, who keep
eating even after they're full.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/think.html#f10n"&gt;&lt;font color="#dddddd"&gt;10&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The three components of independent-mindedness work in concert:
fastidiousness about truth and resistance to being told what to
think leave space in your brain, and curiosity finds new ideas to
fill it.&lt;br/&gt;&lt;br/&gt;Interestingly, the three components can substitute for one another
in much the same way muscles can. If you're sufficiently fastidious
about truth, you don't need to be as resistant to being told what
to think, because fastidiousness alone will create sufficient gaps
in your knowledge. And either one can compensate for curiosity,
because if you create enough space in your brain, your discomfort
at the resulting vacuum will add force to your curiosity. Or curiosity
can compensate for them: if you're sufficiently curious, you don't
need to clear space in your brain, because the new ideas you discover
will push out the conventional ones you acquired by default.&lt;br/&gt;&lt;br/&gt;Because the components of independent-mindedness are so interchangeable,
you can have them to varying degrees and still get the same result.
So there is not just a single model of independent-mindedness. Some
independent-minded people are openly subversive, and others are
quietly curious. They all know the secret handshake though.&lt;br/&gt;&lt;br/&gt;Is there a way to cultivate curiosity? To start with, you want to
avoid situations that suppress it. How much does the work you're
currently doing engage your curiosity? If the answer is "not much,"
maybe you should change something.&lt;br/&gt;&lt;br/&gt;The most important active step you can take to cultivate your
curiosity is probably to seek out the topics that engage it. Few
adults are equally curious about everything, and it doesn't seem
as if you can choose which topics interest you. So it's up to you
to &lt;a href="https://paulgraham.com/genius.html"&gt;&lt;u&gt;find&lt;/u&gt;&lt;/a&gt; them. Or invent them, if
necessary.&lt;br/&gt;&lt;br/&gt;Another way to increase your curiosity is to indulge it, by
investigating things you're interested in. Curiosity is unlike
most other appetites in this respect: indulging it tends to increase
rather than to sate it. Questions lead to more questions.&lt;br/&gt;&lt;br/&gt;Curiosity seems to be more individual than fastidiousness about
truth or resistance to being told what to think. To the degree
people have the latter two, they're usually pretty general, whereas
different people can be curious about very different things. So
perhaps curiosity is the compass here. Perhaps, if your goal is to
discover novel ideas, your motto should not be "do what you love"
so much as "do what you're curious about."&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
One convenient consequence of the fact that no one identifies
as conventional-minded is that you can say what you like about
conventional-minded people without getting in too much trouble.
When I wrote &lt;a href="https://paulgraham.com/conformism.html"&gt;&lt;u&gt;"The Four Quadrants of
Conformism"&lt;/u&gt;&lt;/a&gt; I expected a firestorm of rage from the
aggressively conventional-minded, but in fact it was quite muted.
They sensed that there was something about the essay that they
disliked intensely, but they had a hard time finding a specific
passage to pin it on.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
When I ask myself what in my life is like high school, the
answer is Twitter. It's not just full of conventional-minded people,
as anything its size will inevitably be, but subject to violent
storms of conventional-mindedness that remind me of descriptions
of Jupiter. But while it probably is a net loss to spend time there,
it has at least made me think more about the distinction between
independent- and conventional-mindedness, which I probably wouldn't
have done otherwise.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
The decrease in independent-mindedness in growing startups is
still an open problem, but there may be solutions.&lt;br/&gt;&lt;br/&gt;Founders can delay the problem by making a conscious effort only
to hire independent-minded people. Which of course also has the
ancillary benefit that they have better ideas.&lt;br/&gt;&lt;br/&gt;Another possible solution is to create policies that somehow disrupt
the force of conformism, much as control rods slow chain reactions,
so that the conventional-minded aren't as dangerous. The physical
separation of Lockheed's Skunk Works may have had this as a side
benefit. Recent examples suggest employee forums like Slack may not
be an unmitigated good.&lt;br/&gt;&lt;br/&gt;The most radical solution would be to grow revenues without growing
the company. You think hiring that junior PR person will be cheap,
compared to a programmer, but what will be the effect on the average
level of independent-mindedness in your company? (The growth in
staff relative to faculty seems to have had a similar effect on
universities.) Perhaps the rule about outsourcing work that's not
your "core competency" should be augmented by one about outsourcing
work done by people who'd ruin your culture as employees.&lt;br/&gt;&lt;br/&gt;Some investment firms already seem to be able to grow revenues
without growing the number of employees. Automation plus the ever
increasing articulation of the "tech stack" suggest this may one
day be possible for product companies.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
There are intellectual fashions in every field, but their
influence varies. One of the reasons politics, for example, tends
to be boring is that it's so extremely subject to them. The threshold
for having opinions about politics is much &lt;a href="https://paulgraham.com/identity.html"&gt;&lt;u&gt;lower&lt;/u&gt;&lt;/a&gt; than the one for having
opinions about set theory. So while there are some ideas in politics,
in practice they tend to be swamped by waves of intellectual fashion.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
The conventional-minded are often fooled by the strength of
their opinions into believing that they're independent-minded. But
strong convictions are not a sign of independent-mindedness. Rather
the opposite.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]
Fastidiousness about truth doesn't imply that an independent-minded
person won't be dishonest, but that he won't be deluded. It's sort
of like the definition of a gentleman as someone who is never
unintentionally rude.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f7n"&gt;&lt;font color="#000000"&gt;7&lt;/font&gt;&lt;/a&gt;]
You see this especially among political extremists. They think
themselves nonconformists, but actually they're niche conformists.
Their opinions may be different from the average person's, but they
are often more influenced by their peers' opinions than the average
person's are.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f8n"&gt;&lt;font color="#000000"&gt;8&lt;/font&gt;&lt;/a&gt;]
If we broaden the concept of fastidiousness about truth so that
it excludes pandering, bogusness, and pomposity as well as falsehood
in the strict sense, our model of independent-mindedness can expand
further into the arts.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f9n"&gt;&lt;font color="#000000"&gt;9&lt;/font&gt;&lt;/a&gt;]
This correlation is far from perfect, though. Gödel and Dirac
don't seem to have been very strong in the humor department. But
someone who is both "neurotypical" and humorless is very likely to
be conventional-minded.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f10n"&gt;&lt;font color="#000000"&gt;10&lt;/font&gt;&lt;/a&gt;]
Exception: gossip. Almost everyone is curious about gossip.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Paul Buchheit, Patrick Collison, Jessica
Livingston, Robert Morris, Harj Taggar, and Peter Thiel for reading
drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//think.html</guid>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Early Work</title>
      <link>https://paulgraham.com//early.html</link>
      <description>&lt;font face="verdana" size="2"&gt;October 2020&lt;br/&gt;&lt;br/&gt;One of the biggest things holding people back from doing great work
is the fear of making something lame. And this fear is not an
irrational one. Many great projects go through a stage early on
where they don't seem very impressive, even to their creators. You
have to push through this stage to reach the great work that lies
beyond. But many people don't. Most people don't even reach the
stage of making something they're embarrassed by, let alo&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;October 2020&lt;br/&gt;&lt;br/&gt;One of the biggest things holding people back from doing great work
is the fear of making something lame. And this fear is not an
irrational one. Many great projects go through a stage early on
where they don't seem very impressive, even to their creators. You
have to push through this stage to reach the great work that lies
beyond. But many people don't. Most people don't even reach the
stage of making something they're embarrassed by, let alone continue
past it. They're too frightened even to start.&lt;br/&gt;&lt;br/&gt;Imagine if we could turn off the fear of making something lame.
Imagine how much more we'd do.&lt;br/&gt;&lt;br/&gt;Is there any hope of turning it off? I think so. I think the habits
at work here are not very deeply rooted.&lt;br/&gt;&lt;br/&gt;Making new things is itself a new thing for us as a species. It has
always happened, but till the last few centuries it happened so
slowly as to be invisible to individual humans. And since we didn't
need customs for dealing with new ideas, we didn't develop any.&lt;br/&gt;&lt;br/&gt;We just don't have enough experience with early versions of ambitious
projects to know how to respond to them. We judge them as we would
judge more finished work, or less ambitious projects. We don't
realize they're a special case.&lt;br/&gt;&lt;br/&gt;Or at least, most of us don't. One reason I'm confident we can do
better is that it's already starting to happen. There are already
a few places that are living in the future in this respect. Silicon
Valley is one of them: an unknown person working on a strange-sounding
idea won't automatically be dismissed the way they would back home.
In Silicon Valley, people have learned how dangerous that is.&lt;br/&gt;&lt;br/&gt;The right way to deal with new ideas is to treat them as a challenge
to your imagination  not just to have lower standards, but to
&lt;a href="https://paulgraham.com/altair.html"&gt;&lt;u&gt;switch polarity&lt;/u&gt;&lt;/a&gt; entirely, from listing 
the reasons an idea won't
work to trying to think of ways it could. That's what I do when I
meet people with new ideas. I've become quite good at it, but I've
had a lot of practice. Being a partner at Y Combinator means being
practically immersed in strange-sounding ideas proposed by unknown
people. Every six months you get thousands of new ones thrown at
you and have to sort through them, knowing that in a world with a
power-law distribution of outcomes, it will be painfully obvious
if you miss the needle in this haystack. Optimism becomes
urgent.&lt;br/&gt;&lt;br/&gt;But I'm hopeful that, with time, this kind of optimism can become
widespread enough that it becomes a social custom, not just a trick
used by a few specialists. It is after all an extremely lucrative
trick, and those tend to spread quickly.&lt;br/&gt;&lt;br/&gt;Of course, inexperience is not the only reason people are too harsh
on early versions of ambitious projects. They also do it to seem
clever. And in a field where the new ideas are risky, like startups,
those who dismiss them are in fact more likely to be right. Just
not when their predictions are 
&lt;a href="https://paulgraham.com/swan.html"&gt;&lt;u&gt;weighted by outcome&lt;/u&gt;&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;But there is another more sinister reason people dismiss new ideas.
If you try something ambitious, many of those around you will hope,
consciously or unconsciously, that you'll fail. They worry that if
you try something ambitious and succeed, it will put you above them.
In some countries this is not just an individual failing but part
of the national culture.&lt;br/&gt;&lt;br/&gt;I wouldn't claim that people in Silicon Valley overcome these
impulses because they're morally better. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
The reason many hope
you'll succeed is that they hope to rise with you. For investors
this incentive is particularly explicit. They want you to succeed
because they hope you'll make them rich in the process. But many
other people you meet can hope to benefit in some way from your
success. At the very least they'll be able to say, when you're
famous, that they've known you since way back.&lt;br/&gt;&lt;br/&gt;But even if Silicon Valley's encouraging attitude
is rooted in self-interest, it has over time actually grown into a
sort of benevolence. Encouraging startups has been practiced for
so long that it has become a custom. Now it just seems that that's
what one does with startups.&lt;br/&gt;&lt;br/&gt;Maybe Silicon Valley is too optimistic. Maybe it's too easily fooled
by impostors. Many less optimistic journalists want to believe that.
But the lists of impostors they cite are suspiciously short, and
plagued with asterisks. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt; If you use revenue as the test, Silicon
Valley's optimism seems better tuned than the rest of the world's.
And because it works, it will spread.&lt;br/&gt;&lt;br/&gt;There's a lot more to new ideas than new startup ideas, of course.
The fear of making something lame holds people back in every field.
But Silicon Valley shows how quickly customs can evolve to support
new ideas. And that in turn proves that dismissing new ideas is not
so deeply rooted in human nature that it can't be unlearnt.&lt;br/&gt;&lt;br/&gt;
&lt;center&gt;___________&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
Unfortunately, if you want to do new things, you'll face a force
more powerful than other people's skepticism: your own skepticism.
You too will judge your early work too harshly. How do you avoid
that?&lt;br/&gt;&lt;br/&gt;This is a difficult problem, because you don't want to completely
eliminate your horror of making something lame. That's what steers
you toward doing good work. You just want to turn it off temporarily,
the way a painkiller temporarily turns off pain.&lt;br/&gt;&lt;br/&gt;People have already discovered several techniques that work. Hardy
mentions two in &lt;i&gt;A Mathematician's Apology&lt;/i&gt;:
&lt;blockquote&gt;
  Good work is not done by "humble" men. It is one of the first
  duties of a professor, for example, in any subject, to exaggerate
  a little both the importance of his subject and his importance
  in it.
&lt;/blockquote&gt;
If you overestimate the importance of what you're working on, that
will compensate for your mistakenly harsh judgment of your initial
results. If you look at something that's 20% of the way to a goal
worth 100 and conclude that it's 10% of the way to a goal worth
200, your estimate of its expected value is correct even though
both components are wrong.&lt;br/&gt;&lt;br/&gt;It also helps, as Hardy suggests, to be slightly overconfident.
I've noticed in many fields that the most successful people are
slightly overconfident. On the face of it this seems implausible.
Surely it would be optimal to have exactly the right estimate of
one's abilities. How could it be an advantage to be mistaken?
Because this error compensates for other sources of error in the
opposite direction: being slightly overconfident armors you against
both other people's skepticism and your own.&lt;br/&gt;&lt;br/&gt;Ignorance has a similar effect. It's safe to make the mistake of
judging early work as finished work if you're a sufficiently lax
judge of finished work. I doubt it's possible to cultivate this
kind of ignorance, but empirically it's a real advantage, especially
for the young.&lt;br/&gt;&lt;br/&gt;Another way to get through the lame phase of ambitious projects is
to surround yourself with the right people  to create an eddy in
the social headwind. But it's not enough to collect people who are
always encouraging. You'd learn to discount that. You need colleagues
who can actually tell an ugly duckling from a baby swan. The people
best able to do this are those working on similar projects of their
own, which is why university departments and research labs work so
well. You don't need institutions to collect colleagues. They
naturally coalesce, given the chance. But it's very much worth
accelerating this process by seeking out other people trying to do
new things.&lt;br/&gt;&lt;br/&gt;Teachers are in effect a special case of colleagues. It's a teacher's
job both to see the promise of early work and to encourage you to
continue. But teachers who are good at this are unfortunately quite
rare, so if you have the opportunity to learn from one, take it.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;For some it might work to rely on sheer discipline: to tell yourself
that you just have to press on through the initial crap phase and
not get discouraged. But like a lot of "just tell yourself" advice,
this is harder than it sounds. And it gets still harder as you get
older, because your standards rise. The old do have one compensating
advantage though: they've been through this before.&lt;br/&gt;&lt;br/&gt;It can help if you focus less on where you are and more on the rate
of change. You won't worry so much about doing bad work if you can
see it improving. Obviously the faster it improves, the easier this
is. So when you start something new, it's good if you can spend a
lot of time on it. That's another advantage of being young: you
tend to have bigger blocks of time.&lt;br/&gt;&lt;br/&gt;Another common trick is to start by considering new work to be of
a different, less exacting type. To start a painting saying that
it's just a sketch, or a new piece of software saying that it's
just a quick hack. Then you judge your initial results by a lower
standard. Once the project is rolling you can sneakily convert it
to something more.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;This will be easier if you use a medium that lets you work fast and
doesn't require too much commitment up front. It's easier to convince
yourself that something is just a sketch when you're drawing in a
notebook than when you're carving stone. Plus you get initial results
faster. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f6n"&gt;&lt;font color="#dddddd"&gt;6&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;It will be easier to try out a risky project if you think of it as
a way to learn and not just as a way to make something. Then even
if the project truly is a failure, you'll still have gained by it.
If the problem is sharply enough defined, failure itself is
knowledge: if the theorem you're trying to prove turns out to
be false, or you use a structural member of a certain size and
it fails under stress, you've learned something, even if it
isn't what you wanted to learn.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f7n"&gt;&lt;font color="#dddddd"&gt;7&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;One motivation that works particularly well for me is curiosity.
I like to try new things just to see how they'll turn out. We started
Y Combinator in this spirit, and it was one of main things that
kept me going while I was working on 
&lt;a href="https://paulgraham.com/bel.html"&gt;&lt;u&gt;Bel&lt;/u&gt;&lt;/a&gt;. Having worked for so long
with various dialects of Lisp, I was very curious to see what its
inherent shape was: what you'd end up with if you followed the
axiomatic approach all the way.&lt;br/&gt;&lt;br/&gt;But it's a bit strange that you have to play mind games with yourself
to avoid being discouraged by lame-looking early efforts. The thing
you're trying to trick yourself into believing is in fact the truth.
A lame-looking early version of an ambitious project truly is more
valuable than it seems. So the ultimate solution may be to teach
yourself that.&lt;br/&gt;&lt;br/&gt;One way to do it is to study the histories of people who've
done great work. What were they thinking early on? What was the
very first thing they did? It can sometimes be hard to get an
accurate answer to this question, because people are often embarrassed
by their earliest work and make little effort to publish it. (They
too misjudge it.) But when you can get an accurate picture of the
first steps someone made on the path to some great work, they're
often pretty feeble.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/early.html#f8n"&gt;&lt;font color="#dddddd"&gt;8&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Perhaps if you study enough such cases, you can teach yourself to
be a better judge of early work. Then you'll be immune both to other
people's skepticism and your own fear of making something lame.
You'll see early work for what it is.&lt;br/&gt;&lt;br/&gt;Curiously enough, the solution to the problem of judging early work
too harshly is to realize that our attitudes toward it are themselves
early work. Holding everything to the same standard is a crude
version 1. We're already evolving better customs, and we can already
see signs of how big the payoff will be.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
This assumption may be too conservative. There is some evidence
that historically the Bay Area has attracted a 
&lt;a href="https://paulgraham.com/cities.html"&gt;&lt;u&gt;different sort of person&lt;/u&gt;&lt;/a&gt; than, 
say, New York City.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
One of their great favorites is Theranos. But the most conspicuous
feature of Theranos's cap table is the absence of Silicon Valley
firms. Journalists were fooled by Theranos, but Silicon Valley
investors weren't.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
I made two mistakes about teachers when I was younger.  I
cared more about professors' research than their reputations as
teachers, and I was also wrong about what it meant to be a good
teacher. I thought it simply meant to be good at explaining things.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
Patrick Collison points out that you can go past treating
something as a hack in the sense of a prototype and onward to the
sense of the word that means something closer to a practical joke:
&lt;blockquote&gt;
  I think there may be something related to being a hack that can
  be powerful  the idea of making the tenuousness and implausibility
  &lt;i&gt;a feature&lt;/i&gt;. "Yes, it's a bit ridiculous, right?  I'm just trying
  to see how far such a naive approach can get." YC seemed to me
  to have this characteristic.
&lt;/blockquote&gt;
[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
Much of the advantage of switching from physical to digital
media is not the software per se but that it lets you start something
new with little upfront commitment.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f6n"&gt;&lt;font color="#000000"&gt;6&lt;/font&gt;&lt;/a&gt;]
John Carmack adds:
&lt;blockquote&gt;
  The value of a medium without a vast gulf between the early work
  and the final work is exemplified in game mods. The original
  Quake game was a golden age for mods, because everything was very
  flexible, but so crude due to technical limitations, that quick
  hacks to try out a gameplay idea weren't all &lt;i&gt;that&lt;/i&gt; far from the
  official game. Many careers were born from that, but as the
  commercial game quality improved over the years, it became almost
  a full time job to make a successful mod that would be appreciated
  by the community. This was dramatically reversed with Minecraft
  and later Roblox, where the entire esthetic of the experience was
  so explicitly crude that innovative gameplay concepts became the
  overriding value. These "crude" game mods by single authors are
  now often bigger deals than massive professional teams' work.
&lt;/blockquote&gt;
[&lt;a name="f7n"&gt;&lt;font color="#000000"&gt;7&lt;/font&gt;&lt;/a&gt;]
Lisa Randall suggests that we
&lt;blockquote&gt;
  treat new things as experiments. That way there's no such thing
  as failing, since you learn something no matter what. You treat
  it like an experiment in the sense that if it really rules something
  out, you give up and move on, but if there's some way to vary it
  to make it work better, go ahead and do that
&lt;/blockquote&gt;
[&lt;a name="f8n"&gt;&lt;font color="#000000"&gt;8&lt;/font&gt;&lt;/a&gt;]
Michael Nielsen points out that the internet has made this
easier, because you can see programmers' first commits, musicians'
first videos, and so on.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, John Carmack, Patrick Collison, Jessica
Livingston, Michael Nielsen, and Lisa Randall for reading drafts
of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//early.html</guid>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Modeling a Wealth Tax</title>
      <link>https://paulgraham.com//wtax.html</link>
      <description>&lt;font face="verdana" size="2"&gt;August 2020&lt;br/&gt;&lt;br/&gt;Some politicians are proposing to introduce wealth taxes in addition
to income and capital gains taxes. Let's try modeling the effects of various levels
of wealth tax to see what they would mean in practice for a startup
founder.&lt;br/&gt;&lt;br/&gt;Suppose you start a successful startup in your twenties, and then
live for another 60 years. How much of your stock will a wealth tax
consume?&lt;br/&gt;&lt;br/&gt;If the wealth tax applies to all your assets, it's easy to&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;August 2020&lt;br/&gt;&lt;br/&gt;Some politicians are proposing to introduce wealth taxes in addition
to income and capital gains taxes. Let's try modeling the effects of various levels
of wealth tax to see what they would mean in practice for a startup
founder.&lt;br/&gt;&lt;br/&gt;Suppose you start a successful startup in your twenties, and then
live for another 60 years. How much of your stock will a wealth tax
consume?&lt;br/&gt;&lt;br/&gt;If the wealth tax applies to all your assets, it's easy to
calculate its effect. A wealth tax of 1% means you get to keep
99% of your stock each year. After 60 years the proportion
of stock you'll have left will be .99^60, or .547. So a
straight 1% wealth tax means the government will over the
course of your life take 45% of your stock.&lt;br/&gt;&lt;br/&gt;(Losing shares does not, obviously, mean becoming &lt;i&gt;net&lt;/i&gt;
poorer unless the value per share is increasing by less than the 
wealth tax rate.)&lt;br/&gt;&lt;br/&gt;Here's how much stock the government would take over 60
years at various levels of wealth tax:&lt;br/&gt;&lt;br/&gt;&lt;center&gt;
&lt;table border="1" cellpadding="4" cellspacing="0"&gt;&lt;tr&gt;&lt;td align="center"&gt;&lt;font size="2"&gt;wealth tax&lt;td align="center"&gt;&lt;font size="2"&gt;government takes&lt;/font&gt;&lt;/td&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;0.1%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;6%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;0.5%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;26%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;1.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;45%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;2.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;70%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;3.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;84%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;4.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;91%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;5.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;95%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;
A wealth tax will usually have a threshold at which it starts.
How much difference would a high threshold make? To model that,
we need to make some assumptions about the initial value of
your stock and the growth rate.&lt;br/&gt;&lt;br/&gt;Suppose your stock is initially
worth $2 million, and the company's trajectory is as follows:
the value of your stock grows 3x for 2 years, then 2x for 2 years,
then 50% for 2 years, after
which you just get a typical public company growth rate,
which we'll call 8%. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/wtax.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;
Suppose the wealth tax threshold is
$50 million. How much stock does the government take now?
&lt;center&gt;
&lt;table border="1" cellpadding="4" cellspacing="0"&gt;&lt;tr&gt;&lt;td align="center"&gt;&lt;font size="2"&gt;wealth tax&lt;td align="center"&gt;&lt;font size="2"&gt;government takes&lt;/font&gt;&lt;/td&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;0.1%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;5%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;0.5%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;23%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;1.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;41%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;2.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;65%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;3.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;79%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;4.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;88%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;5.0%&lt;/font&gt;&lt;td align="right"&gt;&lt;font size="2"&gt;93%&lt;/font&gt;&lt;/td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;
It may at first seem surprising that such apparently small tax rates
produce such dramatic effects. A 2% wealth tax with a $50 million
threshold takes about two thirds of a successful founder's stock.&lt;br/&gt;&lt;br/&gt;The reason wealth taxes have such dramatic effects is that they're
applied over and over to the same money. Income tax
happens every year, but only to that year's income. Whereas if you
live for 60 years after acquiring some asset, a wealth tax will tax
that same asset 60 times. A wealth tax compounds.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Note&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
In practice, eventually some of this 8% would come in the form of 
dividends, which are taxed as income at issue, so this model actually
represents the most optimistic case for the founder.&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;/p&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//wtax.html</guid>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The Four Quadrants of Conformism</title>
      <link>https://paulgraham.com//conformism.html</link>
      <description>&lt;font face="verdana" size="2"&gt;July 2020&lt;br/&gt;&lt;br/&gt;One of the most revealing ways to classify people is by the degree
and aggressiveness of their conformism. Imagine a Cartesian coordinate
system whose horizontal axis runs from conventional-minded on the
left to independent-minded on the right, and whose vertical axis
runs from passive at the bottom to aggressive at the top. The
resulting four quadrants define four types of people. Starting in
the upper left and going counter-clockwise: aggressive&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;July 2020&lt;br/&gt;&lt;br/&gt;One of the most revealing ways to classify people is by the degree
and aggressiveness of their conformism. Imagine a Cartesian coordinate
system whose horizontal axis runs from conventional-minded on the
left to independent-minded on the right, and whose vertical axis
runs from passive at the bottom to aggressive at the top. The
resulting four quadrants define four types of people. Starting in
the upper left and going counter-clockwise: aggressively
conventional-minded, passively conventional-minded, passively
independent-minded, and aggressively independent-minded.&lt;br/&gt;&lt;br/&gt;I think that you'll find all four types in most societies, and that
which quadrant people fall into depends more on their own personality
than the beliefs prevalent in their society.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/conformism.html#f1n"&gt;&lt;font color="#dddddd"&gt;1&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Young children offer some of the best evidence for both points.
Anyone who's been to primary school has seen the four types, and
the fact that school rules are so arbitrary is strong evidence that
which quadrant people fall into depends more on them than the rules.&lt;br/&gt;&lt;br/&gt;The kids in the upper left quadrant, the aggressively conventional-minded
ones, are the tattletales. They believe not only that rules must
be obeyed, but that those who disobey them must be punished.&lt;br/&gt;&lt;br/&gt;The kids in the lower left quadrant, the passively conventional-minded,
are the sheep. They're careful to obey the rules, but when other
kids break them, their impulse is to worry that those kids will be
punished, not to ensure that they will.&lt;br/&gt;&lt;br/&gt;The kids in the lower right quadrant, the passively independent-minded,
are the dreamy ones. They don't care much about rules and probably
aren't 100% sure what the rules even are.&lt;br/&gt;&lt;br/&gt;And the kids in the upper right quadrant, the aggressively
independent-minded, are the naughty ones. When they see a rule,
their first impulse is to question it. Merely being told what to
do makes them inclined to do the opposite.&lt;br/&gt;&lt;br/&gt;When measuring conformism, of course, you have to say with respect
to what, and this changes as kids get older. For younger kids it's
the rules set by adults. But as kids get older, the source of rules
becomes their peers. So a pack of teenagers who all flout school
rules in the same way are not independent-minded; rather the opposite.&lt;br/&gt;&lt;br/&gt;In adulthood we can recognize the four types by their distinctive
calls, much as you could recognize four species of birds. The call
of the aggressively conventional-minded is "Crush &amp;lt;outgroup&amp;gt;!" (It's
rather alarming to see an exclamation point after a variable, but
that's the whole problem with the aggressively conventional-minded.)
The call of the passively conventional-minded is "What will the
neighbors think?" The call of the passively independent-minded is
"To each his own." And the call of the aggressively independent-minded
is "Eppur si muove."&lt;br/&gt;&lt;br/&gt;The four types are not equally common. There are more passive people
than aggressive ones, and far more conventional-minded people than
independent-minded ones. So the passively conventional-minded are
the largest group, and the aggressively independent-minded the
smallest.&lt;br/&gt;&lt;br/&gt;Since one's quadrant depends more on one's personality than the
nature of the rules, most people would occupy the same quadrant
even if they'd grown up in a quite different society.&lt;br/&gt;&lt;br/&gt;Princeton professor Robert George recently wrote:
&lt;blockquote&gt;
   I sometimes ask students what their position on slavery would
   have been had they been white and living in the South before
   abolition. Guess what? They all would have been abolitionists!
   They all would have bravely spoken out against slavery, and
   worked tirelessly against it.
&lt;/blockquote&gt;
He's too polite to say so, but of course they wouldn't. And indeed,
our default assumption should not merely be that his students would,
on average, have behaved the same way people did at the time, but
that the ones who are aggressively conventional-minded today would
have been aggressively conventional-minded then too. In other words,
that they'd not only not have fought against slavery, but that
they'd have been among its staunchest defenders.&lt;br/&gt;&lt;br/&gt;I'm biased, I admit, but it seems to me that aggressively
conventional-minded people are responsible for a disproportionate
amount of the trouble in the world, and that a lot of the customs
we've evolved since the Enlightenment have been designed to protect
the rest of us from them. In particular, the retirement of the
concept of heresy and its replacement by the principle of freely
debating all sorts of different ideas, even ones that are currently
considered unacceptable, without any punishment for those who try
them out to see if they work.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/conformism.html#f2n"&gt;&lt;font color="#dddddd"&gt;2&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Why do the independent-minded need to be protected, though? Because
they have all the new ideas. To be a successful scientist, for
example, it's not enough just to be right. You have to be right
when everyone else is wrong. Conventional-minded people can't do
that. For similar reasons, all successful startup CEOs are not
merely independent-minded, but aggressively so. So it's no coincidence
that societies prosper only to the extent that they have customs
for keeping the conventional-minded at bay.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/conformism.html#f3n"&gt;&lt;font color="#dddddd"&gt;3&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;In the last few years, many of us have noticed that the customs
protecting free inquiry have been weakened. Some say we're overreacting
 that they haven't been weakened very much, or that they've been
weakened in the service of a greater good. The latter I'll dispose
of immediately. When the conventional-minded get the upper hand,
they always say it's in the service of a greater good.  It just
happens to be a different, incompatible greater good each time.&lt;br/&gt;&lt;br/&gt;As for the former worry, that the independent-minded are being
oversensitive, and that free inquiry hasn't been shut down that
much, you can't judge that unless you are yourself independent-minded.
You can't know how much of the space of ideas is being lopped off
unless you have them, and only the independent-minded have the ones
at the edges. Precisely because of this, they tend to be very
sensitive to changes in how freely one can explore ideas. They're
the canaries in this coalmine.&lt;br/&gt;&lt;br/&gt;The conventional-minded say, as they always do, that they don't
want to shut down the discussion of all ideas, just the bad ones.&lt;br/&gt;&lt;br/&gt;You'd think it would be obvious just from that sentence what a
dangerous game they're playing. But I'll spell it out. There are
two reasons why we need to be able to discuss even "bad" ideas.&lt;br/&gt;&lt;br/&gt;The first is that any process for deciding which ideas to ban is
bound to make mistakes. All the more so because no one intelligent
wants to undertake that kind of work, so it ends up being done by
the stupid. And when a process makes a lot of mistakes, you need
to leave a margin for error. Which in this case means you need to
ban fewer ideas than you'd like to. But that's hard for the
aggressively conventional-minded to do, partly because they enjoy
seeing people punished, as they have since they were children, and
partly because they compete with one another. Enforcers of orthodoxy
can't allow a borderline idea to exist, because that gives other
enforcers an opportunity to one-up them in the moral purity department,
and perhaps even to turn enforcer upon them. So instead of getting
the margin for error we need, we get the opposite: a race to the
bottom in which any idea that seems at all bannable ends up being
banned. 
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/conformism.html#f4n"&gt;&lt;font color="#dddddd"&gt;4&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;The second reason it's dangerous to ban the discussion of ideas is
that ideas are more closely related than they look. Which means if
you restrict the discussion of some topics, it doesn't only affect
those topics. The restrictions propagate back into any topic that
yields implications in the forbidden ones. And that is not an edge
case. The best ideas do exactly that: they have consequences
in fields far removed from their origins. Having ideas in a world
where some ideas are banned is like playing soccer on a pitch that
has a minefield in one corner. You don't just play the same game
you would have, but on a different shaped pitch. You play a much
more subdued game even on the ground that's safe.&lt;br/&gt;&lt;br/&gt;In the past, the way the independent-minded protected themselves
was to congregate in a handful of places   first in courts, and
later in universities  where they could to some extent make their
own rules. Places where people work with ideas tend to have customs
protecting free inquiry, for the same reason wafer fabs have powerful
air filters, or recording studios good sound insulation. For the
last couple centuries at least, when the aggressively conventional-minded
were on the rampage for whatever reason, universities were the
safest places to be.&lt;br/&gt;&lt;br/&gt;That may not work this time though, due to the unfortunate fact
that the latest wave of intolerance began in universities. It began
in the mid 1980s, and by 2000 seemed to have died down, but it has
recently flared up again with the arrival of social media. This
seems, unfortunately, to have been an own goal by Silicon Valley.
Though the people who run Silicon Valley are almost all independent-minded,
they've handed the aggressively conventional-minded a tool such as
they could only have dreamed of.&lt;br/&gt;&lt;br/&gt;On the other hand, perhaps the decline in the spirit of free inquiry
within universities is as much the symptom of the departure of the
independent-minded as the cause. People who would have become
professors 50 years ago have other options now. Now they can become
quants or start startups. You have to be independent-minded to
succeed at either of those. If these people had been professors,
they'd have put up a stiffer resistance on behalf of academic
freedom. So perhaps the picture of the independent-minded fleeing
declining universities is too gloomy. Perhaps the universities are
declining because so many have already left.
&lt;font color="#dddddd"&gt;[&lt;a href="https://paulgraham.com/conformism.html#f5n"&gt;&lt;font color="#dddddd"&gt;5&lt;/font&gt;&lt;/a&gt;]&lt;/font&gt;&lt;br/&gt;&lt;br/&gt;Though I've spent a lot of time thinking about this situation, I
can't predict how it plays out. Could some universities reverse the
current trend and remain places where the independent-minded want
to congregate? Or will the independent-minded gradually abandon
them? I worry a lot about what we might lose if that happened.&lt;br/&gt;&lt;br/&gt;But I'm hopeful long term. The independent-minded are good at
protecting themselves. If existing institutions are compromised,
they'll create new ones. That may require some imagination. But
imagination is, after all, their specialty.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[&lt;a name="f1n"&gt;&lt;font color="#000000"&gt;1&lt;/font&gt;&lt;/a&gt;]
I realize of course that if people's personalities vary in any
two ways, you can use them as axes and call the resulting four
quadrants personality types. So what I'm really claiming is that
the axes are orthogonal and that there's significant variation in
both.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f2n"&gt;&lt;font color="#000000"&gt;2&lt;/font&gt;&lt;/a&gt;]
The aggressively conventional-minded aren't responsible for all
the trouble in the world. Another big source of trouble is the sort
of charismatic leader who gains power by appealing to them. They
become much more dangerous when such leaders emerge.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f3n"&gt;&lt;font color="#000000"&gt;3&lt;/font&gt;&lt;/a&gt;]
I never worried about writing things that offended the
conventional-minded when I was running Y Combinator. If YC were a
cookie company, I'd have faced a difficult moral choice.
Conventional-minded people eat cookies too. But they don't start
successful startups. So if I deterred them from applying to YC, the
only effect was to save us work reading applications.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f4n"&gt;&lt;font color="#000000"&gt;4&lt;/font&gt;&lt;/a&gt;]
There has been progress in one area: the punishments for talking
about banned ideas are less severe than in the past. There's little
danger of being killed, at least in richer countries. The aggressively
conventional-minded are mostly satisfied with getting people fired.&lt;br/&gt;&lt;br/&gt;[&lt;a name="f5n"&gt;&lt;font color="#000000"&gt;5&lt;/font&gt;&lt;/a&gt;]
Many professors are independent-minded  especially in math,
the hard sciences, and engineering, where you have to be to succeed.
But students are more representative of the general population, and
thus mostly conventional-minded. So when professors and students
are in conflict, it's not just a conflict between generations but
also between different types of people.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Sam Altman, Trevor Blackwell, Nicholas Christakis, Patrick
Collison, Sam Gichuru, Jessica Livingston, Patrick McKenzie, Geoff
Ralston, and Harj Taggar for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//conformism.html</guid>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Orthodox Privilege</title>
      <link>https://paulgraham.com//orth.html</link>
      <description>&lt;font face="verdana" size="2"&gt;July 2020&lt;br/&gt;&lt;br/&gt;&lt;table cellpadding="6" cellspacing="0" width="100%"&gt;
&lt;tr&gt;&lt;td bgcolor="#ffffdd"&gt;
&lt;table cellpadding="0" cellspacing="0" width="100%"&gt;
&lt;tr&gt;&lt;td bgcolor="#ffffdd"&gt;&lt;font size="2"&gt;&#13;
"Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social environment. Most people are even incapable of forming such opinions."&lt;br/&gt;&lt;br/&gt; Einstein&#13;
&lt;/font&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;
There has been &lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;July 2020&lt;br/&gt;&lt;br/&gt;&lt;table cellpadding="6" cellspacing="0" width="100%"&gt;
&lt;tr&gt;&lt;td bgcolor="#ffffdd"&gt;
&lt;table cellpadding="0" cellspacing="0" width="100%"&gt;
&lt;tr&gt;&lt;td bgcolor="#ffffdd"&gt;&lt;font size="2"&gt;&#13;
"Few people are capable of expressing with equanimity opinions which differ from the prejudices of their social environment. Most people are even incapable of forming such opinions."&lt;br/&gt;&lt;br/&gt; Einstein&#13;
&lt;/font&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;
There has been a lot of talk about privilege lately. Although the&#13;
concept is overused, there is something to it, and in particular&#13;
to the idea that privilege makes you blind  that you can't see&#13;
things that are visible to someone whose life is very different&#13;
from yours.&lt;br/&gt;&lt;br/&gt;But one of the most pervasive examples of this kind of blindness&#13;
is one that I haven't seen mentioned explicitly. I'm going to call&#13;
it &lt;i&gt;orthodox privilege&lt;/i&gt;: The more conventional-minded someone is, the&#13;
more it seems to them that it's safe for everyone to express their&#13;
opinions.&lt;br/&gt;&lt;br/&gt;It's safe for &lt;i&gt;them&lt;/i&gt; to express their opinions, because the source&#13;
of their opinions is whatever it's currently acceptable to believe.&#13;
So it seems to them that it must be safe for everyone. They literally&#13;
can't imagine a true statement that would get you in trouble.&lt;br/&gt;&lt;br/&gt;And yet at every point in history, there &lt;a href="https://paulgraham.com/say.html"&gt;&lt;u&gt;were&lt;/u&gt;&lt;/a&gt;&#13;
true things that would&#13;
get you in trouble to say. &#13;
Is ours the first where this&#13;
isn't so? What an amazing coincidence that would be.&lt;br/&gt;&lt;br/&gt;Surely it should at least be the default assumption that our time&#13;
is not unique, and that there are true things you can't say now,&#13;
just as there have always been. You would think. But even in the&#13;
face of such overwhelming historical evidence, most people will go&#13;
with their gut on this one.&lt;br/&gt;&lt;br/&gt;In the most extreme cases, people suffering from orthodox&#13;
privilege will not only deny that there's anything true that &#13;
you can't say, but will accuse you of heresy merely for saying there is. &#13;
Though if there's more than one heresy current in&#13;
your time, these accusations will be weirdly non-deterministic:&#13;
you must either be an xist or a yist.&lt;br/&gt;&lt;br/&gt;Frustrating as it is to deal with these people, it's important to&#13;
realize that they're in earnest. They're not pretending they think&#13;
it's impossible for an idea to be both unorthodox and true. The&#13;
world really looks that way to them.&lt;br/&gt;&lt;br/&gt;Indeed, this is a uniquely tenacious form of privilege. People can&#13;
overcome the blindness induced by most forms of privilege by learning&#13;
more about whatever they're not. But they can't overcome orthodox&#13;
privilege just by learning more. They'd have to become more&#13;
independent-minded. If that happens at all, it doesn't happen on&#13;
the time scale of one conversation.&lt;br/&gt;&lt;br/&gt;It may be possible to convince some people that orthodox privilege&#13;
must exist even though they can't sense it, just as one can with,&#13;
say, dark matter. There may be some who could be convinced, for&#13;
example, that it's very unlikely that this is the first point in&#13;
history at which there's nothing true you can't say, even if they&#13;
can't imagine specific examples.&lt;br/&gt;&lt;br/&gt;But in general I don't think it will work to say&#13;
"check your privilege" about this type of privilege, because those&#13;
in its demographic don't realize they're in it. It doesn't seem to&#13;
conventional-minded people that they're conventional-minded. It&#13;
just seems to them that they're right. Indeed, they tend to be&#13;
particularly sure of it.&lt;br/&gt;&lt;br/&gt;Perhaps the solution is to appeal to politeness. If someone says&#13;
they can hear a high-pitched noise that you can't, it's only polite&#13;
to take them at their word, instead of demanding evidence that's&#13;
impossible to produce, or simply denying that they hear anything.&#13;
Imagine how rude that would seem. Similarly, if someone says they&#13;
can think of things that are true but that cannot be said, it's&#13;
only polite to take them at their word, even if you can't think of&#13;
any yourself.&#13;
&lt;!--&#13;
Once you realize that orthodox privilege exists, a lot of other&#13;
things become clearer. For example, how can it be that a large&#13;
number of reasonable, intelligent people worry about something they&#13;
call "cancel culture," while other reasonable, intelligent people&#13;
deny that it's a problem? Once you understand the concept of orthodox&#13;
privilege, it's easy to see the source of this disagreement.  If&#13;
you believe there's nothing true that you can't say, then anyone&#13;
who gets in trouble for something they say must deserve it. --&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Sam Altman, Trevor Blackwell, Patrick Collison, Antonio Garcia-Martinez,&#13;
Jessica Livingston, Robert Morris, Michael Nielsen, Geoff Ralston, Max Roser, and&#13;
Harj Taggar for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//orth.html</guid>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Coronavirus and Credibility</title>
      <link>https://paulgraham.com//cred.html</link>
      <description>&lt;font face="verdana" size="2"&gt;April 2020&lt;br/&gt;&lt;br/&gt;I recently saw a 
&lt;a href="https://www.youtube.com/watch?v=NAh4uS4f78o"&gt;&lt;u&gt;video&lt;/u&gt;&lt;/a&gt; 
of TV journalists and politicians confidently
saying that the coronavirus would be no worse than the flu. What
struck me about it was not just how mistaken they seemed, but how
daring. How could they feel safe saying such things?&lt;br/&gt;&lt;br/&gt;The answer, I realized, is that they didn't think they could get
caught. They didn't realize there was any danger in maki&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;April 2020&lt;br/&gt;&lt;br/&gt;I recently saw a 
&lt;a href="https://www.youtube.com/watch?v=NAh4uS4f78o"&gt;&lt;u&gt;video&lt;/u&gt;&lt;/a&gt; 
of TV journalists and politicians confidently
saying that the coronavirus would be no worse than the flu. What
struck me about it was not just how mistaken they seemed, but how
daring. How could they feel safe saying such things?&lt;br/&gt;&lt;br/&gt;The answer, I realized, is that they didn't think they could get
caught. They didn't realize there was any danger in making false
predictions. These people constantly make false predictions, and
get away with it, because the things they make predictions about
either have mushy enough outcomes that they can bluster their way
out of trouble, or happen so far in the future that few remember
what they said.&lt;br/&gt;&lt;br/&gt;An epidemic is different. It falsifies your predictions rapidly and
unequivocally.&lt;br/&gt;&lt;br/&gt;But epidemics are rare enough that these people clearly
didn't realize this was even a possibility. Instead they just
continued to use their ordinary m.o., which, as the epidemic has
made clear, is to talk confidently about things they don't
understand.&lt;br/&gt;&lt;br/&gt;An event like this is thus a uniquely powerful way of taking people's
measure. As Warren Buffett said, "It's only when the tide goes out
that you learn who's been swimming naked." And the tide has just
gone out like never before.&lt;br/&gt;&lt;br/&gt;Now that we've seen the results, let's remember what we saw, because
this is the most accurate test of credibility we're ever likely to have. I hope.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//cred.html</guid>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to Write Usefully</title>
      <link>https://paulgraham.com//useful.html</link>
      <description>&lt;font face="verdana" size="2"&gt;February 2020&lt;br/&gt;&lt;br/&gt;What should an essay be? Many people would say persuasive. That's
what a lot of us were taught essays should be. But I think we can
aim for something more ambitious: that an essay should be useful.&lt;br/&gt;&lt;br/&gt;To start with, that means it should be correct. But it's not enough
merely to be correct. It's easy to make a statement correct by
making it vague. That's a common flaw in academic writing, for
example. If you know nothing at all about an i&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;February 2020&lt;br/&gt;&lt;br/&gt;What should an essay be? Many people would say persuasive. That's
what a lot of us were taught essays should be. But I think we can
aim for something more ambitious: that an essay should be useful.&lt;br/&gt;&lt;br/&gt;To start with, that means it should be correct. But it's not enough
merely to be correct. It's easy to make a statement correct by
making it vague. That's a common flaw in academic writing, for
example. If you know nothing at all about an issue, you can't go
wrong by saying that the issue is a complex one, that there are
many factors to be considered, that it's a mistake to take too
simplistic a view of it, and so on.&lt;br/&gt;&lt;br/&gt;Though no doubt correct, such statements tell the reader nothing.
Useful writing makes claims that are as strong as they can be made
without becoming false.&lt;br/&gt;&lt;br/&gt;For example, it's more useful to say that Pike's Peak is near the
middle of Colorado than merely somewhere in Colorado. But if I say
it's in the exact middle of Colorado, I've now gone too far, because
it's a bit east of the middle.&lt;br/&gt;&lt;br/&gt;Precision and correctness are like opposing forces. It's easy to
satisfy one if you ignore the other. The converse of vaporous
academic writing is the bold, but false, rhetoric of demagogues.
Useful writing is bold, but true.&lt;br/&gt;&lt;br/&gt;It's also two other things: it tells people something important,
and that at least some of them didn't already know.&lt;br/&gt;&lt;br/&gt;Telling people something they didn't know doesn't always mean
surprising them. Sometimes it means telling them something they
knew unconsciously but had never put into words. In fact those may
be the more valuable insights, because they tend to be more
fundamental.&lt;br/&gt;&lt;br/&gt;Let's put them all together. Useful writing tells people something
true and important that they didn't already know, and tells them
as unequivocally as possible.&lt;br/&gt;&lt;br/&gt;Notice these are all a matter of degree. For example, you can't
expect an idea to be novel to everyone. Any insight that you have
will probably have already been had by at least one of the world's
7 billion people. But it's sufficient if an idea is novel to a lot
of readers.&lt;br/&gt;&lt;br/&gt;Ditto for correctness, importance, and strength. In effect the four
components are like numbers you can multiply together to get a score
for usefulness. Which I realize is almost awkwardly reductive, but
nonetheless true.&lt;br/&gt;&lt;br/&gt;&lt;center&gt;_____&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
How can you ensure that the things you say are true and novel and
important? Believe it or not, there is a trick for doing this. I
learned it from my friend Robert Morris, who has a horror of saying
anything dumb. His trick is not to say anything unless he's sure
it's worth hearing. This makes it hard to get opinions out of him,
but when you do, they're usually right.&lt;br/&gt;&lt;br/&gt;Translated into essay writing, what this means is that if you write
a bad sentence, you don't publish it. You delete it and try again.
Often you abandon whole branches of four or five paragraphs. Sometimes
a whole essay.&lt;br/&gt;&lt;br/&gt;You can't ensure that every idea you have is good, but you can
ensure that every one you publish is, by simply not publishing the
ones that aren't.&lt;br/&gt;&lt;br/&gt;In the sciences, this is called publication bias, and is considered
bad. When some hypothesis you're exploring gets inconclusive results,
you're supposed to tell people about that too. But with essay
writing, publication bias is the way to go.&lt;br/&gt;&lt;br/&gt;My strategy is loose, then tight. I write the first draft of an
essay fast, trying out all kinds of ideas. Then I spend days rewriting
it very carefully.&lt;br/&gt;&lt;br/&gt;I've never tried to count how many times I proofread essays, but
I'm sure there are sentences I've read 100 times before publishing
them. When I proofread an essay, there are usually passages that
stick out in an annoying way, sometimes because they're clumsily
written, and sometimes because I'm not sure they're true. The
annoyance starts out unconscious, but after the tenth reading or
so I'm saying "Ugh, that part" each time I hit it. They become like
briars that catch your sleeve as you walk past. Usually I won't
publish an essay till they're all gone  till I can read through
the whole thing without the feeling of anything catching.&lt;br/&gt;&lt;br/&gt;I'll sometimes let through a sentence that seems clumsy, if I can't
think of a way to rephrase it, but I will never knowingly let through
one that doesn't seem correct. You never have to. If a sentence
doesn't seem right, all you have to do is ask why it doesn't, and
you've usually got the replacement right there in your head.&lt;br/&gt;&lt;br/&gt;This is where essayists have an advantage over journalists. You
don't have a deadline. You can work for as long on an essay as you
need to get it right. You don't have to publish the essay at all,
if you can't get it right. Mistakes seem to lose courage in the
face of an enemy with unlimited resources. Or that's what it feels
like. What's really going on is that you have different expectations
for yourself. You're like a parent saying to a child "we can sit
here all night till you eat your vegetables." Except you're the
child too.&lt;br/&gt;&lt;br/&gt;I'm not saying no mistake gets through. For example, I added condition
(c) in &lt;a href="https://paulgraham.com/bias.html"&gt;&lt;u&gt;"A Way to Detect Bias"&lt;/u&gt;&lt;/a&gt; 
after readers pointed out that I'd
omitted it. But in practice you can catch nearly all of them.&lt;br/&gt;&lt;br/&gt;There's a trick for getting importance too. It's like the trick I
suggest to young founders for getting startup ideas: to make something
you yourself want. You can use yourself as a proxy for the reader.
The reader is not completely unlike you, so if you write about
topics that seem important to you, they'll probably seem important
to a significant number of readers as well.&lt;br/&gt;&lt;br/&gt;Importance has two factors. It's the number of people something
matters to, times how much it matters to them. Which means of course
that it's not a rectangle, but a sort of ragged comb, like a Riemann
sum.&lt;br/&gt;&lt;br/&gt;The way to get novelty is to write about topics you've thought about
a lot. Then you can use yourself as a proxy for the reader in this
department too. Anything you notice that surprises you, who've
thought about the topic a lot, will probably also surprise a
significant number of readers. And here, as with correctness and
importance, you can use the Morris technique to ensure that you
will. If you don't learn anything from writing an essay, don't
publish it.&lt;br/&gt;&lt;br/&gt;You need humility to measure novelty, because acknowledging the
novelty of an idea means acknowledging your previous ignorance of
it. Confidence and humility are often seen as opposites, but in
this case, as in many others, confidence helps you to be humble.
If you know you're an expert on some topic, you can freely admit
when you learn something you didn't know, because you can be confident
that most other people wouldn't know it either.&lt;br/&gt;&lt;br/&gt;The fourth component of useful writing, strength, comes from two
things: thinking well, and the skillful use of qualification. These
two counterbalance each other, like the accelerator and clutch in
a car with a manual transmission. As you try to refine the expression
of an idea, you adjust the qualification accordingly. Something
you're sure of, you can state baldly with no qualification at all,
as I did the four components of useful writing. Whereas points that
seem dubious have to be held at arm's length with perhapses.&lt;br/&gt;&lt;br/&gt;As you refine an idea, you're pushing in the direction of less
qualification. But you can rarely get it down to zero. Sometimes
you don't even want to, if it's a side point and a fully refined
version would be too long.&lt;br/&gt;&lt;br/&gt;Some say that qualifications weaken writing. For example, that you
should never begin a sentence in an essay with "I think," because
if you're saying it, then of course you think it. And it's true
that "I think x" is a weaker statement than simply "x." Which is
exactly why you need "I think." You need it to express your degree
of certainty.&lt;br/&gt;&lt;br/&gt;But qualifications are not scalars. They're not just experimental
error. There must be 50 things they can express: how broadly something
applies, how you know it, how happy you are it's so, even how it
could be falsified. I'm not going to try to explore the structure
of qualification here. It's probably more complex than the whole
topic of writing usefully. Instead I'll just give you a practical
tip: Don't underestimate qualification. It's an important skill in
its own right, not just a sort of tax you have to pay in order to
avoid saying things that are false. So learn and use its full range.
It may not be fully half of having good ideas, but it's part of
having them.&lt;br/&gt;&lt;br/&gt;There's one other quality I aim for in essays: to say things as
simply as possible. But I don't think this is a component of
usefulness. It's more a matter of consideration for the reader. And
it's a practical aid in getting things right; a mistake is more
obvious when expressed in simple language. But I'll admit that the
main reason I write simply is not for the reader's sake or because
it helps get things right, but because it bothers me to use more
or fancier words than I need to. It seems inelegant, like a program
that's too long.&lt;br/&gt;&lt;br/&gt;I realize florid writing works for some people. But unless you're
sure you're one of them, the best advice is to write as simply as
you can.&lt;br/&gt;&lt;br/&gt;&lt;center&gt;_____&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
I believe the formula I've given you, importance + novelty +
correctness + strength, is the recipe for a good essay. But I should
warn you that it's also a recipe for making people mad.&lt;br/&gt;&lt;br/&gt;The root of the problem is novelty. When you tell people something
they didn't know, they don't always thank you for it. Sometimes the
reason people don't know something is because they don't want to
know it. Usually because it contradicts some cherished belief. And
indeed, if you're looking for novel ideas, popular but mistaken
beliefs are a good place to find them. Every popular mistaken belief
creates a &lt;a href="https://paulgraham.com/nov.html"&gt;&lt;u&gt;dead zone&lt;/u&gt;&lt;/a&gt; of ideas around 
it that are relatively unexplored because they contradict it.&lt;br/&gt;&lt;br/&gt;The strength component just makes things worse. If there's anything
that annoys people more than having their cherished assumptions
contradicted, it's having them flatly contradicted.&lt;br/&gt;&lt;br/&gt;Plus if you've used the Morris technique, your writing will seem
quite confident. Perhaps offensively confident, to people who
disagree with you. The reason you'll seem confident is that you are
confident: you've cheated, by only publishing the things you're
sure of.  It will seem to people who try to disagree with you that
you never admit you're wrong. In fact you constantly admit you're
wrong. You just do it before publishing instead of after.&lt;br/&gt;&lt;br/&gt;And if your writing is as simple as possible, that just makes things
worse. Brevity is the diction of command. If you watch someone
delivering unwelcome news from a position of inferiority, you'll
notice they tend to use lots of words, to soften the blow. Whereas
to be short with someone is more or less to be rude to them.&lt;br/&gt;&lt;br/&gt;It can sometimes work to deliberately phrase statements more weakly
than you mean. To put "perhaps" in front of something you're actually
quite sure of. But you'll notice that when writers do this, they
usually do it with a wink.&lt;br/&gt;&lt;br/&gt;I don't like to do this too much. It's cheesy to adopt an ironic
tone for a whole essay. I think we just have to face the fact that
elegance and curtness are two names for the same thing.&lt;br/&gt;&lt;br/&gt;You might think that if you work sufficiently hard to ensure that
an essay is correct, it will be invulnerable to attack. That's sort
of true. It will be invulnerable to valid attacks. But in practice
that's little consolation.&lt;br/&gt;&lt;br/&gt;In fact, the strength component of useful writing will make you
particularly vulnerable to misrepresentation. If you've stated an
idea as strongly as you could without making it false, all anyone
has to do is to exaggerate slightly what you said, and now it is
false.&lt;br/&gt;&lt;br/&gt;Much of the time they're not even doing it deliberately. One of the
most surprising things you'll discover, if you start writing essays,
is that people who disagree with you rarely disagree with what
you've actually written. Instead they make up something you said
and disagree with that.&lt;br/&gt;&lt;br/&gt;For what it's worth, the countermove is to ask someone who does
this to quote a specific sentence or passage you wrote that they
believe is false, and explain why. I say "for what it's worth"
because they never do. So although it might seem that this could
get a broken discussion back on track, the truth is that it was
never on track in the first place.&lt;br/&gt;&lt;br/&gt;Should you explicitly forestall likely misinterpretations? Yes, if
they're misinterpretations a reasonably smart and well-intentioned
person might make. In fact it's sometimes better to say something
slightly misleading and then add the correction than to try to get
an idea right in one shot. That can be more efficient, and can also
model the way such an idea would be discovered.&lt;br/&gt;&lt;br/&gt;But I don't think you should explicitly forestall intentional
misinterpretations in the body of an essay. An essay is a place to
meet honest readers. You don't want to spoil your house by putting
bars on the windows to protect against dishonest ones. The place
to protect against intentional misinterpretations is in end-notes.
But don't think you can predict them all. People are as ingenious
at misrepresenting you when you say something they don't want to
hear as they are at coming up with rationalizations for things they
want to do but know they shouldn't. I suspect it's the same skill.&lt;br/&gt;&lt;br/&gt;&lt;center&gt;_____&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
As with most other things, the way to get better at writing essays
is to practice. But how do you start? Now that we've examined the
structure of useful writing, we can rephrase that question more
precisely. Which constraint do you relax initially? The answer is,
the first component of importance: the number of people who care
about what you write.&lt;br/&gt;&lt;br/&gt;If you narrow the topic sufficiently, you can probably find something
you're an expert on. Write about that to start with. If you only
have ten readers who care, that's fine. You're helping them, and
you're writing. Later you can expand the breadth of topics you write
about.&lt;br/&gt;&lt;br/&gt;The other constraint you can relax is a little surprising: publication.
Writing essays doesn't have to mean publishing them. That may seem
strange now that the trend is to publish every random thought, but
it worked for me. I wrote what amounted to essays in notebooks for
about 15 years. I never published any of them and never expected
to. I wrote them as a way of figuring things out. But when the web
came along I'd had a lot of practice.&lt;br/&gt;&lt;br/&gt;Incidentally, 
&lt;a href="http://www.foundersatwork.com/steve-wozniak.html"&gt;&lt;u&gt;Steve 
Wozniak&lt;/u&gt;&lt;/a&gt; did the same thing. In high school he
designed computers on paper for fun. He couldn't build them because
he couldn't afford the components. But when Intel launched 4K DRAMs
in 1975, he was ready.&lt;br/&gt;&lt;br/&gt;&lt;center&gt;_____&lt;/center&gt;&lt;br/&gt;&lt;br/&gt;
How many essays are there left to write though? The answer to that
question is probably the most exciting thing I've learned about
essay writing. Nearly all of them are left to write.&lt;br/&gt;&lt;br/&gt;Although &lt;a href="https://paulgraham.com/essay.html"&gt;&lt;u&gt;the essay&lt;/u&gt;&lt;/a&gt; 
is an old form, it hasn't been assiduously
cultivated. In the print era, publication was expensive, and there
wasn't enough demand for essays to publish that many. You could
publish essays if you were already well known for writing something
else, like novels. Or you could write book reviews that you took
over to express your own ideas. But there was not really a direct
path to becoming an essayist. Which meant few essays got written,
and those that did tended to be about a narrow range of subjects.&lt;br/&gt;&lt;br/&gt;Now, thanks to the internet, there's a path. Anyone can publish
essays online. You start in obscurity, perhaps, but at least you
can start. You don't need anyone's permission.&lt;br/&gt;&lt;br/&gt;It sometimes happens that an area of knowledge sits quietly for
years, till some change makes it explode. Cryptography did this to
number theory. The internet is doing it to the essay.&lt;br/&gt;&lt;br/&gt;The exciting thing is not that there's a lot left to write, but
that there's a lot left to discover. There's a certain kind of idea
that's best discovered by writing essays. If most essays are still
unwritten, most such ideas are still undiscovered.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Notes&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;[1] Put railings on the balconies, but don't put bars on the windows.&lt;br/&gt;&lt;br/&gt;[2] Even now I sometimes write essays that are not meant for
publication. I wrote several to figure out what Y Combinator should
do, and they were really helpful.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Thanks&lt;/b&gt; to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and
Robert Morris for reading drafts of this.&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//useful.html</guid>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Being a Noob</title>
      <link>https://paulgraham.com//noob.html</link>
      <description>&lt;font face="verdana" size="2"&gt;January 2020&lt;br/&gt;&lt;br/&gt;When I was young, I thought old people had everything figured out.
Now that I'm old, I know this isn't true.&lt;br/&gt;&lt;br/&gt;I constantly feel like a noob. It seems like I'm always talking to
some startup working in a new field I know nothing about, or reading
a book about a topic I don't understand well enough, or visiting some new
country where I don't know how things work.&lt;br/&gt;&lt;br/&gt;It's not pleasant to feel like a noob. And the word "noob" is
certa&lt;/font&gt;</description>
      <content:encoded>&lt;font face="verdana" size="2"&gt;January 2020&lt;br/&gt;&lt;br/&gt;When I was young, I thought old people had everything figured out.
Now that I'm old, I know this isn't true.&lt;br/&gt;&lt;br/&gt;I constantly feel like a noob. It seems like I'm always talking to
some startup working in a new field I know nothing about, or reading
a book about a topic I don't understand well enough, or visiting some new
country where I don't know how things work.&lt;br/&gt;&lt;br/&gt;It's not pleasant to feel like a noob. And the word "noob" is
certainly not a compliment. And yet today I realized something
encouraging about being a noob: the more of a noob you are locally,
the less of a noob you are globally.&lt;br/&gt;&lt;br/&gt;For example, if you stay in your home country, you'll feel less
of a noob than if you move to Farawavia, where everything works
differently. And yet you'll know more if you move.
So the feeling of being a noob is inversely correlated with actual
ignorance.&lt;br/&gt;&lt;br/&gt;But if the feeling of being a noob is good for us, why do we dislike
it? What evolutionary purpose could such an aversion serve?&lt;br/&gt;&lt;br/&gt;I think the answer is that there are two sources of feeling like a
noob: being stupid, and doing something novel. Our dislike of feeling
like a noob is our brain telling us "Come on, come on, figure this
out." Which was the right thing to be thinking for most of human
history. The life of hunter-gatherers was complex, but it didn't
change as much as life does now. They didn't suddenly have to figure
out what to do about cryptocurrency. So it made sense to be biased
toward competence at existing problems over the discovery of new
ones. It made sense for humans to dislike the feeling of being a
noob, just as, in a world where food was scarce, it made sense for
them to dislike the feeling of being hungry.&lt;br/&gt;&lt;br/&gt;Now that too much food is more of a problem than too little, our
dislike of feeling hungry leads us astray. And I think our dislike
of feeling like a noob does too.&lt;br/&gt;&lt;br/&gt;Though it feels unpleasant, and people will sometimes ridicule you
for it, the more you feel like a noob, the better.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/font&gt;</content:encoded>
      <guid isPermaLink="false">https://paulgraham.com//noob.html</guid>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
